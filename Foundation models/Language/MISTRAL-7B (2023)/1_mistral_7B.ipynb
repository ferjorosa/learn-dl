{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBJyDr-C9J8m"
   },
   "source": [
    "# MISTRAL-7B\n",
    "\n",
    "List of main relevant papers:\n",
    "* [Jiang et al. (2023)](https://arxiv.org/abs/2310.06825). Mistral 7B\n",
    "\n",
    "List of main relevant blogs:\n",
    "* [Mistral (2023)](https://www.pinecone.io/learn/image-search/). Mistral 7B. The best 7B model to date, Apache 2.0\n",
    "\n",
    "List of main relevant Youtube videos:\n",
    "* [PromptEngineering (2023)](https://www.youtube.com/watch?v=z4wPiallZcI). Mistral 7B -The Most Powerful 7B Model Yet\n",
    "\n",
    "Model: https://huggingface.co/mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9C0Tkn819J8o"
   },
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxfHS4Lk9J8o"
   },
   "source": [
    "In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model performance often necessitates an escalation in model size. However, this scaling tends to increase computational costs and inference latency, thereby raising barriers to deployment in practical, real-world scenarios. In this context, the search for balanced models delivering both high-level performance and efficiency becomes critically essential.\n",
    "\n",
    "Mistral 7B, demonstrates that a carefully designed language model can deliver high performance while maintaining an efficient inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [Touvron et al., 2023](https://arxiv.org/pdf/2307.09288.pdf)) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [Touvron et al., 2023](https://arxiv.org/pdf/2302.13971.pdf)) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B ([Rozière et al., 2023](https://arxiv.org/abs/2308.12950)), without sacrificing performance on non-code related benchmarks.\n",
    "\n",
    "\n",
    "Mistral 7B leverages grouped-query attention (GQA, [Ainslie et al., 2023](https://arxiv.org/abs/2305.13245)), and sliding window attention (SWA, [Beltagy et al., 2020](https://arxiv.org/abs/2004.05150)). GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs.\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** These attention mechanisms are aimed towards improving the efficiency of inference and the but should not improve the general model performance just by themselves. The intuition behind this performance improve is the propietary data that was used to train the model, probably much cleaner than most open-source data sources\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/results_bar.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The benchmarks are categorized by their themes:\n",
    "\n",
    "* **Commonsense Reasoning:** 0-shot average of Hellaswag, Winogrande, PIQA, SIQA, OpenbookQA, ARC-Easy, ARC-Challenge, and CommonsenseQA.\n",
    "* **World Knowledge:** 5-shot average of NaturalQuestions and TriviaQA.\n",
    "* **Reading Comprehension:** 0-shot average of BoolQ and QuAC.\n",
    "* **Math:** Average of 8-shot GSM8K with maj@8 and 4-shot MATH with maj@4\n",
    "* **Code:** Average of 0-shot Humaneval and 3-shot MBPP\n",
    "* **Popular aggregated results:** 5-shot MMLU, 3-shot BBH, and 3-5-shot AGI Eval (English multiple-choice questions only)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/results_table.png\" width=\"900\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "An interesting metric to compare how models fare in the cost/performance plane is to compute “equivalent model sizes”. On reasoning, comprehension and STEM reasoning (MMLU), M**istral 7B performs equivalently to a Llama 2 that would be more than 3x its size**. This is as much saved in memory and gained in throughput.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/results_effective_sizes.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which restricts the amount of knowledge it can compress)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0sST8Cd9J8p"
   },
   "source": [
    "# 2 - Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXdZLRHH9J8p"
   },
   "source": [
    "Mistral 7B is based on an autoregressive Transformer architecture. The main paramters of the architecture are summarized on the following table:\n",
    "\n",
    "| Parameter     | Value    |\n",
    "|-------------- | -------- |\n",
    "| dim           | 4096     |\n",
    "| n_layers      | 32       |\n",
    "| head_dim      | 128      |\n",
    "| hidden_dim    | 14336    |\n",
    "| n_heads       | 32       |\n",
    "| n_kv_heads    | 8        |\n",
    "| window_size   | 4096     |\n",
    "| context_len   | 8192     |\n",
    "| vocab_size    | 32000    |\n",
    "\n",
    "Compared to LLaMA, it introduces a few changes:\n",
    "\n",
    "* **Sliding Window Attention (SWA)**.\n",
    "* **Rolling Buffer Cache**.\n",
    "* **Pre-fill and Chunking**.\n",
    "\n",
    "In addition, it introduces GQA (which was not used for small models in LLaMA 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "923OAEya9J8p"
   },
   "source": [
    "## 2.1 - Sliding Window Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVzaPm9t9J8p"
   },
   "source": [
    "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To adress this limitation, [Beltagy et al. (2020)](https://arxiv.org/abs/2004.05150) introduces a sliding window. By limiting the attention computation to a fixed-size window, Sliding Window Attention (SWA) reduces the time complexity from quadratic to linear or sublinear. This makes it more efficient for long sequences, as it avoids the need to attend to all tokens simultaneously.\n",
    "\n",
    "With SWA, each token can attend to at most $W$ tokens from the previous layer. However, tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by $W$ tokens. Hence, after k attention layers, information can move forward by up to $k * W$ tokens.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/sliding_window.png\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "SWA exploits stacked layers of transformer to attend information beyond the window size $W$. The hidden state in position $i$ of the layer $k$, $h_{i}$, attends to all hidden states from the previous layer with positions between $i - W$ and $i$. Recursively, $h_{i}$ can access tokens from the input layer at a distance up to $k * W$ tokens. At the last layer, using a window size of 4096, we have a theoretical attention span of approximately 131K tokens. In practice, for a squence length of 16K and $W = 4096$ changes made to [FlashAttention](https://github.com/Dao-AILab/flash-attention) and [xFormers](https://github.com/facebookresearch/xformers) yield a 2x speed improvement over a vanilla attention baseline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dum050EF9J8p"
   },
   "source": [
    "## 2.2 - Rolling Buffer (KV) Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22zsul-z9J8p"
   },
   "source": [
    "### 2.2.1 - Reminder of KV cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O73jO1Nj9J8q"
   },
   "source": [
    "Since the decoder is causal (i.e., the attention of a token only depends on its preceding tokens), at each generation step we are recalculating the same previous token attention, **when we actually just want to calculate the attention for the new token**.\n",
    "\n",
    "This is where KV comes into play. By caching the previous Keys and Values, we can focus on only calculating the attention for the new token.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/kv_caching_2.gif\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kORB4MpW9J8q"
   },
   "source": [
    "### 2.2.1 - Rolling KV cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCqLCgsT9J8q"
   },
   "source": [
    "A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of $W$, and the keys and values for the timestep $i$ are stored in position $i$ % $W$ of the cache. As a result, when the position $i$ is larger than $W$, past value in the cache are overwritten, and the size of the cache stops increasing.\n",
    "\n",
    "To illustrate this, let's say you have a rolling buffer cache with a fixed size of $W = 3$. Now, consider a sequence of timesteps, i = 1, 2, 3, 4, and so on:\n",
    "\n",
    "* For timestep $i$ = 1, the position will be 1 % 3, which is 1. So, the keys and values for timestep 1 will be stored in position 1 of the cache.\n",
    "* For timestep $i$ = 2, the position will be 2 % 3, which is 2. So, the keys and values for timestep 2 will be stored in position 2 of the cache.\n",
    "* For timestep $i$ = 3, the position will be 3 % 3, which is 0. So, the keys and values for timestep 3 will be stored in position 0 of the cache.\n",
    "* For timestep $i$ = 4, the position will be 4 % 3, which is 1. So, the keys and values for timestep 4 will overwrite what's in position 1 of the cache because it's older, and the cache doesn't grow beyond size W.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/rolling_kv_cache.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jhzv0WmJ9J8q"
   },
   "source": [
    "## 2.3 - Pre-fill and Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcR3E3S19J8q"
   },
   "source": [
    "When generating a sequence, we need to predict tokens one-by-one, as each token is conditioned on the previous ones. However, the prompt is known in advance, and we can pre-fill the KV cache with the prompt. If the prompt is very large we can chunk it into smaller pieces, and pre-fill the cache with each chunk. For this purpose, we can select the window size as our chunk size. For each chunk, we thus need to compute the attention over the cache and over the chunk\n",
    "\n",
    "The following Figure shows how the attention mask works over both the cache and the chunk.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/prefill_chunking.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVj2v_Vh9J8q"
   },
   "source": [
    "# 3 - Testing Mistral 7b\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZB5nsL7huURe"
   },
   "source": [
    "The majority of models on Hugging Face are stored and run in 32-bit floating-point (FP32) precision by default. This format provides a wide dynamic range, which helps in maintaining numerical stability during training.\n",
    "\n",
    "Quantization is a technique used to reduce the model size by converting the model weights from a higher precision (like FP32) to lower precision (such as 16-bit floating point (FP16), 8-bit integer (INT8)) or 4-bit (INT4).\n",
    "\n",
    "Since running a 7b parameter requires around 24GB of GPU memory, we are going to use a quantized version of the model. More specifically:\n",
    "* 8-bit version (about 7GB of memory during inference)\n",
    "* 4-bit version (about 3.5GB of memory during inference)\n",
    "\n",
    "-----\n",
    "\n",
    "> **Warning:** As of 11/10/2024, `bitsandbytes` is only supported on CUDA GPU hardware. Support for AMD GPUs and M1 chips (MacOS) is coming soon.\n",
    "\n",
    "-----\n",
    "\n",
    "#### When does quantization occur?\n",
    "\n",
    "An interesting thing about quantization is that happens AFTER we download the model, so we can try different quantizations without needing to have duplicated model parameters in the disk.\n",
    "\n",
    "* Quantization, such as 8-bit (using bitsandbytes or similar libraries), is applied locally after the model has been downloaded and loaded into memory.\n",
    "* The full-precision model is initially loaded, and then the quantization library reduces the precision of the weights from 32-bit or 16-bit floating-point down to 8-bit or 4-bit integers, depending on your configuration.\n",
    "* This process happens in-memory, so there is no need to re-download the model each time you use a different quantization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfklSWjWx26r"
   },
   "outputs": [],
   "source": [
    "#!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvFZPRGMx6nj"
   },
   "source": [
    "> Google Colab: Remember to restart the session after installing the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j_NFbfF7x-Gu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import device\n",
    "from huggingface_hub import login\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qRbYqQ56x_zA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token not found. Make sure it's set in the .zshrc file.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Load the Hugging Face token from the environment variable\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Log in to Hugging Face\n",
    "if hf_token:\n",
    "    login(hf_token)\n",
    "else:\n",
    "    print(\"Hugging Face token not found. Make sure it's set in the .zshrc file.\")\n",
    "    login(\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLiwslchzl4A"
   },
   "source": [
    "## 3.1 - 8-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "AvmXCKQpzs5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06538adf799e428b936fe59b60a0f025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the quantization configuration for 8-bit\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Change to 8-bit quantization\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ct4FgpbD0Fo5"
   },
   "outputs": [],
   "source": [
    "# tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "# model_4bit.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "print(model.get_input_embeddings())\n",
    "\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSa1HQ9Nztjs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device (usually device 0)\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # Memory allocated on the GPU (in bytes)\n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "    # Memory reserved by PyTorch's memory allocator (in bytes)\n",
    "    reserved_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "    # Print the memory information (in MB for better readability)\n",
    "    print(f\"Allocated memory: {allocated_memory / (1024 ** 2)} MB\")\n",
    "    print(f\"Reserved memory: {reserved_memory / (1024 ** 2)} MB\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZzxpnK90Anx"
   },
   "outputs": [],
   "source": [
    "# Prepare a prompt\n",
    "prompt = \"<s>[INST] What is your favourite condiment? [/INST]\"\n",
    "# prompt = \"What is your favourite condiment?\"\n",
    "\n",
    "# Tokenize the input prompt with padding\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure the attention mask is set properly\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "# Generate response from the model\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    ")\n",
    "\n",
    "# Decode the generated tokens to text\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3fWaPn6uJtX"
   },
   "source": [
    "## 3.2 - 4-bit quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N87HGprPSia-"
   },
   "source": [
    "For more information about INT4 quantization: https://arxiv.org/pdf/2301.12017\n",
    "\n",
    "> **Warning:** 4-bit quantization is much more UNSTABLE than 8-bit, I had issues trying to execute the model\n",
    "\n",
    "\n",
    "### Breakdown of the quantization configuration\n",
    "\n",
    "- **`load_in_4bit=True`**:\n",
    "  This parameter indicates that the model should be loaded using 4-bit quantization. This significantly reduces the model size compared to the default 32-bit or even 16-bit floating-point formats, allowing for faster inference and lower memory usage.\n",
    "\n",
    "- **`bnb_4bit_compute_dtype=torch.float16`**:\n",
    "  This specifies the data type used for computations during inference. Setting this to `torch.float16` means that even though the model weights are quantized to 4 bits, the computations (such as activations and gradients, if applicable) will be performed in 16-bit floating-point (FP16). This can help maintain accuracy while leveraging the reduced precision of the weights.\n",
    "\n",
    "- **`bnb_4bit_quant_type=\"nf4\"`**:\n",
    "  This specifies the quantization method used for 4-bit representation. The `\"nf4\"` quantization type refers to “normalized float 4,” which is a specific approach to quantizing weights. It retains more information about the distribution of the weights compared to simpler quantization methods. This can lead to better model performance after quantization.\n",
    "\n",
    "- **`bnb_4bit_use_double_quant=True`**:\n",
    "  When set to `True`, this parameter enables double quantization. Double quantization can help reduce the quantization error by quantizing the quantized values again, providing more stability and accuracy. It essentially applies quantization twice to achieve better performance and accuracy, particularly in complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:52:59.262231Z",
     "start_time": "2024-10-11T13:52:58.971986Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "6b4b24a8caef466793d611f6482ddbf9",
      "8a1cc15be660497794bbf464d260d31c",
      "fa4ee3d459324eeea36136af48691d79",
      "a356124f42634d1f934b5abd3b575be9",
      "d8af957d345e4f66866b424c498c4ac8",
      "a52f6c73f6844b02818932d3388a91ee",
      "a5d6894b45ad4c509c83c51e9f80de96",
      "5dd37e92116941e9b67f4f2c6fc137d7",
      "ba2eb751da5441a991db1acc726ef98d",
      "99d084092207460495eb445082606531",
      "4d4fa1b1dd6144f8853945d08cb39abb"
     ]
    },
    "id": "p9bxMS399J8r",
    "outputId": "be208b21-5ec6-4c4d-d4d3-5b70cdb81920"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4b24a8caef466793d611f6482ddbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1B1I74zEebl",
    "outputId": "94ae089f-2332-4603-bc43-d49017ed50c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your favourite condiment? \n"
     ]
    }
   ],
   "source": [
    "# Prepare a prompt\n",
    "prompt = \"<s>[INST] What is your favourite condiment? [/INST]\"\n",
    "\n",
    "# Tokenize the input prompt with padding\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure the attention mask is set properly\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "# Generate response from the model\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    ")\n",
    "\n",
    "# Decode the generated tokens to text\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcOqBWMdO7TF",
    "outputId": "c3d29e4c-7ff3-4b91-de23-0edeae9c7804"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 23325,     2]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Basic HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/main/en/chat_templating#advanced-tool-use--function-calling\n",
    "\n",
    "\n",
    "In order for this to work correctly, you should write your functions in the format above, so that they can be parsed correctly as tools. Specifically, you should follow these rules:\n",
    "\n",
    "1. **The function should have a descriptive name.**\n",
    "\n",
    "2. **Every argument must have a type hint.**\n",
    "\n",
    "3. **The function must have a docstring in the standard Google style**, which includes:\n",
    "   - An initial function description.\n",
    "   - An `Args:` block that describes the arguments, unless the function does not have any arguments.\n",
    "\n",
    "4. **Do not include types in the `Args:` block.**\n",
    "   - Write `a: The first number to multiply`, **not** `a (int): The first number to multiply`.\n",
    "   - Type hints should go in the function header instead.\n",
    "\n",
    "5. **The function can have a return type and a `Returns:` block in the docstring**, but these are optional because most tool-use models ignore them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_temperature(location: str, unit: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the current temperature at a location.\n",
    "    \n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, Country\"\n",
    "        unit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"])\n",
    "    Returns:\n",
    "        The current temperature at the specified location in the specified units, as a float.\n",
    "    \"\"\"\n",
    "    return 22.  # A real function should probably actually get the temperature!\n",
    "\n",
    "def get_current_wind_speed(location: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the current wind speed in km/h at a given location.\n",
    "    \n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, Country\"\n",
    "    Returns:\n",
    "        The current wind speed at the given location in km/h, as a float.\n",
    "    \"\"\"\n",
    "    return 6.  # A real function should probably actually get the wind speed!\n",
    "\n",
    "tools = [get_current_temperature, get_current_wind_speed]\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries. You should reply with the unit used in the queried location.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tools=tools, \n",
    "    add_generation_prompt=True, \n",
    "    return_dict=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TOOL_CALLS] [{\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}]</s>\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(**inputs, max_new_tokens=256)\n",
    "print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Using Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html\n",
    "\n",
    "* `poetry add langchain-huggingface`\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "IMPORTANT: The `ChatHuggingFace` requires the tokenizer to be passed as argument\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "IMPORTANT: From the following issue, it seems that Function calling does not work with `HuggingFacePipeline`\n",
    "\n",
    "https://github.com/langchain-ai/langchain/issues/24430\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, tokenizer=llm.pipeline.tokenizer, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple chain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe user said: What is the capital of France?. How would you respond?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "<s>[INST] The user said: What is the capital of France?. How would you respond?[/INST] The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary classes from LangChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Step 2: Define a prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],  # Variable that will be passed as input\n",
    "    template=\"The user said: {user_input}. How would you respond?\"\n",
    ")\n",
    "\n",
    "# Step 3: Set up the chain\n",
    "chat_chain = LLMChain(llm=chat, prompt=prompt, verbose=True)\n",
    "\n",
    "# Step 4: Run the chain with a test input\n",
    "test_input = \"What is the capital of France?\"\n",
    "response = chat_chain.run(user_input=test_input)\n",
    "\n",
    "# Print the output\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "chat_with_tools = chat.bind_tools(tools)\n",
    "\n",
    "query = \"What is 3.012314 * 12.7764? (Use tools)\"\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "ai_msg = chat_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<s>[INST] What is 3.012314 * 12.7764? (Use tools)[/INST] To calculate the product of 3.012314 and 12.7764, you can use a calculator. Here's the result:\\n\\n3.012314 * 12.7764 = 38.3848639284\", additional_kwargs={}, response_metadata={}, id='run-d66ccce5-f349-4df3-af68-2ef9d1e06826-0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4d4fa1b1dd6144f8853945d08cb39abb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5dd37e92116941e9b67f4f2c6fc137d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b4b24a8caef466793d611f6482ddbf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a1cc15be660497794bbf464d260d31c",
       "IPY_MODEL_fa4ee3d459324eeea36136af48691d79",
       "IPY_MODEL_a356124f42634d1f934b5abd3b575be9"
      ],
      "layout": "IPY_MODEL_d8af957d345e4f66866b424c498c4ac8"
     }
    },
    "8a1cc15be660497794bbf464d260d31c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a52f6c73f6844b02818932d3388a91ee",
      "placeholder": "​",
      "style": "IPY_MODEL_a5d6894b45ad4c509c83c51e9f80de96",
      "value": "Loading checkpoint shards:  67%"
     }
    },
    "99d084092207460495eb445082606531": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a356124f42634d1f934b5abd3b575be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99d084092207460495eb445082606531",
      "placeholder": "​",
      "style": "IPY_MODEL_4d4fa1b1dd6144f8853945d08cb39abb",
      "value": " 2/3 [00:57&lt;00:28, 28.67s/it]"
     }
    },
    "a52f6c73f6844b02818932d3388a91ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5d6894b45ad4c509c83c51e9f80de96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba2eb751da5441a991db1acc726ef98d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8af957d345e4f66866b424c498c4ac8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa4ee3d459324eeea36136af48691d79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dd37e92116941e9b67f4f2c6fc137d7",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ba2eb751da5441a991db1acc726ef98d",
      "value": 2
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
