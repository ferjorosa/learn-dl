{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBJyDr-C9J8m"
   },
   "source": [
    "# MISTRAL-7B\n",
    "\n",
    "List of main relevant papers:\n",
    "* [Jiang et al. (2023)](https://arxiv.org/abs/2310.06825). Mistral 7B\n",
    "\n",
    "List of main relevant blogs:\n",
    "* [Mistral (2023)](https://www.pinecone.io/learn/image-search/). Mistral 7B. The best 7B model to date, Apache 2.0\n",
    "\n",
    "List of main relevant Youtube videos:\n",
    "* [PromptEngineering (2023)](https://www.youtube.com/watch?v=z4wPiallZcI). Mistral 7B -The Most Powerful 7B Model Yet\n",
    "\n",
    "Model: https://huggingface.co/mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9C0Tkn819J8o"
   },
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxfHS4Lk9J8o"
   },
   "source": [
    "In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model performance often necessitates an escalation in model size. However, this scaling tends to increase computational costs and inference latency, thereby raising barriers to deployment in practical, real-world scenarios. In this context, the search for balanced models delivering both high-level performance and efficiency becomes critically essential.\n",
    "\n",
    "Mistral 7B, demonstrates that a carefully designed language model can deliver high performance while maintaining an efficient inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [Touvron et al., 2023](https://arxiv.org/pdf/2307.09288.pdf)) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [Touvron et al., 2023](https://arxiv.org/pdf/2302.13971.pdf)) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B ([Rozière et al., 2023](https://arxiv.org/abs/2308.12950)), without sacrificing performance on non-code related benchmarks.\n",
    "\n",
    "\n",
    "Mistral 7B leverages grouped-query attention (GQA, [Ainslie et al., 2023](https://arxiv.org/abs/2305.13245)), and sliding window attention (SWA, [Beltagy et al., 2020](https://arxiv.org/abs/2004.05150)). GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs.\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** These attention mechanisms are aimed towards improving the efficiency of inference and the but should not improve the general model performance just by themselves. The intuition behind this performance improve is the propietary data that was used to train the model, probably much cleaner than most open-source data sources\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/results_bar.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The benchmarks are categorized by their themes:\n",
    "\n",
    "* **Commonsense Reasoning:** 0-shot average of Hellaswag, Winogrande, PIQA, SIQA, OpenbookQA, ARC-Easy, ARC-Challenge, and CommonsenseQA.\n",
    "* **World Knowledge:** 5-shot average of NaturalQuestions and TriviaQA.\n",
    "* **Reading Comprehension:** 0-shot average of BoolQ and QuAC.\n",
    "* **Math:** Average of 8-shot GSM8K with maj@8 and 4-shot MATH with maj@4\n",
    "* **Code:** Average of 0-shot Humaneval and 3-shot MBPP\n",
    "* **Popular aggregated results:** 5-shot MMLU, 3-shot BBH, and 3-5-shot AGI Eval (English multiple-choice questions only)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/results_table.png\" width=\"900\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "An interesting metric to compare how models fare in the cost/performance plane is to compute “equivalent model sizes”. On reasoning, comprehension and STEM reasoning (MMLU), M**istral 7B performs equivalently to a Llama 2 that would be more than 3x its size**. This is as much saved in memory and gained in throughput.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/results_effective_sizes.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which restricts the amount of knowledge it can compress)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0sST8Cd9J8p"
   },
   "source": [
    "# 2 - Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXdZLRHH9J8p"
   },
   "source": [
    "Mistral 7B is based on an autoregressive Transformer architecture. The main paramters of the architecture are summarized on the following table:\n",
    "\n",
    "| Parameter     | Value    |\n",
    "|-------------- | -------- |\n",
    "| dim           | 4096     |\n",
    "| n_layers      | 32       |\n",
    "| head_dim      | 128      |\n",
    "| hidden_dim    | 14336    |\n",
    "| n_heads       | 32       |\n",
    "| n_kv_heads    | 8        |\n",
    "| window_size   | 4096     |\n",
    "| context_len   | 8192     |\n",
    "| vocab_size    | 32000    |\n",
    "\n",
    "Compared to LLaMA, it introduces a few changes:\n",
    "\n",
    "* **Sliding Window Attention (SWA)**.\n",
    "* **Rolling Buffer Cache**.\n",
    "* **Pre-fill and Chunking**.\n",
    "\n",
    "In addition, it introduces GQA (which was not used for small models in LLaMA 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "923OAEya9J8p"
   },
   "source": [
    "## 2.1 - Sliding Window Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVzaPm9t9J8p"
   },
   "source": [
    "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To adress this limitation, [Beltagy et al. (2020)](https://arxiv.org/abs/2004.05150) introduces a sliding window. By limiting the attention computation to a fixed-size window, Sliding Window Attention (SWA) reduces the time complexity from quadratic to linear or sublinear. This makes it more efficient for long sequences, as it avoids the need to attend to all tokens simultaneously.\n",
    "\n",
    "With SWA, each token can attend to at most $W$ tokens from the previous layer. However, tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by $W$ tokens. Hence, after k attention layers, information can move forward by up to $k * W$ tokens.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/sliding_window.png\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "SWA exploits stacked layers of transformer to attend information beyond the window size $W$. The hidden state in position $i$ of the layer $k$, $h_{i}$, attends to all hidden states from the previous layer with positions between $i - W$ and $i$. Recursively, $h_{i}$ can access tokens from the input layer at a distance up to $k * W$ tokens. At the last layer, using a window size of 4096, we have a theoretical attention span of approximately 131K tokens. In practice, for a squence length of 16K and $W = 4096$ changes made to [FlashAttention](https://github.com/Dao-AILab/flash-attention) and [xFormers](https://github.com/facebookresearch/xformers) yield a 2x speed improvement over a vanilla attention baseline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dum050EF9J8p"
   },
   "source": [
    "## 2.2 - Rolling Buffer (KV) Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22zsul-z9J8p"
   },
   "source": [
    "### 2.2.1 - Reminder of KV cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O73jO1Nj9J8q"
   },
   "source": [
    "Since the decoder is causal (i.e., the attention of a token only depends on its preceding tokens), at each generation step we are recalculating the same previous token attention, **when we actually just want to calculate the attention for the new token**.\n",
    "\n",
    "This is where KV comes into play. By caching the previous Keys and Values, we can focus on only calculating the attention for the new token.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/kv_caching_2.gif\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kORB4MpW9J8q"
   },
   "source": [
    "### 2.2.1 - Rolling KV cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCqLCgsT9J8q"
   },
   "source": [
    "A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of $W$, and the keys and values for the timestep $i$ are stored in position $i$ % $W$ of the cache. As a result, when the position $i$ is larger than $W$, past value in the cache are overwritten, and the size of the cache stops increasing.\n",
    "\n",
    "To illustrate this, let's say you have a rolling buffer cache with a fixed size of $W = 3$. Now, consider a sequence of timesteps, i = 1, 2, 3, 4, and so on:\n",
    "\n",
    "* For timestep $i$ = 1, the position will be 1 % 3, which is 1. So, the keys and values for timestep 1 will be stored in position 1 of the cache.\n",
    "* For timestep $i$ = 2, the position will be 2 % 3, which is 2. So, the keys and values for timestep 2 will be stored in position 2 of the cache.\n",
    "* For timestep $i$ = 3, the position will be 3 % 3, which is 0. So, the keys and values for timestep 3 will be stored in position 0 of the cache.\n",
    "* For timestep $i$ = 4, the position will be 4 % 3, which is 1. So, the keys and values for timestep 4 will overwrite what's in position 1 of the cache because it's older, and the cache doesn't grow beyond size W.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/rolling_kv_cache.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jhzv0WmJ9J8q"
   },
   "source": [
    "## 2.3 - Pre-fill and Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcR3E3S19J8q"
   },
   "source": [
    "When generating a sequence, we need to predict tokens one-by-one, as each token is conditioned on the previous ones. However, the prompt is known in advance, and we can pre-fill the KV cache with the prompt. If the prompt is very large we can chunk it into smaller pieces, and pre-fill the cache with each chunk. For this purpose, we can select the window size as our chunk size. For each chunk, we thus need to compute the attention over the cache and over the chunk\n",
    "\n",
    "The following Figure shows how the attention mask works over both the cache and the chunk.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/prefill_chunking.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "metadata": {
    "id": "IVj2v_Vh9J8q"
   },
   "cell_type": "markdown",
   "source": [
    "# 3 - Testing Mistral 7b\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The majority of models on Hugging Face are stored and run in 32-bit floating-point (FP32) precision by default. This format provides a wide dynamic range, which helps in maintaining numerical stability during training.\n",
    "\n",
    "Quantization is a technique used to reduce the model size by converting the model weights from a higher precision (like FP32) to lower precision (such as 16-bit floating point (FP16), 8-bit integer (INT8)) or 4-bit (INT4).\n",
    "\n",
    "Since running a 7b parameter requires around 24GB of GPU memory, we are going to use a quantized version of the model. More specifically:\n",
    "* 8-bit version (about 7GB of memory during inference)\n",
    "* 4-bit version (about 3.5GB of memory during inference)\n",
    "\n",
    "-----\n",
    "\n",
    "> **Warning:** As of 11/10/2024, `bitsandbytes` is only supported on CUDA GPU hardware. Support for AMD GPUs and M1 chips (MacOS) is coming soon.\n",
    "\n",
    "-----\n",
    "\n",
    "#### When does quantization occur?\n",
    "\n",
    "An interesting thing about quantization is that happens AFTER we download the model, so we can try different quantizations without needing to have duplicated model parameters in the disk.\n",
    "\n",
    "* Quantization, such as 8-bit (using bitsandbytes or similar libraries), is applied locally after the model has been downloaded and loaded into memory.\n",
    "* The full-precision model is initially loaded, and then the quantization library reduces the precision of the weights from 32-bit or 16-bit floating-point down to 8-bit or 4-bit integers, depending on your configuration.\n",
    "* This process happens in-memory, so there is no need to re-download the model each time you use a different quantization scheme."
   ],
   "metadata": {
    "id": "ZB5nsL7huURe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U bitsandbytes"
   ],
   "metadata": {
    "id": "sfklSWjWx26r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Google Colab: Remember to restart the session after installing the package"
   ],
   "metadata": {
    "id": "jvFZPRGMx6nj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import device\n",
    "from huggingface_hub import login\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ],
   "metadata": {
    "id": "j_NFbfF7x-Gu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "login()"
   ],
   "metadata": {
    "id": "qRbYqQ56x_zA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8-bit quantization"
   ],
   "metadata": {
    "id": "gLiwslchzl4A"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the quantization configuration for 8-bit\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Change to 8-bit quantization\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)"
   ],
   "metadata": {
    "id": "AvmXCKQpzs5e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "# model_4bit.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "print(model.get_input_embeddings())\n",
    "\n",
    "print(tokenizer.special_tokens_map)"
   ],
   "metadata": {
    "id": "ct4FgpbD0Fo5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device (usually device 0)\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # Memory allocated on the GPU (in bytes)\n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "\n",
    "    # Memory reserved by PyTorch's memory allocator (in bytes)\n",
    "    reserved_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "    # Print the memory information (in MB for better readability)\n",
    "    print(f\"Allocated memory: {allocated_memory / (1024 ** 2)} MB\")\n",
    "    print(f\"Reserved memory: {reserved_memory / (1024 ** 2)} MB\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")\n"
   ],
   "metadata": {
    "id": "bSa1HQ9Nztjs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare a prompt\n",
    "prompt = \"<s>[INST] What is your favourite condiment? [/INST]\"\n",
    "# prompt = \"What is your favourite condiment?\"\n",
    "\n",
    "# Tokenize the input prompt with padding\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure the attention mask is set properly\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "# Generate response from the model\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    ")\n",
    "\n",
    "# Decode the generated tokens to text\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ],
   "metadata": {
    "id": "KZzxpnK90Anx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4-bit quantization"
   ],
   "metadata": {
    "id": "H3fWaPn6uJtX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For more information about INT4 quantization: https://arxiv.org/pdf/2301.12017\n",
    "\n",
    "> **Warning:** 4-bit quantization is much more UNSTABLE than 8-bit, I had issues trying to execute the model\n",
    "\n",
    "\n",
    "### Breakdown of the quantization configuration\n",
    "\n",
    "- **`load_in_4bit=True`**:\n",
    "  This parameter indicates that the model should be loaded using 4-bit quantization. This significantly reduces the model size compared to the default 32-bit or even 16-bit floating-point formats, allowing for faster inference and lower memory usage.\n",
    "\n",
    "- **`bnb_4bit_compute_dtype=torch.float16`**:\n",
    "  This specifies the data type used for computations during inference. Setting this to `torch.float16` means that even though the model weights are quantized to 4 bits, the computations (such as activations and gradients, if applicable) will be performed in 16-bit floating-point (FP16). This can help maintain accuracy while leveraging the reduced precision of the weights.\n",
    "\n",
    "- **`bnb_4bit_quant_type=\"nf4\"`**:\n",
    "  This specifies the quantization method used for 4-bit representation. The `\"nf4\"` quantization type refers to “normalized float 4,” which is a specific approach to quantizing weights. It retains more information about the distribution of the weights compared to simpler quantization methods. This can lead to better model performance after quantization.\n",
    "\n",
    "- **`bnb_4bit_use_double_quant=True`**:\n",
    "  When set to `True`, this parameter enables double quantization. Double quantization can help reduce the quantization error by quantizing the quantized values again, providing more stability and accuracy. It essentially applies quantization twice to achieve better performance and accuracy, particularly in complex models."
   ],
   "metadata": {
    "id": "N87HGprPSia-"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:52:59.262231Z",
     "start_time": "2024-10-11T13:52:58.971986Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "6b4b24a8caef466793d611f6482ddbf9",
      "8a1cc15be660497794bbf464d260d31c",
      "fa4ee3d459324eeea36136af48691d79",
      "a356124f42634d1f934b5abd3b575be9",
      "d8af957d345e4f66866b424c498c4ac8",
      "a52f6c73f6844b02818932d3388a91ee",
      "a5d6894b45ad4c509c83c51e9f80de96",
      "5dd37e92116941e9b67f4f2c6fc137d7",
      "ba2eb751da5441a991db1acc726ef98d",
      "99d084092207460495eb445082606531",
      "4d4fa1b1dd6144f8853945d08cb39abb"
     ]
    },
    "id": "p9bxMS399J8r",
    "outputId": "be208b21-5ec6-4c4d-d4d3-5b70cdb81920"
   },
   "cell_type": "code",
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b4b24a8caef466793d611f6482ddbf9"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare a prompt\n",
    "prompt = \"<s>[INST] What is your favourite condiment? [/INST]\"\n",
    "\n",
    "# Tokenize the input prompt with padding\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure the attention mask is set properly\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "# Generate response from the model\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    ")\n",
    "\n",
    "# Decode the generated tokens to text\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1B1I74zEebl",
    "outputId": "94ae089f-2332-4603-bc43-d49017ed50c5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "What is your favourite condiment? \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "outputs"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcOqBWMdO7TF",
    "outputId": "c3d29e4c-7ff3-4b91-de23-0edeae9c7804"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[    1, 23325,     2]])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "6b4b24a8caef466793d611f6482ddbf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a1cc15be660497794bbf464d260d31c",
       "IPY_MODEL_fa4ee3d459324eeea36136af48691d79",
       "IPY_MODEL_a356124f42634d1f934b5abd3b575be9"
      ],
      "layout": "IPY_MODEL_d8af957d345e4f66866b424c498c4ac8"
     }
    },
    "8a1cc15be660497794bbf464d260d31c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a52f6c73f6844b02818932d3388a91ee",
      "placeholder": "​",
      "style": "IPY_MODEL_a5d6894b45ad4c509c83c51e9f80de96",
      "value": "Loading checkpoint shards:  67%"
     }
    },
    "fa4ee3d459324eeea36136af48691d79": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dd37e92116941e9b67f4f2c6fc137d7",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ba2eb751da5441a991db1acc726ef98d",
      "value": 2
     }
    },
    "a356124f42634d1f934b5abd3b575be9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99d084092207460495eb445082606531",
      "placeholder": "​",
      "style": "IPY_MODEL_4d4fa1b1dd6144f8853945d08cb39abb",
      "value": " 2/3 [00:57&lt;00:28, 28.67s/it]"
     }
    },
    "d8af957d345e4f66866b424c498c4ac8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a52f6c73f6844b02818932d3388a91ee": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5d6894b45ad4c509c83c51e9f80de96": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5dd37e92116941e9b67f4f2c6fc137d7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba2eb751da5441a991db1acc726ef98d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "99d084092207460495eb445082606531": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d4fa1b1dd6144f8853945d08cb39abb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
