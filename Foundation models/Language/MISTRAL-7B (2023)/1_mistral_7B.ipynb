{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISTRAL-7B\n",
    "\n",
    "List of main relevant papers:\n",
    "* [Jiang et al. (2023)](https://arxiv.org/abs/2310.06825). Mistral 7B\n",
    "\n",
    "List of main relevant blogs:\n",
    "* [Mistral (2023)](https://www.pinecone.io/learn/image-search/). Mistral 7B. The best 7B model to date, Apache 2.0\n",
    "\n",
    "List of main relevant Youtube videos:\n",
    "* [PromptEngineering (2023)](https://www.youtube.com/watch?v=z4wPiallZcI). Mistral 7B -The Most Powerful 7B Model Yet\n",
    "\n",
    "Model: https://huggingface.co/mistralai/Mistral-7B-v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model performance often necessitates an escalation in model size. However, this scaling tends to increase computational costs and inference latency, thereby raising barriers to deployment in practical, real-world scenarios. In this context, the search for balanced models delivering both high-level performance and efficiency becomes critically essential. \n",
    "\n",
    "Mistral 7B, demonstrates that a carefully designed language model can deliver high performance while maintaining an efficient inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [Touvron et al., 2023](https://arxiv.org/pdf/2307.09288.pdf)) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [Touvron et al., 2023](https://arxiv.org/pdf/2302.13971.pdf)) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B ([Rozière et al., 2023](https://arxiv.org/abs/2308.12950)), without sacrificing performance on non-code related benchmarks.\n",
    "\n",
    "\n",
    "Mistral 7B leverages grouped-query attention (GQA, [Ainslie et al., 2023](https://arxiv.org/abs/2305.13245)), and sliding window attention (SWA, [Beltagy et al., 2020](https://arxiv.org/abs/2004.05150)). GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs.\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** These attention mechanisms are aimed towards improving the efficiency of inference and the but should not improve the general model performance just by themselves. The intuition behind this performance improve is the propietary data that was used to train the model, probably much cleaner than most open-source data sources\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/results_bar.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The benchmarks are categorized by their themes:\n",
    "\n",
    "* **Commonsense Reasoning:** 0-shot average of Hellaswag, Winogrande, PIQA, SIQA, OpenbookQA, ARC-Easy, ARC-Challenge, and CommonsenseQA.\n",
    "* **World Knowledge:** 5-shot average of NaturalQuestions and TriviaQA.\n",
    "* **Reading Comprehension:** 0-shot average of BoolQ and QuAC.\n",
    "* **Math:** Average of 8-shot GSM8K with maj@8 and 4-shot MATH with maj@4\n",
    "* **Code:** Average of 0-shot Humaneval and 3-shot MBPP\n",
    "* **Popular aggregated results:** 5-shot MMLU, 3-shot BBH, and 3-5-shot AGI Eval (English multiple-choice questions only)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/results_table.png\" width=\"900\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "An interesting metric to compare how models fare in the cost/performance plane is to compute “equivalent model sizes”. On reasoning, comprehension and STEM reasoning (MMLU), M**istral 7B performs equivalently to a Llama 2 that would be more than 3x its size**. This is as much saved in memory and gained in throughput. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/results_effective_sizes.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which restricts the amount of knowledge it can compress)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral 7B is based on an autoregressive Transformer architecture. The main paramters of the architecture are summarized on the following table:\n",
    "\n",
    "| Parameter     | Value    |\n",
    "|-------------- | -------- |\n",
    "| dim           | 4096     |\n",
    "| n_layers      | 32       |\n",
    "| head_dim      | 128      |\n",
    "| hidden_dim    | 14336    |\n",
    "| n_heads       | 32       |\n",
    "| n_kv_heads    | 8        |\n",
    "| window_size   | 4096     |\n",
    "| context_len   | 8192     |\n",
    "| vocab_size    | 32000    |\n",
    "\n",
    "Compared to LLaMA, it introduces a few changes:\n",
    "\n",
    "* **Sliding Window Attention (SWA)**.\n",
    "* **Rolling Buffer Cache**.\n",
    "* **Pre-fill and Chunking**.\n",
    "\n",
    "In addition, it introduces GQA (which was not used for small models in LLaMA 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Sliding Window Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To adress this limitation, [Beltagy et al. (2020)](https://arxiv.org/abs/2004.05150) introduces a sliding window. By limiting the attention computation to a fixed-size window, Sliding Window Attention (SWA) reduces the time complexity from quadratic to linear or sublinear. This makes it more efficient for long sequences, as it avoids the need to attend to all tokens simultaneously.\n",
    "\n",
    "With SWA, each token can attend to at most $W$ tokens from the previous layer. However, tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by $W$ tokens. Hence, after k attention layers, information can move forward by up to $k * W$ tokens.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/sliding_window.png\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "SWA exploits stacked layers of transformer to attend information beyond the window size $W$. The hidden state in position $i$ of the layer $k$, $h_{i}$, attends to all hidden states from the previous layer with positions between $i - W$ and $i$. Recursively, $h_{i}$ can access tokens from the input layer at a distance up to $k * W$ tokens. At the last layer, using a window size of 4096, we have a theoretical attention span of approximately 131K tokens. In practice, for a squence length of 16K and $W = 4096$ changes made to [FlashAttention](https://github.com/Dao-AILab/flash-attention) and [xFormers](https://github.com/facebookresearch/xformers) yield a 2x speed improvement over a vanilla attention baseline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Rolling Buffer (KV) Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 - Reminder of KV cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the decoder is causal (i.e., the attention of a token only depends on its preceding tokens), at each generation step we are recalculating the same previous token attention, **when we actually just want to calculate the attention for the new token**.\n",
    "\n",
    "This is where KV comes into play. By caching the previous Keys and Values, we can focus on only calculating the attention for the new token.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/kv_caching_2.gif\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 - Rolling KV cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of $W$, and the keys and values for the timestep $i$ are stored in position $i$ % $W$ of the cache. As a result, when the position $i$ is larger than $W$, past value in the cache are overwritten, and the size of the cache stops increasing. \n",
    "\n",
    "To illustrate this, let's say you have a rolling buffer cache with a fixed size of $W = 3$. Now, consider a sequence of timesteps, i = 1, 2, 3, 4, and so on:\n",
    "\n",
    "* For timestep $i$ = 1, the position will be 1 % 3, which is 1. So, the keys and values for timestep 1 will be stored in position 1 of the cache.\n",
    "* For timestep $i$ = 2, the position will be 2 % 3, which is 2. So, the keys and values for timestep 2 will be stored in position 2 of the cache.\n",
    "* For timestep $i$ = 3, the position will be 3 % 3, which is 0. So, the keys and values for timestep 3 will be stored in position 0 of the cache.\n",
    "* For timestep $i$ = 4, the position will be 4 % 3, which is 1. So, the keys and values for timestep 4 will overwrite what's in position 1 of the cache because it's older, and the cache doesn't grow beyond size W.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/rolling_kv_cache.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Pre-fill and Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating a sequence, we need to predict tokens one-by-one, as each token is conditioned on the previous ones. However, the prompt is known in advance, and we can pre-fill the KV cache with the prompt. If the prompt is very large we can chunk it into smaller pieces, and pre-fill the cache with each chunk. For this purpose, we can select the window size as our chunk size. For each chunk, we thus need to compute the attention over the cache and over the chunk\n",
    "\n",
    "The following Figure shows how the attention mask works over both the cache and the chunk.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/prefill_chunking.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
