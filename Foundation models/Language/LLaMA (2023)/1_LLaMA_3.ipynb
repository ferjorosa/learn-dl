{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Login to HuggingFace\n",
    "\n",
    "We do this to get access to Llama models, we need permission from meta and thus once granted we need to \"identify\" using our HuggingFace account\n",
    "\n",
    "Model requirements\n",
    "\n",
    "* Llama 3.1: https://llamaimodel.com/requirements/\n",
    "* LLama 3.2: https://llamaimodel.com/requirements-3-2/"
   ],
   "id": "fe4ca2420baba2f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:20:56.287271Z",
     "start_time": "2024-10-11T13:20:56.284068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login"
   ],
   "id": "4c98484360c8a770",
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-11T13:20:49.375478Z",
     "start_time": "2024-10-11T13:20:43.277614Z"
    }
   },
   "source": [
    "# Check if a GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the Hugging Face token from the environment variable\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Log in to Hugging Face\n",
    "if hf_token:\n",
    "    login(hf_token)\n",
    "else:\n",
    "    print(\"Hugging Face token not found. Make sure it's set in the .zshrc file.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/fernando/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Llama model",
   "id": "5fcc96a75b4c1686"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T10:52:06.835265Z",
     "start_time": "2024-10-11T10:51:16.852037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ],
   "id": "9105113ac1d2f9be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "390a5ee662244e1daf832ca5499ddeef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00012c19495d4480b6bc1c06a1d88649"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2c9ca2fe1d84bb8beacf721597d9e05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e76dc642cdb4fff8e9e209ecbccaf18"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a759b3540e4046818e7678d4ae326bd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.hf.co/repos/46/00/46001ae0478831903029d3dfca642974a652dfe8c5a4a525a48a3c700a7a99dd/68a2e4be76fa709455a60272fba8e512c02d81c46e6c671cc9449e374fd6809a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1728903082&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyODkwMzA4Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQ2LzAwLzQ2MDAxYWUwNDc4ODMxOTAzMDI5ZDNkZmNhNjQyOTc0YTY1MmRmZThjNWE0YTUyNWE0OGEzYzcwMGE3YTk5ZGQvNjhhMmU0YmU3NmZhNzA5NDU1YTYwMjcyZmJhOGU1MTJjMDJkODFjNDZlNmM2NzFjYzk0NDllMzc0ZmQ2ODA5YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=KSol4Z-twYs07k0IfRumP79QEHV0MSoNoTTH-H-jVShJal56yUdq31%7E2PNKx3yShWO7xZVq5oT%7EEVa6J-7sGpObWNTeyNhgQzhbMKf2qlRmPuMK756lGpG3Sk8JvK0DBgxZ1vkBQxldHdToFjDd-zzVNiZiwExsnpYUnoXoTZjMA5YEBdpjJYO7F436Z4uuVqIO6uN-nRVGVET-yUg6vJ6gq6RFJOLeosEFZxff%7EvlCeGTxY1n7bni6SgqNsfMEheKXgCRtE8RnAfSRvJWEpnpdFr5nj0qDsDZR7nAx9UkiQZZxxKIwRRURbGljFGkX7JA8uHNl5UYzfz9xVuYJqqg__&Key-Pair-Id=K24J24Z295AEI9: [SSL] record layer failure (_ssl.c:2548)\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:  99%|#########8| 2.44G/2.47G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46d4a005c4dc485d9718a0088a7fbd25"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d3aa82832dd408e8defe4ca8f56c983"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Notes on Embedding Dimension and Tokenizer's Vocabulary\n",
    "\n",
    "The LLaMA 3.2 tokenizer includes several special tokens. Here are some key examples:\n",
    "\n",
    "```\n",
    "128000: AddedToken(\"<|begin_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    " 128001: AddedToken(\"<|end_of_text|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    " 128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    " 128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    " 128004: AddedToken(\"<|finetune_right_pad_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    " 128005: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    " 128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    " 128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    " 128008: AddedToken(\"<|eom_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    " 128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    " 128010: AddedToken(\"<|python_tag|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "```\n",
    "\n",
    "These tokens are fundamental to the tokenizer, but there are also several \"extra\" special tokens that appear to have been introduced primarily to reach a fixed embedding size of 128,256, likely for numerical reasons:\n",
    "\n",
    "```\n",
    " 128011: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    " 128012: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "When we inspect the `tokenizer.special_tokens_map`, we only see a few essential tokens:\n",
    "\n",
    "{'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>'}\n",
    "\n",
    "\n",
    "As noted in [this Hugging Face forum discussion](https://discuss.huggingface.co/t/how-to-set-the-pad-token-for-meta-llama-llama-3-models/103418), the LLaMA tokenizer doesn't come with a dedicated padding token by default. While this is not a critical issue for inference tasks, it becomes important during fine-tuning. As a best practice, we recommend using one of the reserved tokens for padding, specifically `\"<|finetune_right_pad_id|>\"`.\n"
   ],
   "id": "aa0afbb9831f668d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:00:07.010160Z",
     "start_time": "2024-10-11T13:00:06.999849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(model.get_input_embeddings())\n",
    "\n",
    "# Set the \"<|finetune_right_pad_id|>\" as the padding token\n",
    "tokenizer.pad_token = tokenizer.added_tokens_decoder[128004].content\n",
    "\n",
    "# Verify that the pad_token_id is correctly set to the ID of \"<|finetune_right_pad_id|>\"\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "print(f\"Pad token ID is set to: {pad_token_id}\")"
   ],
   "id": "76ab19a79905b1a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000\n",
      "Embedding(128256, 2048)\n",
      "Pad token ID is set to: 128004\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing the model",
   "id": "c95c50c4b3b33a74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:34:32.623876Z",
     "start_time": "2024-10-11T13:34:32.620250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temperature = 0.6 # default\n",
    "do_sample = True # without it, it works weird, need to check chat models"
   ],
   "id": "1aa77184f679963f",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using `generate()`",
   "id": "b0976930d8689e97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:34:04.261632Z",
     "start_time": "2024-10-11T13:34:00.063996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare a prompt\n",
    "prompt = \"What is the color of the sky?\"\n",
    "\n",
    "# Tokenize the input prompt with padding\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Ensure the attention mask is set properly\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "# Generate response from the model\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    pad_token_id=pad_token_id,  # Use the custom pad token ID\n",
    "    temperature=temperature,\n",
    "    do_sample=do_sample\n",
    ")\n",
    "\n",
    "# Decode the generated tokens to text\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ],
   "id": "2444a88ec059a530",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the color of the sky? Is it blue? Is it green? Or is it a combination of both? In this article, we will explore the different shades of sky, their meanings, and how to use them in your own art\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using `pipeline()`",
   "id": "16d5326d243a68d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:34:49.999393Z",
     "start_time": "2024-10-11T13:34:46.416994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Generate text with specified max length\n",
    "output = pipe(\n",
    "    prompt, \n",
    "    max_length=50, \n",
    "    pad_token_id=tokenizer.pad_token_id, \n",
    "    truncation=True,\n",
    "    temperature=temperature,\n",
    "    do_sample=do_sample,\n",
    ")\n",
    "\n",
    "print(output)"
   ],
   "id": "bfb6a3781675b232",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What is the color of the sky? The sky is blue. What is the color of the grass? The grass is green. What is the color of the ocean? The ocean is blue. What is the color of the sky? The sky'}]\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4df67a3f291d05c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
