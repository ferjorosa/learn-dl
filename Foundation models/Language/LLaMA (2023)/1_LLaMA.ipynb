{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA (2023)\n",
    "\n",
    "The purpose of this notebook is to provide an overview of the LLaMA family of models. It is an evolving family. Therefore, we are going to cover both the LLaMA model family and the LLaMA 2 model family. The objective is to understand the most important aspects that make these models perform better than its predecesors, even with a lower parameter count.\n",
    "\n",
    "List of main relevant papers:\n",
    "* [Touvron et al. (2023)](https://arxiv.org/pdf/2302.13971.pdf). LLaMA: Open and Efficient Foundation Language Models\n",
    "* [Touvron et al. (2023)](https://arxiv.org/pdf/2307.09288.pdf). LLaMA 2: Open Foundation and Fine-Tuned Chat Models\n",
    "* [Llama Team (2024)](https://arxiv.org/pdf/2407.21783). The LLaMA 3 Herd of Models\n",
    "\n",
    "List of main relevant blogs:\n",
    "* [Meta (2023)](https://ai.meta.com/blog/large-language-model-llama-meta-ai/). Introducing LLaMA: A foundational, 65-billion-parameter large language model\n",
    "* [Meta (2023)](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/). LLaMA 2: Open Foundation and Fine-Tuned Chat Models\n",
    "\n",
    "List of main relevant Youtube videos:\n",
    "* [Kilcher (2023)](https://www.youtube.com/watch?v=E5OnoYF2oAk). LLaMA explained\n",
    "* [Jamil (2023)](https://www.youtube.com/watch?v=Mn_9W1nCFLo). LLaMA explained\n",
    "* [Khan (2024)](https://lightning.ai/fareedhassankhan12/studios/building-llama-3-from-scratch). Building Llama 3 from scratch with Pytorch\n",
    "* [Sebastian Raschka (2024)](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-gpt-to-llama2.ipynb). Converting GPT-2 to Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a few examples ([Brow et al., 2020](https://arxiv.org/abs/2005.14165)). These few-shot properties first appeared when scaling models to a sufficient size, resulting in a line of work that focuses on further scaling these models. \n",
    "\n",
    "However, recent work from [Hoffmann et al. (2022)](https://arxiv.org/abs/2203.15556) shows that , for a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data. In the article, [Hoffmann et al. (2022)](https://arxiv.org/abs/2203.15556) discuss that the trend up to that point in large language model training has been to increase the model size, often without increasing the number of training tokens. The authors hypothesize that the race to train larger and larger models is resulting in models that are substantially underperforming compared to what could be achieved with the same compute budget. In this context, the authors propose a new [*scaling law*](https://en.wikipedia.org/wiki/Neural_scaling_law) that determines how to best scale the dataset and model sizes for a particular training compute budget (in TFLOP terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMA authors build on top of these ideas, but argue that Chinchilla's scaling law by Hoffmann et al. focus on training budget only is not enough because it disregards the inference budget, which becomes critical when serving a language model at scale. In this contex, give a target level of performance, **the preferred model is not the fastest to train but the fastest at inference**, and although it may be cheaper to train a large model to reach a certain performance, a smaller one trained longer will ultimately be cheaper at inference. For instance, although [Hoffmann et al. (2022)](https://arxiv.org/abs/2203.15556) recommends training a 10B parameter model on 200B tokens, **LLaMA authors find that the performance of a 7B model continues to improve even after 1T tokens**.\n",
    "\n",
    "The focus of the paper is to train a series of language models that achieve the best possible performance at various inference budgets, **by training on more tokens than what is typically used**. the resulting models range from 7B to 65B parameters with competitive performance compared to the best existing LLMs. Another important aspect of the paper is that unlike Chinchilla ([Hoffmann et al., 2022](https://arxiv.org/abs/2203.15556)), PaLM ([Chowdhery et al., 2022](https://arxiv.org/abs/2204.02311)), or GPT-3 ([Brow et al., 2020](https://arxiv.org/abs/2005.14165)), LLaMA was trained only with publicly available data.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/llama_1_table.png\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - LLaMA 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMA 2 is an updated version of LLaMA 1, trained on a new mix of publicly avaibale data. To create a new familiy of LLaMA 2 models, they began with the pretraining approach of LLaMA, using an optimized auto-regressive transformer (i.e., decoder only Transformer), but made several changes to improve performance. Specifically, they made the following changes:\n",
    "\n",
    "* Performed more robust data cleaning\n",
    "* Updated the data mixes\n",
    "* Trained on 40% more total tokens (100% for smaller models since they had not been previously trained on all of the data)\n",
    "* Doubled the context length\n",
    "* Used Gropued-quary attention (GQA) to improve inference scalability in combination with KV cache\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/llama_2_table.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "In addition to a foundation model, the authors released **a fine-tuned version of LLaMA 2 that was optimized for dialogue use cases** using Reinforcement Learning with Human Feedback (RLHF) methodologies, specifically through rejection sampling and Proximal Policy Optimization (PPO). Throughout the RLHF stagge, the accumulation of iterative reward modeling data in parallel with model enhancements is crucial to ensure the reward models remain within distribution.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/llama_chat.png\" width=\"1000\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMA 3 represents significant advancements over LLaMA 2 in several key areas, including scalability, efficiency, and performance. Here's a focused breakdown of the improvements:\n",
    "\n",
    "- **Model size and vocabulary:** LLaMA 3 scales up to **128,256-token vocabulary**, compared to LLaMA 2’s 32,000 tokens.\n",
    "- **Training data:** Trained on **15 trillion tokens**, over 7 times more than LLaMA 2’s 2 trillion, improving its knowledge base and multilingual capabilities.\n",
    "- **Context length:** LLaMA 3 doubles the context length to **8,192 tokens**\n",
    "- **Architectural optimizations:** Llama 2 used **Grouped-Query Attention (GQA)** only on big models. Llama 3 uses it on smaller models too, like the 8B model.\n",
    "- **Multilingual support:** Significantly better at handling **30+ languages** (Llama 2 supported 20) due to expanded training data and vocabulary. More non-english tokens too\n",
    "- **Dialogue optimization:** Uses **Proximal Policy Optimization (PPO)** and **Direct Preference Optimization (DPO)** for improved conversational and interactive tasks.\n",
    "- **Safety and AI responsibility:** Stronger emphasis on safety with features like **Llama Guard 2** and red-teaming practices, enhancing responsible deployment.\n",
    "- **Training efficiency:** LLaMA 3 benefits from Meta’s **advanced 24,000-GPU clusters**, tripling training efficiency compared to LLaMA 2.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/llama_3_table.png\" width=\"500\"/></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first difference between the traditional Transformer architecture is that **LLaMA is a decoder-only model**. In addition, it leverages various improvements that were subsequently proposed and used in different models. Here are the main differences with the original architecture, and where the authors found the inspiration from this change:\n",
    "\n",
    "* **Pre-normalization with RMSNorm [GPT-3]**. To improve the training stability, the input of each transformer sub-layer is normalized instead of normalizing the output. They use the **RMSNorm** normalizing function, introduced by [Zhang and Sennrich (2019)](https://arxiv.org/abs/1910.07467)\n",
    "\n",
    "* **Rotary embeddings [GPTNeo]**. They remove the absolute positional embeddings, and instead, add rotary positional embeddings (RoPE), introduced by [Su et al. (2021)](https://arxiv.org/abs/2104.09864), at each layer of the network\n",
    "\n",
    "* **Grouped query attention** ([Ainslie et al., 2023](https://arxiv.org/abs/2305.13245)). A new type of attention mechanism that improves the performance and efficiency of transformer models by dividing the query heads into groups and sharing the key and value heads across each group\n",
    "\n",
    "* **SwiGLU activation function [PALM]**. They replace the ReLU non-linearity aby the SwiGLU activation function, introduced by [Shazeer (2020)](https://arxiv.org/abs/2002.05202). They use a dimenstion of $\\frac{2}{3} 4d$ instead of $4d$ as in PaLM\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/llama_3_architecture.jpeg\" width=\"1300\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is tokenized with the Byte-pair encoding (BPE) algorithm ([Sennrich et al., 2016](https://arxiv.org/abs/1508.07909)) using the implementation from SentencePiece ([Kudo and richardson, 2018](https://arxiv.org/abs/1808.06226)). Notably all numbers are split into individual digits, and fallback to bytes to decompose unknown UTF-8 characters. The total vocabulary size is 32K tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Layer normalization & RMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer normalization (LN)** is a technique for normalizing the activations of a neural network layer. This helps to stabilize the training process and improve the performance of the model.  Layer normalization works by subtracting the mean of the activations from each activation and then dividing by the standard deviation of the activations. This ensures that the activations have a mean of zero and a standard deviation of one.\n",
    "\n",
    "Basically, with LN, we are transforming the input/output (depending of where we apply the normalization) of layers into values from a standard Gaussian distribution $\\mathcal{N}(0,1)$. This can be easily showed. Assuming our data $X$ follows a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma) = \\mathcal{N}(5, 6)$, when we substract the mean and divide by the standard deviation we would get $\\frac{X-5}{6} = z \\approx \\mathcal{N}(0,1)$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/layer_normalization.png\" width=\"450\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Root Mean Square Normalization (RMSNorm)** is a simplified version of LN that is less computationally expensive. RMSNorm works by subtracting the square root of the mean squared activations from each activation. RMSNorm only focuses on re-scaling invariance. The authors hypothesize that re-scaling invariance is the reason for success of LN, rather than recentering invariance. Intuitively, RMSNorm simplifies LN by totally removing the mean statistic:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/rmsnorm.png\" width=\"850\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Both LN and RMSNorm are effective at normalizing the activations of neural networks. However, LN is more computationally expensive than RMSNorm. This is because LN needs to compute the mean and standard deviation of the activations for each layer, while RMSNorm only needs to compute the square root of the mean squared activations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Positional encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 - What is the difference between absolute and relative positional encodings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute positional encodings use stickers that say \"I'm in the first position,\" \"I'm in the second position,\" and so on. Relative positional encodings use stickers that say \"I'm close to this one,\" \"I'm far from that one,\" to help us understand where the blocks are in relation to each other.\n",
    "\n",
    "**Absolute Positional Encodings ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)):**\n",
    "* Absolute positional encodings are typically fixed embeddings that are added to the token embeddings based on the absolute positions of tokens in the input sequence.\n",
    "* They are typically precomputed or learned independently of the input data.\n",
    "* For example, in the original transformer model, absolute positional encodings are typically represented as sine and cosine functions of the position and are added to the token embeddings to provide a fixed positional signal for the model.\n",
    "\n",
    "**Relative Positional Encodings ([Shaw et al., 2018](https://arxiv.org/abs/1803.02155)):**\n",
    "* Relative positional encodings are computed based on the relative positions of tokens within a sequence.\n",
    "* They capture how each token relates to the others in terms of their positions, making them more adaptable to different sequence lengths.\n",
    "* Relative positional encodings are particularly useful in cases where sequences have variable lengths or when dealing with very long sequences.\n",
    "* These encodings can be learned from the data, taking into account the relative distances between tokens, which allows the model to adapt to different sequences.\n",
    "\n",
    "**Sinusoidal functions** allow models to estimate relative positions between tokens through their periodic and smooth nature. By encoding absolute positions with sine and cosine at different frequencies, the embeddings create predictable, continuous differences between positions. This enables models to infer relative distances, helping capture both short- and long-range dependencies in a sequence.\n",
    "\n",
    "**Sinusoidal functions** in positional embeddings have limitations with **fixed maximum sequence lengths**, typically defined up to a certain point (e.g., 512 or 1024). Beyond this limit, the model struggles to encode additional positions, affecting its utility for longer sequences and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 - Rotary positional encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [Very good blog post about RoPE](https://karthick.ai/blog/2024/Rotatory-Position-Embedding-(RoPE)/)\n",
    "\n",
    "The main idea is that the model can adapt and **learn the best rotations for the positional information during training, rather than relying on fixed sinusoidal functions**. Learning these rotations allows the model to adapt better to the specifics of the data it's processing.\n",
    "\n",
    "Advantages of RoPE\n",
    "* Long-Range Context: RoPE adeptly captures relationships between tokens across lengthy sequences, a challenge for conventional positional embeddings.\n",
    "* Rotation Invariance: By design, RoPE maintains effectiveness irrespective of sequence length, addressing a limitation of sine-cosine embeddings.\n",
    "* Interpretability: The rotational approach offers an intuitive geometric interpretation of how positional information influences the attention mechanism.\n",
    "\n",
    "Other practical differences with the original positional encodings:\n",
    "* Rotary position embeddings are only applied to the query and the keys, but not the values. \n",
    "* Rotary position embeddings are applied after **q** and **k** vectors have been multiplied by the W matrix in the atention mechanism, while in the vanilla Transformer they are applied before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder of multi-head self-attention:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/self_attention.png\" width=\"1000\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 - KV cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the decoder is causal (i.e., the attention of a token only depends on its preceding tokens), at each generation step we are recalculating the same previous token attention, **when we actually just want to calculate the attention for the new token**.\n",
    "\n",
    "This is where KV comes into play. By caching the previous Keys and Values, we can focus on only calculating the attention for the new token.\n",
    "\n",
    "Why is this optimization important? As seen in the picture above, the matrices obtained with KV caching are way smaller, which leads to faster matrix multiplications. The only downside is that it needs more GPU VRAM (or CPU RAM if GPU is not being used) to cache the Key and Value states.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/kv_caching_2.gif\" width=\"600\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with KV caching: 8.031 +- 0.0 seconds\n",
      "without KV caching: 47.205 +- 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "for use_cache in (True, False):\n",
    "  times = []\n",
    "  for _ in range(1):  # measuring X generations\n",
    "    start = time.time()\n",
    "    model.generate(**tokenizer(\"What is KV caching?\", return_tensors=\"pt\").to(device), use_cache=use_cache, max_new_tokens=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    times.append(time.time() - start)\n",
    "  print(f\"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 - Grouped Query Attention (GQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs have a \"problem\", they are too fast. For example, a A100 GPU does mathematical operations x40 faster that it can transfer the same amount of information to memory. This discrepancy highlights an important bottleneck: it's not always the number of operations being performed that limits performance, but rather the amount of data transfer required for those operations. \n",
    "\n",
    "For example, computing the same operation on the same tensor $N$ times may be faster than computing the same operation on $N$ different tensors, even if they have the same size, this is because the GPU may need to move the tensors around.\n",
    "\n",
    "**Our goal should not only be to optimize the number of operations we do, but also minimize the memory access/transfers that we perform**.\n",
    "\n",
    "To do that, we can modify the way we estimate self-attention. We can distinguish three possible approaches:\n",
    "* Multi-head self-attention ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762))\n",
    "* Multi-query self-attention ([Shazeer, 2019](https://arxiv.org/abs/1911.02150))\n",
    "* Grouped-query self-attention ([Ainslie et al., 2023](https://arxiv.org/abs/2305.13245))\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/different_types_of_attention.png\" width=\"650\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head self-attention\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/multi_head_attention.png\" width=\"900\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head self-attention with KV cache\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/multi_head_attention_kv.png\" width=\"900\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-query attention (MQA) with KV cache\n",
    "\n",
    "[Shazeer (2019)](https://arxiv.org/abs/1911.02150) proposed a refinement to the Multi-Head Attention (MHA) algorithm called Multi-Query Attention (MQA), which reduces memory badnwith overhead of loading keys and values by using multiple query heads with a single key/value head.\n",
    "\n",
    "In the following picture we can see that we do not \"repeat\" $K$ and $V$ tensors by not using the $h$ dimension (which I assume it is the number of heads)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/multi_query_attention.png\" width=\"900\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouped-query attention (GQA)\n",
    "\n",
    "Based on the research of [Ainslie et al. (2023)](https://arxiv.org/abs/2305.13245), MQA highlights certain drawbacks. Specifically, utilizing MQA can lead to a decline in quality and introduce training instability. Consequently, attempting to train distinct models optimised separately for quality and inference may not be a practical solution, as stated in the paper.\n",
    "\n",
    "This is because **the primary goal of employing the MQA technique is to accelerate the inference process, making the modification of the entire model architecture and training approach for this purpose impractical**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/grouped_query_attention.jpg\" width=\"900\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - SwiGLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Shazeer (2020)](https://arxiv.org/abs/2002.05202) studies how GLU variants improve Transformers performance\n",
    "\n",
    "GLU (Gated Linear Units) is a neural network layer, not an activation funciton in the strict sense. It is a linear transformation followed by a gating mechanism. The gating mechanism is a sigmoid function that controls the flow of information from the linear transformation. The GLU has non-linear capabilities, but has a linear path for the gradient so diminishes the vanishing gradient problem.\n",
    "\n",
    "We can deﬁne GLU variants using other activation functions than the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bilinear activation\n",
    "\n",
    "The bilinear layer is a GLU variant that omits the sigmoid function. It is a bilinear transformation followed by an element-wise product.\n",
    "\n",
    "$$\\text{Bilinear}(x, W, V, b, c) = (xW + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReGLU activation\n",
    "\n",
    "ReGLU is a GLU variant that uses ReLU as the activation function:\n",
    "\n",
    "$$\\text{ReGLU}(x, W, V, b, c) = \\text{ReLU}(xW + b) \\otimes (xV + c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GEGLU activation\n",
    "\n",
    "GEGLU is a GLU variant that uses GELU as the activation function.\n",
    "\n",
    "$$\\text{GEGLU}(x, W, V, b, c) = \\text{GELU}(xW + b) \\otimes (xV + c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SwiGLU activation\n",
    "\n",
    "SwiGLU is a GLU variant that uses Swish as the activation function.\n",
    "\n",
    "$$\\text{SwiGLU}(x,W, V, b, c) = \\text{Swish}_1(xW + b) \\otimes (xV + c)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
