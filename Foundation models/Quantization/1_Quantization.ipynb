{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "\n",
    "> Good one: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization\n",
    "\n",
    "* https://lightning.ai/blog/4-bit-quantization-with-lightning-fabric/\n",
    "* https://lightning.ai/blog/8-bit-quantization-with-lightning-fabric/\n",
    "* https://lightning.ai/docs/fabric/latest/fundamentals/precision.html#quantization-via-bitsandbytes\n",
    "* https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\n",
    "* https://huggingface.co/blog/hf-bitsandbytes-integration\n",
    "* https://pytorch.org/blog/quantization-in-practice/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to make efficient use of both server-side and on-device compute resources when developing machine learning. To support more efficient deployment on servers and edge devices, we can use Quantization.\n",
    "\n",
    "Quantization leverages 8bit integer (int8) instructions to **reduce the model size and run the inference faster** (reduced latency) and can be the difference when fitting a model into the available resources. In addition, even when resources aren't quite so constrained, it may enable you to deploy a larger and more accurate model.\n",
    "\n",
    "Therefore, by doing both computations and memory accesses with lower precision data (usually int8 compared to floating point implementations), it enables performance gains in several important areas:\n",
    "\n",
    "* 4x reduction in model size;\n",
    "* 2-4x reduction in memory bandwith;\n",
    "* 2-4x faster inference due to savings in memory bandwith and faster compute with int8 arithmetic (the exact speed up vaires depending on the hardware, the runtime, and the model)\n",
    "\n",
    "Quantization does however come with some costs. Fundamentally, **quantization means introducing approximations and the resulting networks have slightly less accuracy**. These techniques attempt to minimize the gap between the full floating point accuracy and the quantized accuracy.\n",
    "\n",
    "For information about INT4 Quantization: https://arxiv.org/pdf/2301.12017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Quantization in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization was designed to fit into the PyTorch framework. This means that:\n",
    "\n",
    "1. PyTorch has data types corresponding to [quantized tensors](https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor), which share many of the features of \"normal\" tensors.\n",
    "\n",
    "2. One can write kernels with quantized tensors, much like kernels for floating point tensors to customize their implementation. PyTorch supports quantized modules for common operations as part of the `torch.nn.quantized` and `torch.nn.quantized.dynamic` name-space.\n",
    "\n",
    "3. Quantization is compatible with the rest of PyTorch: quantized models are traceable and scriptable. The quantization method is virtually identical for both server and mobile backends. One can easily mix qunatized and floating point operations in a model.\n",
    "\n",
    "4. Mapping of floating point tensors to quantized tensors is customizable with user defined observer/fake-quantization blocks. PyTorch provides default implementations that should work for most use cases.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/torch_quantization.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - The Three Modes of Quantization Supported in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 - Dynamic Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest method of quantization PyTorch supports is called **dynamic quantization**. This involves not just converting the weights to `int8` - as happens in all quantization variants -  but also converting the activations to `int8` on the fly, just before doing the computation (hence \"dynamic\"). \n",
    "\n",
    "The computations will thus be performed using efficient `int8` matrix multiplication and convolution implementations, resulting in faster compute. However, the activations are read and written to memory in floating point format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a simple API for dynamic quantization in PyTorch. torch.quantization.quantize_dynamic takes in a model, as well as a couple other arguments, and produces a quantized model!\n",
    "\n",
    "[PyTorch documentation contains an end-to-end tutorial that illustrates how to do it for a BERT model.](https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html)\n",
    "\n",
    "Neverhtless, the part that quantizes the model is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, \n",
    "    {torch.nn.Linear}, \n",
    "    dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 - Post-Training Static Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can further improve the performance (latency) by converting networks to use both integer arithmetic and `int8` memory accesses.\n",
    "\n",
    "Static quantization performs the additional step of first feeding batches of data through the network and computing the resulting distributions of the different activations (specifically, this is done by inserting \"observer\" modules at different points that record these distributions). **This information is used to determine how specifically the different activations should be quantized at inference time.**\n",
    "\n",
    "A simple technique for static quantization would be to divide the entire range of activations into 256 levels, but there are more sophisticated methods as well.\n",
    "\n",
    "Importantly, this additional step allows us to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.\n",
    "\n",
    "**The Process (Simplified)**\n",
    "\n",
    "* **Calibration:** Feed a calibration dataset through the model. Analyze how values are distributed.\n",
    "\n",
    "* **Determining Scaling Factors:** Calculate how to transform the range of floating-point values into the narrower range of integers.\n",
    "\n",
    "* **Quantization:** Convert weights and activations into integers using the scaling factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers several features that allow users to optimize their static quantization:\n",
    "\n",
    "1. **Observers** (`torch.quantization.prepare`): we can customize observer modules, which specify how statistics are collected prior to quantization to try out more advanced methods to quantize your data.\n",
    "\n",
    "2. **Operator fusion** (`torch.quantization.fuse_modules`): we can fuse multiple operations into a single operation, saving on memory access while also improving the operation's numerical accuracy.\n",
    "\n",
    "3. Per-channel quantization: we can independently quantize weights for each output channel in a convolution/linear layer, which can lead to higher accuracy with almost the same speed.\n",
    "\n",
    "[This tutorial shows how to do post-training static quantization, as well as illustrating two more advanced techniques - per-channel quantization and quantization-aware training - to further improve the modelâ€™s accuracy.](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)\n",
    "\n",
    "Finally, quantization itself is done using `torch.quantization.convert`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set quantization config for server (x86)\n",
    "# 'fbgemm' for server, 'qnnpack' for mobile\n",
    "deploymentmyModel.qconfig = torch.quantization.get_default_config('fbgemm')\n",
    "\n",
    "# insert observers, calibrate the model, and collect statistics\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# convert to quantized version\n",
    "torch.quantization.convert(myModel, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 - Quantization Aware Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantization-aware training (QAT)** is the third method, and the one that typically results in highest accuracy of these three.\n",
    "\n",
    "With QAT, all weights and activations are \"fake quantized\" during both the forward and backward passes of training: that is, float values are rounded to mimic `int8` values, but all computations are still done with floating point numbers. Thus, **all the weight adjustments during training are made while \"aware\" of the fact that the model will ultimately be quantized**; after quantizing, therefore, this method usually yields higher accuracy than the other two methods.\n",
    "\n",
    "[This tutorial shows how to do post-training static quantization, as well as illustrating two more advanced techniques - per-channel quantization and quantization-aware training - to further improve the modelâ€™s accuracy.](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `torch.quantization.prepare_qat` inserts fake quantization modules to model quantization.\n",
    "\n",
    "* Mimicking the static quantization API, `torch.quantization.convert `actually quantizes the model once training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify quantization config for QAT\n",
    "# 'fbgemm' for server, 'qnnpack' for mobile\n",
    "qat_model.qconfig=torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# prepare QAT\n",
    "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
    "\n",
    "# convert to quantized version, removing dropout, to check for accuracy on each\n",
    "epochquantized_model=torch.quantization.convert(qat_model.eval(), inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Device and Operator Support in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization support is restricted to a subset of available operators, depending on the method being used, for a list of supported operators, please see the documentation at https://pytorch.org/docs/stable/quantization.html\n",
    "\n",
    "The set of available operators and the quantization numerics also depend on the backend being used to run quantized models. Currently quantized operator are supported only for CPU inference in the following backends: \n",
    "\n",
    "* x86 (server).\n",
    "* ARM (mobile).\n",
    "\n",
    "Both the quantization configuration (how tensors should be quantized) and the quantized kernels (arithmetic with quantized tensors) are backend dependent. One can specify the backend by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchbackend='fbgemm'\n",
    "\n",
    "# 'fbgemm' for server, 'qnnpack' for mobile\n",
    "my_model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "\n",
    "# prepare and convert model\n",
    "# Set the backend on which the quantized kernels need to be run\n",
    "torch.backends.quantized.engine=backend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, quantization aware training occurs in full floating point and can run on either GPU or CPU. Quantization aware training is typically only used in CNN models when post training static or dynamic quantization doesnâ€™t yield sufficient accuracy. This can occur with models that are highly optimized to achieve small size (such as Mobilenet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Choosing a Quantization approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of which scheme to use depends on multiple factors:\n",
    "\n",
    "* Model/Target requirements: Some models might be sensitive to quantization, requiring quantization aware training.\n",
    "\n",
    "* Operator/Backend support: Some backends require fully quantized operators.\n",
    "\n",
    "Currently, operator coverage is limited and may restrict the choices listed in the table below: The table below provides a guideline.\n",
    "\n",
    "| Model Type | Preferred Scheme | Why |\n",
    "|---|---|---|\n",
    "| LSTM/RNN | Dynamic Quantization | Throughput dominated by compute/memory bandwidth for weights |\n",
    "| BERT/Transformer | Dynamic Quantization | Throughput dominated by compute/memory bandwidth for weights |\n",
    "| CNN | Static Quantization | Throughput limited by memory bandwidth for activations |\n",
    "| CNN | Quantization Aware Training | In the case where accuracy can't be achieved with static quantization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 - Performance Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization provides a 4x reduction in the model size and a speedup of 2x to 3x compared to floating point implementations depending on the hardware platform and the model being benchmarked. Some sample results are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Float Latency (ms) | Quantized Latency (ms) | Inference Performance Gain | Device | Notes |\n",
    "|---|---|---|---|---|---|\n",
    "| BERT | 581 | 313 | 1.8x | Xeon-D2191 (1.6GHz) | Batch size = 1, Maximum sequence length= 128, Single thread, x86-64, Dynamic quantization |\n",
    "| Resnet-50 | 214 | 103 | 2x | Xeon-D2191 (1.6GHz) | Single thread, x86-64, Static quantization |\n",
    "| Mobilenet-v2 | 97 | 17 | 5.7x | Samsung S9 | Static quantization, Floating point numbers are based on Caffe2 run-time and are not optimized |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 - Accuracy results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also compared the accuracy of static quantized models with the floating point models on Imagenet. For dynamic quantization, we compared the F1 score of BERT on the GLUE benchmark for MRPC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computer Vision Model accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Top-1 Accuracy (Float) | Top-1 Accuracy (Quantized) | Quantization Scheme |\n",
    "|---|---|---|---|\n",
    "| Googlenet | 69.8 | 69.7 | Static post-training quantization |\n",
    "| Inception-v3 | 77.5 | 77.1 | Static post-training quantization |\n",
    "| ResNet-18 | 69.8 | 69.4 | Static post-training quantization |\n",
    "| ResNet-50 | 76.1 | 75.9 | Static post-training quantization |\n",
    "| ResNeXt-101 32x8d | 79.3 | 79 | Static post-training quantization |\n",
    "| Mobilenet-v2 | 71.9 | 71.6 | Quantization Aware Training |\n",
    "| Shufflenet-v2 | 69.4 | 68.4 | Static post-training quantization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speech and NLP Model accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | F1 (GLUEMRPC) Float | F1 (GLUEMRPC) Quantized | Quantization scheme |\n",
    "|---|---|---|---|\n",
    "| BERT | 0.902 | 0.895 | Dynamic quantization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Quantization with Lightning Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Quantization via Bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - 8-bit Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8-bit quantization is discussed in the popular paper [8-bit Optimizers via Block-wise Quantization (Dettmers et al., 2022)](https://arxiv.org/abs/2110.02861) and was introduced in [FP8 Formats for Deep Learning (Micikevicius et al., 2022)](https://arxiv.org/pdf/2209.05433.pdf).\n",
    "\n",
    "As stated in the original paper, 8-bit quantization was the natural progression after 16-bit precision. Although it was the natural progression, the implementation was not as simple as moving from FP32 to FP16 â€“ as **those two floating point types share the same representation scheme and 8-bit does not**.\n",
    "\n",
    "8-bit quantization requires a new representation scheme, and this new scheme allows for fewer numbers to be represented than FP16 or FP32. This means model performance may be affected when using quantization, so it is good to be aware of this trade-off. Additionally, model performance should be evaluated in its quantized form if the weights will be used on an edge device that requires quantization.\n",
    "\n",
    "Lightning Fabric can use 8-bit quantization by setting the `mode` flag to `int8` for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.fabric import Fabric\n",
    "from lightning.fabric.plugins import BitsandbytesPrecision\n",
    "\n",
    "# available 8-bit quantization modes\n",
    "# (\"int8\")\n",
    "\n",
    "mode = \"int8\"\n",
    "plugin = BitsandbytesPrecision(mode=mode)\n",
    "fabric = Fabric(plugins=plugin)\n",
    "\n",
    "model = CustomModule() # your PyTorch model\n",
    "model = fabric.setup_module(model) # quantizes the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - 4-bit Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-bit quantization is discussed in the popular paper [QLoRA: Efficient Finetuning of Quantized LLMs. (Dettmers el al., 2023)](https://arxiv.org/abs/2305.14314). QLoRA is a finetuning method that uses 4-bit quantization. The paper introduces this finetuning technique and demonstrates how it can be used to \"finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance\" by using the NF4 (normal float) format.\n",
    "\n",
    "Lightning Fabric can use 4-bit quantization by setting the `mode` flag to either `nf4` or `fp4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.fabric import Fabric\n",
    "from lightning.fabric.plugins import BitsandbytesPrecision\n",
    "\n",
    "# available 4-bit quantization modes\n",
    "# (\"nf4\", \"fp4\")\n",
    "\n",
    "mode = \"nf4\"\n",
    "plugin = BitsandbytesPrecision(mode=mode)\n",
    "fabric = Fabric(plugins=plugin)\n",
    "\n",
    "model = CustomModule() # your PyTorch model\n",
    "model = fabric.setup_module(model) # quantizes the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Double Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double qunatization exists as an extra 4-bit quantization setting introduced alongside NF4 in the QLoRA paper. Double qunatization works by quantizing the quantization constants that are internal to bitsandbytesâ€™ procedures.\n",
    "\n",
    "Lightning Fabric can use 4-bit double quantization by setting the `mode` flag to either `nf4-dq` or `fp4-dq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.fabric import Fabric\n",
    "from lightning.fabric.plugins import BitsandbytesPrecision\n",
    "\n",
    "# available 4-bit double quantization modes\n",
    "# (\"nf4-dq\", \"fp4-dq\")\n",
    "\n",
    "mode = \"nf4-dq\"\n",
    "plugin = BitsandbytesPrecision(mode=mode)\n",
    "fabric = Fabric(plugins=plugin)\n",
    "\n",
    "model = CustomModule() # your PyTorch model\n",
    "model = fabric.setup_module(model) # quantizes the layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
