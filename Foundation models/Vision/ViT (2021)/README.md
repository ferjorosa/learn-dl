# Vision Transformers

Vision Transformers (ViTs) adapt the transformer architecture, known for its success in natural language processing, to process images directly. ViTs eliminate the need for convolutional layers and achieve competitive performance in image classification and other vision tasks by leveraging self-attention mechanisms.

**List of main relevant papers:**
* [Vaswani et al. (2017)](https://arxiv.org/pdf/1706.03762.pdf). Attention is all you need.
* [Dosovitskiy et al. (2021)](https://arxiv.org/abs/2010.11929). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.

**List of main relevant blogs:**
* [Briggs and Carnevali (2023)](https://www.pinecone.io/learn/image-search/). Embedding methods for image search.
* [HuggingFace documentation on Vision Transformers](https://huggingface.co/docs/transformers/model_doc/vit)