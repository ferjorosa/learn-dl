{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd7655c129a3fb25",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization\n",
    "\n",
    "List of main relevant papers:\n",
    "* https://arxiv.org/pdf/2305.18290\n",
    "\n",
    "List of main relevant blogs / book chapters\n",
    "* https://allam.vercel.app/post/dpo/\n",
    "* https://github.com/rasbt/LLMs-from-scratch/tree/main/ch07/04_preference-tuning-with-dpo\n",
    "* https://huggingface.co/blog/pref-tuning\n",
    "* https://argilla.io/blog/mantisnlp-rlhf-part-11\n",
    "* https://medium.com/mantisnlp/finetuning-an-llm-rlhf-and-alternatives-part-iii-d1dcb1792968\n",
    "\n",
    "List of main relevant tools: \n",
    "* https://github.com/argilla-io/argilla\n",
    "* https://allam.vercel.app/post/dpo/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a3b7c005ccf429",
   "metadata": {},
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a25fc4971c7b8",
   "metadata": {},
   "source": [
    "For most applications, itâ€™s crucial for LLMs to generate text that is contextually consistent and aligned with the intended task and user behavior. This includes developing LLMs that are safe, aligned, and unbiased, or those capable of generating syntactically and functionally correct code, despite the presence of incorrect code in the training data. However, the pre-training process alone does not guarantee specific model behavior. This is where [Reinforcement Learning From Human Feedback (RLHF)](https://huggingface.co/blog/rlhf) becomes vital.\n",
    "\n",
    "RLHF is a technique used to fine-tune LLMs by maximizing a reward function derived from another reward model trained on human feedback from evaluators based on a set of generated samples. This technique is widely used and is considered state-of-the-art. However, RLHF has several drawbacks that limit its effectiveness as a solution.\n",
    "\n",
    "Direct Preference Optimization (DPO) has emerged as a promising alternative for aligning LLMs to human or AI preferences. Unlike traditional alignment methods, which are based on reinforcement learning, DPO recasts the alignment formulation as a simple loss function that can be optimized directly on a dataset of preferences $(x, y_w, y_l)$ where $x$ is a prompt and $(y_w, y_l)$ are the preferred and dispreferred responses.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_2/dpo_vs_rlhf.png\" width=\"900\"/></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b84f02-a038-49be-b807-6837d4e1db07",
   "metadata": {},
   "source": [
    "# 2 - How RLHF works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2404031d-c924-4b95-a4c3-bacda1257f3b",
   "metadata": {},
   "source": [
    "Given a pre-trained LLM, RLHF works in three steps:\n",
    "\n",
    "1. Generate a set of samples using the LLM from a dataset of prompts.\n",
    "2. Human evaluators rate the samples, and train a seperate reward model on the samples and their ratings.\n",
    "3. Fine-tune the LLM using the reward model as a reward signal.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/rlhf.png\" width=\"900\"/></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:28:05.634166Z",
     "start_time": "2024-08-06T10:27:58.486586Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
