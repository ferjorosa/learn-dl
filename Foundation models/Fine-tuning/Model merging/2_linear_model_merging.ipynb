{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Linear Model Merging for LLMs\n",
    "\n",
    "https://lightning.ai/lightning-ai/studios/efficient-linear-model-merging-for-llms?section=blogs\n",
    "\n",
    "This notebooks implements the model merging method as described by the [Wortsman et al. (2022): Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time](https://arxiv.org/abs/2203.05482) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model merging is an approach where multiple pretrained or finetuned models are combined to form a new model that leverages the strengths and knowledge of each individual model. Unlike traditional model ensembling methods, which require the use of multiple models during inference time, model merging is a more efficient approach. It yields a single model that maintains the same size as each of the individual input models, as illustrated in the figure below:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_2/introduction.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "To begin exploring model merging, we will consider one of the earliest approaches in this area ([Worstman et al., 2022](https://arxiv.org/abs/2203.05482)). This paper proposes combining multiple models by averaging their weights, a technique now also referred to as \"linear\" merging. Although the [Model Soups paper](https://arxiv.org/abs/2203.05482) primarily focused on vision models trained with different hyperparameter configurations, this concept equally applies to LLMs that have been finetuned on various datasets and for different target tasks.\n",
    "\n",
    "Assuming that the models we want to merge are based on the same architecture, i.e., have the same number of parameters in each layer, the linear merging approach merges the two models by linear averaging. We can also add an `alpha` parameter as an additional weighting. Setting `alpha=0.5` will lead to each model contributing equally as illustrated in the figure below:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_2/linear_model_merging_example.jpg\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GPT-Neo-125M is a transformer model designed using EleutherAI's replication of the GPT-3 architecture](https://huggingface.co/EleutherAI/gpt-neo-125m). GPT-Neo was trained as an autoregressive language model. This means that its core functionality is taking a string of text and predicting the next token.\n",
    "\n",
    "**Note:** Since we are using linear model merging, we need to consider models that share the same architecture\n",
    "\n",
    "We are going to consider **two finetuned versions of GPT-Neo-125M**:\n",
    "\n",
    "* [b3ck1/gpt-neo-125M-finetuned-beer-recipes](https://huggingface.co/b3ck1/gpt-neo-125M-finetuned-beer-recipes). This model was trained on a custom dataset of ~76,800 beer recipes from the internet. Recipes are generated in a YAML-like format:\n",
    "\n",
    "  ```yaml\n",
    "  style: Pilsner\n",
    "  batch_size: 20\n",
    "  efficiency: 70\n",
    "  boil_size: 24\n",
    "  boil_time: 60\n",
    "  fermentables:\n",
    "  - name: Pale Ale\n",
    "    type: Grain\n",
    "    amount: 6.5\n",
    "  hops:\n",
    "  - name: Saaz\n",
    "    alpha: 3.5\n",
    "    use: Boil\n",
    "    time: 60\n",
    "    amount: 0.06\n",
    "  ...\n",
    "  ```\n",
    "\n",
    "* [flax-community/gpt-neo-125M-code-clippy-dedup-2048](https://huggingface.co/flax-community/gpt-neo-125M-code-clippy-dedup-2048?text=def+func%28%29%3A). The model was trained on the [CodeClippy dataset](https://huggingface.co/datasets/CodedotAI/code_clippy). This dataset was generated by selecting GitHub repositories from a large collection of repositories. These respositories are obtained from SEART GitHub Search using the following criteria:\n",
    "  * More than 10 GitHub stars\n",
    "  * More than 2 commits\n",
    "  * Must have a licence\n",
    "  * Exclude forks\n",
    "  * Size < 70708 bytes\n",
    "  \n",
    "  These repositories  are then combined with all of the GitHub repositories contain in The Pile and filtered for duplicate files. [A more detailed explanation of the dataset can be found here.](https://github.com/ncoop57/datasets/tree/code-clippy/datasets/code_clippy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "\n",
    "set_seed(32)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "base_model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=device, do_sample=True)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "clippy_model_name = \"flax-community/gpt-neo-125M-code-clippy-dedup-2048\"\n",
    "clippy_model = AutoModelForCausalLM.from_pretrained(clippy_model_name, device_map=device, do_sample=True)\n",
    "clippy_tokenizer = AutoTokenizer.from_pretrained(clippy_model_name)\n",
    "\n",
    "beer_model_name = \"b3ck1/gpt-neo-125M-finetuned-beer-recipes\"\n",
    "beer_model = AutoModelForCausalLM.from_pretrained(beer_model_name, device_map=device, do_sample=True)\n",
    "beer_tokenizer = AutoTokenizer.from_pretrained(beer_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Text generation capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three models: \n",
    "* A base model\n",
    "* A model fine-tuned for coding tasks (Clippy)\n",
    "* A model fine-tuned for generating beer recipes (Beer). \n",
    "\n",
    "Before attempting to combine Clippy and Beer to create a potentially \"better\" model, we need to validate their individual strengths. \n",
    "\n",
    "This validation involves assessing each model's performance on its respective domain (coding for Clippy, beer recipes for Beer) compared to the base model. Ideally, Clippy should outperform both the base model and Beer on coding tasks, while Beer should demonstrate superior performance on beer recipe generation compared to the base model and Clippy.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** When calling the `generate()` method I was receiving the following warning:\n",
    "\n",
    "```python\n",
    "\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\"\n",
    "```\n",
    "\n",
    "So I looked into StackOverflow and found that it is quite normal when generating text. The \"solution\" is to modify the `generate()` call by adding the following parameter: `pad_token_id=tokenizer.eos_token_id`\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, device, temperature=1.0, max_length=500):\n",
    "  \"\"\"\n",
    "  Generates text using a provided model, tokenizer, prompt, temperature, and max_length.\n",
    "\n",
    "  Args:\n",
    "      model: The loaded causal language model (e.g., AutoModelForCausalLM).\n",
    "      tokenizer: The tokenizer associated with the model.\n",
    "      prompt: The starting text for the generation.\n",
    "      temperature: Controls randomness of the generation (higher for more variation).\n",
    "      max_length: The maximum length of the generated text.\n",
    "\n",
    "  Returns:\n",
    "      The generated text as a string.\n",
    "  \"\"\"\n",
    "\n",
    "  # Encode the prompt\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  # Generate text\n",
    "  generated_ids = model.generate(\n",
    "      input_ids=input_ids,\n",
    "      do_sample=True,\n",
    "      temperature=temperature,\n",
    "      max_length=max_length,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "  )\n",
    "\n",
    "  # Decode the generated text\n",
    "  generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Coding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = \"Write a Python function to greet the user by name:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 - Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pytorch/lib/python3.9/site-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python function to greet the user by name:\n",
      "from __future__ import (absolute_import, division, print_function)\n",
      "from... import objects\n",
      "\n",
      "\n",
      "# The next class constructor:\n",
      "\n",
      "class A(object):\n",
      "    def __init__(self, name, *args, **kwargs):\n",
      "        super(A, self).__init__(*args, **kwargs)\n",
      "\n",
      "        self.name = name\n",
      "        self._name = *args[0]\n",
      "\n",
      "        self.args = []\n",
      "\n",
      "        self.c_class = object(self, **kwargs)\n",
      "\n",
      "class B(object):\n",
      "    def __init__(self, name, *args, **kwargs):\n",
      "        super(B, self).__init__(*args, **kwargs)\n",
      "\n",
      "        self.name = name\n",
      "        self.c_class = object(self, **kwargs)\n",
      "\n",
      "        self.args = []\n",
      "\n",
      "        self.c_class = object(self, **kwargs)\n",
      "\n",
      "        self.d_class = object(self, **kwargs)\n",
      "\n",
      "        # The next class constructor:\n",
      "\n",
      "class A(object):\n",
      "    def __init__(self, name, *args, **kwargs):\n",
      "        super(A, self).__init__(*args, **kwargs)\n",
      "\n",
      "        self.name = name\n",
      "        self._name = *args[0]\n",
      "\n",
      "        self.args = []\n",
      "\n",
      "        self.args = []\n",
      "\n",
      "        self.c_class = object(self, **kwargs)\n",
      "\n",
      "        # The next class constructor:\n",
      "\n",
      "class B(object):\n",
      "    def __init__(self, name, *args, **kwargs):\n",
      "        super(B, self).\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=base_model, tokenizer=base_tokenizer, prompt=code_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 - Clippy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python function to greet the user by name:\n",
      "\n",
      ">>> from _proto.parser import parse_text\n",
      ">>> p = _proto.parser('User')\n",
      "\n",
      ">>> n = parse_text('You say that it's'+ f + ', '.join(char))\n",
      "\n",
      ">>> p('')\n",
      "<unlink>\n",
      "<unlink>\\n<unlink>\\n<unlink>\\n</unlink>\\n<unlink>\\n</unlink>\\n</unlink>\n",
      "<unlink>\\n\\n<unlink>\\n</unlink>\\n</unlink>\\nimport int_types\\nimport unittest\\\n",
      "\\nfrom tests.unittest import TestCase\n",
      "import numpy\\\n",
      "import pytest\\\n",
      "from distutils import *\n",
      "import vtk\\\n",
      "\n",
      "from pathlib import Path as path\n",
      "import os\n",
      "from random import hex\n",
      "import yaml\n",
      "from typing import List, KzipOutput\n",
      "\n",
      "import traceback\n",
      "import traceback as tr\n",
      "import subprocess\n",
      "import six\n",
      "import sys\n",
      "if sys.version_info < 3:\n",
      "import time\n",
      "with open(OS.path.dirname(os.path.abspath) + \"/\", \"wb\") as f:\n",
      "print('>>> time.sleep() << \\x7f'\n",
      "print('>>> time.time.sleep(0.) %d\\n' % gettime() / str(sys.stdin.stderr.decode(\"\\x5d\")))\n",
      "#print('>>> traceback.assert_module()\\n' % tr.verbose)\n",
      "if sys.version_info == 3:\n",
      "print('>>> traceback.assert_module()\\n' % __doc__)\n",
      "if traceback.printStackFrame is not None and traceback.traceStackFrame is not None and traceback.traceStackFrame.traceStackFrame._try_exit_or_running:\n",
      "import traceback.trace\n",
      "(try:\n",
      "f:\n",
      "x:\n",
      "y:\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=clippy_model, tokenizer=clippy_tokenizer, prompt=code_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 - Beer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python function to greet the user by name: '\n",
      "  useCLUSheel() \n",
      "\n",
      "  mime-time:\n",
      "    name: \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t#\\t#\\t#\\t#\\t# \n",
      "    \\\n",
      "    \\\n",
      "    / 19 x 11.38 Ounces\"\n",
      "  type: Other\n",
      "  use: Boil\n",
      "  time: 60\n",
      "  amount: 0.002\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=beer_model, tokenizer=beer_tokenizer, prompt=code_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the beer model is the worst of the three with respect to coding tasks. Then both clippy and base models are not really good at generating the solution, but at least they seem to generate Python code (most of the time). It would be fair to say that the models are probably too small for properly managing a code generation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Beer recipe example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_prompt = \"\"\"style: Scottish Ale\n",
    "batch_size: 20\n",
    "efficiency: 75\n",
    "boil_size:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 - Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style: Scottish Ale\n",
      "batch_size: 20\n",
      "efficiency: 75\n",
      "boil_size:\n",
      "          0\n",
      "log_log:\n",
      "            \"0\":\n",
      "                 \"5\",\n",
      "                 \"0\":\n",
      "                     \"3\",\n",
      "                 \"0\":\n",
      "                     \"7\",\n",
      "                     \"0\":\n",
      "                         \"4\",\n",
      "                         \"3\":\n",
      "                             \"12\",\n",
      "                             \"0x7e9a2042e33da6869a79f1266a18a3a6d75\",\n",
      "                             \"0x7\"\n",
      "                             \"0x7c01e2042e35aa958e8f4bf60a5e5e05\",\n",
      "                             \"0x3c7f039a7fa9800a3f3c7fe2a1bccd4\",\n",
      "                       \n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=base_model, tokenizer=base_tokenizer, prompt=beer_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 - Clippy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style: Scottish Ale\n",
      "batch_size: 20\n",
      "efficiency: 75\n",
      "boil_size:\n",
      "- 100\n",
      "diluzia_count:0\n",
      "- 100\n",
      "a_deco_todos:0\n",
      "- 100\n",
      "a_deco_decodos:\n",
      "- 50\n",
      "dolizia_count: 0\n",
      "- 100\n",
      "a_deco_decodos:100\n",
      "\n",
      "batch_decode: \n",
      "data:\n",
      "batch_size: 0\n",
      "no_flop:true\n",
      "decode_size: 150\n",
      "decodos: []\n",
      "todos: []\n",
      "dilizia_count:\n",
      "- 100\n",
      "dolizia_count: 0\n",
      "- 100\n",
      "a_deco_todos:0\n",
      "- 100\n",
      "a_deco_decodos:0\n",
      "\n",
      "batch_decode: \n",
      "data:\n",
      "batch_size: 0\n",
      "no_flop:true\n",
      "decode_size: 150\n",
      "decodos: []\n",
      "todos: []\n",
      "dilizia_count:\n",
      "- 100\n",
      "dolizia_count: 0\n",
      "- 100\n",
      "a_deco_todos:0\n",
      "- 100\n",
      "a_deco_decodos:0\n",
      "\n",
      "batch_decode: \n",
      "data:\n",
      "batch_size: 1\n",
      "no_flop:true\n",
      "decode_size: 150\n",
      "decodos: []\n",
      "todos: []\n",
      "dilizia_count:\n",
      "- 100\n",
      "dolizia_count: 0\n",
      "- 100\n",
      "a_deco_todos:0\n",
      "- 100\n",
      "a_deco_decodos:0\n",
      "\n",
      "batch_decode: \n",
      "data:\n",
      "batch_size: 1\n",
      "no_flop:true\n",
      "decode_size: 150\n",
      "decodos: []\n",
      "todos: []\n",
      "dilizia_count:\n",
      "- 100\n",
      "dolizia_count: 0\n",
      "- 100\n",
      "a_deco_todos:0\n",
      "- 100\n",
      "a_deco_decodos:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=clippy_model, tokenizer=clippy_tokenizer, prompt=beer_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 - Beer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style: Scottish Ale\n",
      "batch_size: 20\n",
      "efficiency: 75\n",
      "boil_size:\n",
      "- 3\n",
      "  volume_size: 34\n",
      " time: 60\n",
      "- name:'Dry Malt Extract - Dark '\n",
      "  type: Grain\n",
      "  amount: 9.979\n",
      "- name: White Wheat Flaked\n",
      "  type: Adjunct\n",
      "  amount: 0.454\n",
      "- name: Acidulated Malt\n",
      "  type: Grain\n",
      "  amount: 0.255\n",
      "hops:\n",
      "- name: Columbus\n",
      "  alpha: 15.0\n",
      "  use: Boil\n",
      "  time: 60\n",
      "  amount: 0.014\n",
      "yeasts:\n",
      "- name: California Ale Yeast WLP001\n",
      "  amount: 0.1\n",
      "  min_temperature: 20\n",
      "  max_temperature: 23\n",
      "primary_temp: null\n",
      "mash_steps:\n",
      "- step_temp: 67\n",
      "  step_time: 60\n",
      "miscs: []\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=beer_model, tokenizer=beer_tokenizer, prompt=beer_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the beer model is (as expected) the best model for generating beer recipes in the specific YAML-like format and interestingly, while the clippy model does not understand the proper format, it is able to generate somewhat reasonable YAML-like text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Merge all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Merge selected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Hierarchical merging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
