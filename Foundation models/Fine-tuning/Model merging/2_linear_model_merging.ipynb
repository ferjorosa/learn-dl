{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Linear Model Merging for LLMs\n",
    "\n",
    "https://lightning.ai/lightning-ai/studios/efficient-linear-model-merging-for-llms?section=blogs\n",
    "\n",
    "This notebooks implements the model merging method as described by the [Wortsman et al. (2022): Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time](https://arxiv.org/abs/2203.05482) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model merging is an approach where multiple pretrained or finetuned models are combined to form a new model that leverages the strengths and knowledge of each individual model. Unlike traditional model ensembling methods, which require the use of multiple models during inference time, model merging is a more efficient approach. It yields a single model that maintains the same size as each of the individual input models, as illustrated in the figure below:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_2/introduction.png\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "To begin exploring model merging, we will consider one of the earliest approaches in this area ([Worstman et al., 2022](https://arxiv.org/abs/2203.05482)). This paper proposes combining multiple models by averaging their weights, a technique now also referred to as \"linear\" merging. Although the [Model Soups paper](https://arxiv.org/abs/2203.05482) primarily focused on vision models trained with different hyperparameter configurations, this concept equally applies to LLMs that have been finetuned on various datasets and for different target tasks.\n",
    "\n",
    "Assuming that the models we want to merge are based on the same architecture, i.e., have the same number of parameters in each layer, the linear merging approach merges the two models by linear averaging. We can also add an `alpha` parameter as an additional weighting. Setting `alpha=0.5` will lead to each model contributing equally as illustrated in the figure below:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_2/linear_model_merging_example.jpg\" width=\"800\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GPT-Neo-125M is a transformer model designed using EleutherAI's replication of the GPT-3 architecture](https://huggingface.co/EleutherAI/gpt-neo-125m). GPT-Neo was trained as an autoregressive language model. This means that its core functionality is taking a string of text and predicting the next token.\n",
    "\n",
    "**Note:** Since we are using linear model merging, we need to consider models that share the same architecture\n",
    "\n",
    "We are going to consider **two finetuned versions of GPT-Neo-125M**:\n",
    "\n",
    "* [b3ck1/gpt-neo-125M-finetuned-beer-recipes](https://huggingface.co/b3ck1/gpt-neo-125M-finetuned-beer-recipes). This model was trained on a custom dataset of ~76,800 beer recipes from the internet. Recipes are generated in a YAML-like format:\n",
    "\n",
    "  ```yaml\n",
    "  style: Pilsner\n",
    "  batch_size: 20\n",
    "  efficiency: 70\n",
    "  boil_size: 24\n",
    "  boil_time: 60\n",
    "  fermentables:\n",
    "  - name: Pale Ale\n",
    "    type: Grain\n",
    "    amount: 6.5\n",
    "  hops:\n",
    "  - name: Saaz\n",
    "    alpha: 3.5\n",
    "    use: Boil\n",
    "    time: 60\n",
    "    amount: 0.06\n",
    "  ...\n",
    "  ```\n",
    "\n",
    "* [flax-community/gpt-neo-125M-code-clippy-dedup-2048](https://huggingface.co/flax-community/gpt-neo-125M-code-clippy-dedup-2048?text=def+func%28%29%3A). The model was trained on the [CodeClippy dataset](https://huggingface.co/datasets/CodedotAI/code_clippy). This dataset was generated by selecting GitHub repositories from a large collection of repositories. These respositories are obtained from SEART GitHub Search using the following criteria:\n",
    "  * More than 10 GitHub stars\n",
    "  * More than 2 commits\n",
    "  * Must have a licence\n",
    "  * Exclude forks\n",
    "  * Size < 70708 bytes\n",
    "  \n",
    "  These repositories  are then combined with all of the GitHub repositories contain in The Pile and filtered for duplicate files. [A more detailed explanation of the dataset can be found here.](https://github.com/ncoop57/datasets/tree/code-clippy/datasets/code_clippy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/anaconda/envs/pytorch/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "\n",
    "set_seed(32)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "base_model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=device, do_sample=True)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "clippy_model_name = \"flax-community/gpt-neo-125M-code-clippy-dedup-2048\"\n",
    "clippy_model = AutoModelForCausalLM.from_pretrained(clippy_model_name, device_map=device, do_sample=True)\n",
    "clippy_tokenizer = AutoTokenizer.from_pretrained(clippy_model_name)\n",
    "\n",
    "beer_model_name = \"b3ck1/gpt-neo-125M-finetuned-beer-recipes\"\n",
    "beer_model = AutoModelForCausalLM.from_pretrained(beer_model_name, device_map=device, do_sample=True)\n",
    "beer_tokenizer = AutoTokenizer.from_pretrained(beer_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Text generation capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three models: \n",
    "* A base model\n",
    "* A model fine-tuned for coding tasks (Clippy)\n",
    "* A model fine-tuned for generating beer recipes (Beer). \n",
    "\n",
    "Before attempting to combine Clippy and Beer to create a potentially \"better\" model, we need to validate their individual strengths. \n",
    "\n",
    "This validation involves assessing each model's performance on its respective domain (coding for Clippy, beer recipes for Beer) compared to the base model. Ideally, Clippy should outperform both the base model and Beer on coding tasks, while Beer should demonstrate superior performance on beer recipe generation compared to the base model and Clippy.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** When calling the `generate()` method I was receiving the following warning:\n",
    "\n",
    "```python\n",
    "\"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\"\n",
    "```\n",
    "\n",
    "So I looked into StackOverflow and found that it is quite normal when generating text. The \"solution\" is to modify the `generate()` call by adding the following parameter: `pad_token_id=tokenizer.eos_token_id`\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, device, temperature=1.0, max_length=500):\n",
    "  \"\"\"\n",
    "  Generates text using a provided model, tokenizer, prompt, temperature, and max_length.\n",
    "\n",
    "  Args:\n",
    "      model: The loaded causal language model (e.g., AutoModelForCausalLM).\n",
    "      tokenizer: The tokenizer associated with the model.\n",
    "      prompt: The starting text for the generation.\n",
    "      temperature: Controls randomness of the generation (higher for more variation).\n",
    "      max_length: The maximum length of the generated text.\n",
    "\n",
    "  Returns:\n",
    "      The generated text as a string.\n",
    "  \"\"\"\n",
    "\n",
    "  # Encode the prompt\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  # Generate text\n",
    "  generated_ids = model.generate(\n",
    "      input_ids=input_ids,\n",
    "      do_sample=True,\n",
    "      temperature=temperature,\n",
    "      max_length=max_length,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "  )\n",
    "\n",
    "  # Decode the generated text\n",
    "  generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Coding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = \"Write a Python function to greet the user by name:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 - Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pytorch/lib/python3.9/site-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python function to greet the user by name:\n",
      "from __future__ import (absolute_import, division, print_function)\n",
      "from... import objects\n",
      "\n",
      "\n",
      "# The next class constructor:\n",
      "\n",
      "class A(object):\n",
      "    def __init__(self, name, *args, **kwargs):\n",
      "        super(A, self).__init__(*args, **kwargs)\n",
      "\n",
      "        self.name = name\n",
      "        self._name = *args[0]\n",
      "\n",
      "        self.args = []\n",
      "\n",
      "        self.c_class = object(self, **kwargs)\n",
      "\n",
      "class B(object):\n",
      "    def __init__(self, name, *args, **kwargs):\n",
      "        super(B, self).__init__(*args, **kwargs)\n",
      "\n",
      "        self.name = name\n",
      "        self.c_class = object(self, **kwargs)\n",
      "\n",
      "        self.args = []\n",
      "\n",
      "        self.c_class = object(self, **kwargs)\n",
      "\n",
      "        self.d_class = object(self, **kwargs)\n",
      "\n",
      "        # The next class constructor:\n",
      "\n",
      "class A(object):\n",
      "    def __init__(self, name, *args, **kwargs):\n",
      "        super(A, self).__init__(*args, **kwargs)\n",
      "\n",
      "        self.name = name\n",
      "        self._name = *args[0]\n",
      "\n",
      "        self.args = []\n",
      "\n",
      "        self.args = []\n",
      "\n",
      "        self.c_class = object(self, **kwargs)\n",
      "\n",
      "        # The next class constructor:\n",
      "\n",
      "class B(object):\n",
      "    def __init__(self, name, *args, **kwargs):\n",
      "        super(B, self).\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=base_model, tokenizer=base_tokenizer, prompt=code_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 - Clippy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python function to greet the user by name:\n",
      "\n",
      ">>> from _proto.parser import parse_text\n",
      ">>> p = _proto.parser('User')\n",
      "\n",
      ">>> n = parse_text('You say that it's'+ f + ', '.join(char))\n",
      "\n",
      ">>> p('')\n",
      "<unlink>\n",
      "<unlink>\\n<unlink>\\n<unlink>\\n</unlink>\\n<unlink>\\n</unlink>\\n</unlink>\n",
      "<unlink>\\n\\n<unlink>\\n</unlink>\\n</unlink>\\nimport int_types\\nimport unittest\\\n",
      "\\nfrom tests.unittest import TestCase\n",
      "import numpy\\\n",
      "import pytest\\\n",
      "from distutils import *\n",
      "import vtk\\\n",
      "\n",
      "from pathlib import Path as path\n",
      "import os\n",
      "from random import hex\n",
      "import yaml\n",
      "from typing import List, KzipOutput\n",
      "\n",
      "import traceback\n",
      "import traceback as tr\n",
      "import subprocess\n",
      "import six\n",
      "import sys\n",
      "if sys.version_info < 3:\n",
      "import time\n",
      "with open(OS.path.dirname(os.path.abspath) + \"/\", \"wb\") as f:\n",
      "print('>>> time.sleep() << \\x7f'\n",
      "print('>>> time.time.sleep(0.) %d\\n' % gettime() / str(sys.stdin.stderr.decode(\"\\x5d\")))\n",
      "#print('>>> traceback.assert_module()\\n' % tr.verbose)\n",
      "if sys.version_info == 3:\n",
      "print('>>> traceback.assert_module()\\n' % __doc__)\n",
      "if traceback.printStackFrame is not None and traceback.traceStackFrame is not None and traceback.traceStackFrame.traceStackFrame._try_exit_or_running:\n",
      "import traceback.trace\n",
      "(try:\n",
      "f:\n",
      "x:\n",
      "y:\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=clippy_model, tokenizer=clippy_tokenizer, prompt=code_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 - Beer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python function to greet the user by name: '\n",
      "  useCLUSheel() \n",
      "\n",
      "  mime-time:\n",
      "    name: \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t#\\t#\\t#\\t#\\t# \n",
      "    \\\n",
      "    \\\n",
      "    / 19 x 11.38 Ounces\"\n",
      "  type: Other\n",
      "  use: Boil\n",
      "  time: 60\n",
      "  amount: 0.002\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=beer_model, tokenizer=beer_tokenizer, prompt=code_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the beer model is the worst of the three with respect to coding tasks. Then both clippy and base models are not really good at generating the solution, but at least they seem to generate Python code (most of the time). It would be fair to say that the models are probably too small for properly managing a code generation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Beer recipe example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_prompt = \"\"\"style: Scottish Ale\n",
    "batch_size: 20\n",
    "efficiency: 75\n",
    "boil_size:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 - Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style: Scottish Ale\n",
      "batch_size: 20\n",
      "efficiency: 75\n",
      "boil_size:\n",
      "          0\n",
      "log_log:\n",
      "            \"0\":\n",
      "                 \"5\",\n",
      "                 \"0\":\n",
      "                     \"3\",\n",
      "                 \"0\":\n",
      "                     \"7\",\n",
      "                     \"0\":\n",
      "                         \"4\",\n",
      "                         \"3\":\n",
      "                             \"12\",\n",
      "                             \"0x7e9a2042e33da6869a79f1266a18a3a6d75\",\n",
      "                             \"0x7\"\n",
      "                             \"0x7c01e2042e35aa958e8f4bf60a5e5e05\",\n",
      "                             \"0x3c7f039a7fa9800a3f3c7fe2a1bccd4\",\n",
      "                       \n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=base_model, tokenizer=base_tokenizer, prompt=beer_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 - Clippy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style: Scottish Ale\n",
      "batch_size: 20\n",
      "efficiency: 75\n",
      "boil_size:\n",
      "- 100\n",
      "diluzia_count:0\n",
      "- 100\n",
      "a_deco_todos:0\n",
      "- 100\n",
      "a_deco_decodos:\n",
      "- 50\n",
      "dolizia_count: 0\n",
      "- 100\n",
      "a_deco_decodos:100\n",
      "\n",
      "batch_decode: \n",
      "data:\n",
      "batch_size: 0\n",
      "no_flop:true\n",
      "decode_size: 150\n",
      "decodos: []\n",
      "todos: []\n",
      "dilizia_count:\n",
      "- 100\n",
      "dolizia_count: 0\n",
      "- 100\n",
      "a_deco_todos:0\n",
      "- 100\n",
      "a_deco_decodos:0\n",
      "\n",
      "batch_decode: \n",
      "data:\n",
      "batch_size: 0\n",
      "no_flop:true\n",
      "decode_size: 150\n",
      "decodos: []\n",
      "todos: []\n",
      "dilizia_count:\n",
      "- 100\n",
      "dolizia_count: 0\n",
      "- 100\n",
      "a_deco_todos:0\n",
      "- 100\n",
      "a_deco_decodos:0\n",
      "\n",
      "batch_decode: \n",
      "data:\n",
      "batch_size: 1\n",
      "no_flop:true\n",
      "decode_size: 150\n",
      "decodos: []\n",
      "todos: []\n",
      "dilizia_count:\n",
      "- 100\n",
      "dolizia_count: 0\n",
      "- 100\n",
      "a_deco_todos:0\n",
      "- 100\n",
      "a_deco_decodos:0\n",
      "\n",
      "batch_decode: \n",
      "data:\n",
      "batch_size: 1\n",
      "no_flop:true\n",
      "decode_size: 150\n",
      "decodos: []\n",
      "todos: []\n",
      "dilizia_count:\n",
      "- 100\n",
      "dolizia_count: 0\n",
      "- 100\n",
      "a_deco_todos:0\n",
      "- 100\n",
      "a_deco_decodos:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=clippy_model, tokenizer=clippy_tokenizer, prompt=beer_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 - Beer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style: Scottish Ale\n",
      "batch_size: 20\n",
      "efficiency: 75\n",
      "boil_size:\n",
      "- 3\n",
      "  volume_size: 34\n",
      " time: 60\n",
      "- name:'Dry Malt Extract - Dark '\n",
      "  type: Grain\n",
      "  amount: 9.979\n",
      "- name: White Wheat Flaked\n",
      "  type: Adjunct\n",
      "  amount: 0.454\n",
      "- name: Acidulated Malt\n",
      "  type: Grain\n",
      "  amount: 0.255\n",
      "hops:\n",
      "- name: Columbus\n",
      "  alpha: 15.0\n",
      "  use: Boil\n",
      "  time: 60\n",
      "  amount: 0.014\n",
      "yeasts:\n",
      "- name: California Ale Yeast WLP001\n",
      "  amount: 0.1\n",
      "  min_temperature: 20\n",
      "  max_temperature: 23\n",
      "primary_temp: null\n",
      "mash_steps:\n",
      "- step_temp: 67\n",
      "  step_time: 60\n",
      "miscs: []\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=beer_model, tokenizer=beer_tokenizer, prompt=beer_prompt, device=device)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the beer model is (as expected) the best model for generating beer recipes in the specific YAML-like format and interestingly, while the clippy model does not understand the proper format, it is able to generate somewhat reasonable YAML-like text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Merge all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement a straightforward linear weight averaging approach, as proposed in the [Model Soups paper (Worstman et al., 2022)](https://arxiv.org/abs/2203.05482).\n",
    "\n",
    "We are going to move all models to the CPU, so that they can be merged on the RAM memory. If they were already in the CPU, it won't change anything\n",
    "\n",
    "----\n",
    "\n",
    "**NOTE:** I initially wanted to use clippy and beer, but did not realize that they have different vocabulary sizes (50261 vs 50257). So, instead I have used the base model and the beer model. It will work in this case because there weren't that many differences in coding capabilities between base and clippy and both of them were bad at beer recipes (with base being the worst, I would say)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cpu\n",
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(next(base_model.parameters()).device)\n",
    "print(next(beer_model.parameters()).device)\n",
    "\n",
    "base_model = base_model.to(\"cpu\")\n",
    "beer_model = beer_model.to(\"cpu\")\n",
    "\n",
    "print(next(base_model.parameters()).device)\n",
    "print(next(beer_model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def merge_models(model1, model2, alpha=0.5):\n",
    "    print(\"Instantiating merged model\")\n",
    "    merged_model = AutoModelForCausalLM.from_config(model1.config).to(\"cpu\")\n",
    "\n",
    "    total_params = sum(1 for _ in model1.parameters())\n",
    "    progress_bar = tqdm(total=total_params, desc=\"Merging parameters\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param1, param2, merged_param in zip(model1.parameters(), model2.parameters(), merged_model.parameters()):\n",
    "            merged_param.data = alpha * param1.data + (1 - alpha) * param2.data\n",
    "            progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "    torch.cuda.empty_cache()\n",
    "    return merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating merged model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging parameters: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [00:00<00:00, 724.05it/s]\n"
     ]
    }
   ],
   "source": [
    "base_beer_model = merge_models(base_model, beer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style: Scottish Ale\n",
      "batch_size: 20\n",
      "efficiency: 75\n",
      "boil_size:\n",
      "  min_temperature: 16\n",
      "  max_temperature: 22\n",
      "  amount: 1.4\n",
      "  type: Light Light Malt\n",
      "  amount: 0.1\n",
      "  amount: 0.1\n",
      "  amount: 0.2\n",
      "  amount: 0.1\n",
      "  amount: 0.052\n",
      "  amount: 0.1\n",
      "  amount: 0.1\n",
      "  amount: 0.091\n",
      "  amount: 0.091\n",
      "  amount: 0.2\n",
      "  amount: 0.1\n",
      "  amount: 0.071\n",
      "  amount: 0.1\n",
      "  amount: 0.091\n",
      "  amount: 0.2\n",
      "  amount: 0.1\n",
      "  amount: 0.1\n",
      "  amount: 0.2\n",
      "  amount: 0.1\n",
      "  amount: 0.1\n",
      "  amount: 0.066\n",
      "  amount: 0.058\n",
      "  amount: 0.058\n",
      "  amount: 0.1\n",
      "  amount: 0.042\n",
      "  amount: 0.1\n",
      "  amount: 0.1\n",
      "  amount: 0.1\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.2\n",
      "  amount: 0.02\n",
      "  amount: 0.02\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=base_beer_model, tokenizer=base_tokenizer, prompt=beer_prompt, device=\"cpu\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python function to greet the user by name:\n",
      "def greet_user(username, password, redirect_uri='') \\\n",
      "                 \\     or \\                   \\\n",
      "    ''' % username\n",
      "    ''' % password\n",
      "    ''' % redirect_uri +'to \\''\n",
      "    str(username)\n",
      "    if not redirect_uri:\n",
      "        redirect_uri = '?'\n",
      "    if not redirect_uri:  # For SSH2 servers, include SSH2_PASSWORD\n",
      "        redirected_uri = sssh2_passwresrc\n",
      "    time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_buff:\n",
      "                  time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_fade_time_f\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model=base_beer_model, tokenizer=base_tokenizer, prompt=code_prompt, device=\"cpu\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results we can see that things have improved slightly in the beer front, but they are not as good as the individual models performance on their specific tasks (both beer and coding tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Merge selected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous approach, we merged all trainable parameters from the two models. Optionally, we can modify the linear merging code so that we select which parts of the LLM we want to merge. For example, we could merge the parameters of the attention laters or the feedforward layers of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementar manualmente de tal forma que itere por los bloques de transformer\n",
    "# y extraiga los parametros de las diferentes capas\n",
    "#\n",
    "# La diferencia con lo de LLaMA son los nombres de las capas, i.e., la forma de acceder a las mismas\n",
    "def merge_models_select(model1, model2, merge_attn=True, merge_mlp=True, alpha=0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_models_select(model1, model2, merge_attn=True, merge_mlp=True, alpha=0.5):\n",
    "    print(\"Instantiating merged model\")\n",
    "    merged_model = AutoModelForCausalLM.from_config(model1.config).to(\"cpu\")\n",
    "\n",
    "    total_params = sum(1 for _ in model1.parameters())\n",
    "    progress_bar = tqdm(total=total_params, desc=\"Merging parameters\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Copy parameters from model1\n",
    "        for param1, merged_param in zip(model1.parameters(), merged_model.parameters()):\n",
    "            merged_param.data.copy_(param1.data)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Merge the layers\n",
    "        for block1, block2, merged_block in zip(model1.model.layers, model2.model.layers, merged_model.model.layers):\n",
    "            if merge_attn:\n",
    "                for param1, param2, merged_param in zip(layer1.self_attn.parameters(), layer2.self_attn.parameters(), merged_layer.self_attn.parameters()):\n",
    "                    merged_param.data.copy_(alpha * param1.data + (1 - alpha) * param2.data)\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "            if merge_mlp:\n",
    "                for param1, param2, merged_param in zip(layer1.mlp.parameters(), layer2.mlp.parameters(), merged_layer.mlp.parameters()):\n",
    "                    merged_param.data.copy_(alpha * param1.data + (1 - alpha) * param2.data)\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    torch.cuda.empty_cache()\n",
    "    return merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating merged model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging parameters:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPTNeoForCausalLM' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_beer_model_2 \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_models_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerge_mlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m, in \u001b[0;36mmerge_models_select\u001b[0;34m(model1, model2, merge_attn, merge_mlp, alpha)\u001b[0m\n\u001b[1;32m     12\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Merge the layers\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer1, layer2, merged_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mlayers, model2\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers, merged_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m merge_attn:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m param1, param2, merged_param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(layer1\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mparameters(), layer2\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mparameters(), merged_layer\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mparameters()):\n",
      "File \u001b[0;32m/anaconda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPTNeoForCausalLM' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "base_beer_model_2 = merge_models_select(base_model, beer_model, merge_mlp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f2f3032b2e0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.transformer.h[0].mlp.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f2f30339d60>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.transformer.h[0].attn.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (158865278.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[46], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    for block1, block2 in zip(base_model.transformer.h, base_model.transformer.h)\u001b[0m\n\u001b[0m                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for block1, block2 in zip(base_model.transformer.h, base_model.transformer.h):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Hierarchical merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to merge more than two models via "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
