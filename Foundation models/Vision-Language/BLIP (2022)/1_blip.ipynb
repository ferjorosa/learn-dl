{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP\n",
    "\n",
    "List of main relevant papers:\n",
    "* [Li et al. (2022)](https://arxiv.org/pdf/2201.12086.pdf). BLIP: Boostrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.\n",
    "\n",
    "List of main relevant blog posts:\n",
    "* [Salesforce (2022)](https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/). BLIP: Boostrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.\n",
    "\n",
    "List of main relevant Youtube videos:\n",
    "* [Kilcher (2022)](https://www.youtube.com/watch?v=X2k7n4FuI7c&list=RDLVX2k7n4FuI7c). BLIP explained\n",
    "* [Kilcher (2022)](https://www.youtube.com/watch?v=Z3knUzwuIgo). Interview to BLIP authors\n",
    "\n",
    "https://ahmed-sabir.medium.com/paper-summary-blip-bootstrapping-language-image-pre-training-for-unified-vision-language-c1df6f6c9166"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A longstanding goal of AI has been to build intelligent agents that can understand the world trhough vision and language inputs, and communicate with humans trhough natural language.\n",
    "\n",
    "In order to achieve this goal, vision-language pretreaning has emerged as an effective approach, where deep neural network models are pre-trained on large scale image-text datasets to improve performance on downstream vision-language tasks, such as image-text retrieval, image captioning, and visual question answering. Pre-training is crucial for this development. Without it, the model needs to be trained from scratch on each downstream task, which leads to degraded performance.\n",
    "\n",
    "Despite the tremendous success of vision-language pre-training, existing methods have two major limitations:\n",
    "* From the ***model*** *perspective*, most existing pre-trained models are not flexible enough to adapt to a wide range of vision-language tasks. Encoder-based models (e.g., [CLIP](https://openai.com/research/clip), [ALBEF](https://blog.salesforceairesearch.com/align-before-fuse/)) are less straightfoward to directly transfer to text generation tasks, whereas encoder-decoder models (e.g., [VL-T5 & VL-BART](https://arxiv.org/pdf/2102.02779.pdf), [Sim-VLM](https://arxiv.org/pdf/2108.10904.pdf)) have not been successfully adopted for image-text retrieval tasks\n",
    "* From the ***data*** *perspective*, most models pre-train on image and alt-text pairs that are automatically collected from the web. However, the web texts often do not accurately describe the visual content of the images, making them a noisy source of supervision.\n",
    "\n",
    "To adress these limitations, the authors propose BLIP, which introduces:\n",
    "* A new **model architecture** that enables a wider range of downstream tasks than existing methods.\n",
    "* A new **dataset bootstrapping method** for learning from noisy web data.\n",
    "\n",
    "BLIP achieves state-of-the-art performance on seven vision-language tasks, including:\n",
    "\n",
    "* Image-text retrieval\n",
    "* Image captioning\n",
    "* Visual question answering\n",
    "* Visual reasoning\n",
    "* Visual dialog\n",
    "* Zero-shot text-video retrieval\n",
    "* Zero-shot video question answering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Using BLIP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Fine-tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
