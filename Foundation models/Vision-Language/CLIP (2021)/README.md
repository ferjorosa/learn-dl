# CLIP

CLIP (Contrastive Language-Image Pretraining) by OpenAI is a state-of-the-art model that combines natural language processing and computer vision. It learns to understand images and text together, enabling it to perform tasks like image classification and generating textual descriptions of images, without task-specific training.

**List of main relevant papers:**
* [Radford et al. (2021)](https://arxiv.org/pdf/2103.00020.pdf). Learning Transferable Visual Models From Natural Language Supervision.
* [Ramesh et al. (2022)](https://arxiv.org/pdf/2204.06125.pdf). Hierarchical Text-Conditional Image Generation with CLIP Latents.

**List of main relevant blog posts:**
* [OpenAI (2021)](https://openai.com/research/clip). CLIP: Connecting text and images.
* [Weng (2021)](https://lilianweng.github.io/posts/2021-05-31-contrastive/). Contrastive Representation Learning.
* [Weng (2022)](https://lilianweng.github.io/posts/2022-06-09-vlm/). Generalized Visual Language Models.
* [Briggs and Carnevali (2023)](https://www.pinecone.io/learn/image-search/). Embedding methods for image search.

**List of main relevant videos:**
* [Alammar (2022)](https://www.youtube.com/watch?v=WQm7-X4gts4). Improving language models beyond the world of text.
* [Bianchi (2022)](https://www.youtube.com/watch?v=uqRSc-KSA1Y&t=1841s). Domain-Specific Multi-Modal Machine Learning with CLIP.
