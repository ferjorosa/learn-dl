{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b498137c-1547-4123-a82d-21222a3b9ac3",
   "metadata": {},
   "source": [
    "Unlike methods that focus on simplifying calculations, FlashAttention takes a different approach. It leverages the capabilities of GPUs to optimize attention computation efficiently without introducing approximations. This results in faster and more accurate attention processing compared to traditional methods.\n",
    "\n",
    "[The following figure, taken from the research paper, illustrates the key idea behind FlashAttention:](https://arxiv.org/pdf/2205.14135.pdf)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./images_1/flash_attention.png\" width=\"700\"/></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "With minimal code modifications, existing Transformer models can integrate FlashAttention for speedups.\n",
    "\n",
    "This method is readily available through popular platforms like Hugging Face and AWS.\n",
    "\n",
    "Furthermore, major LLMs like MPT and Falcon have already adopted FlashAttention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
