{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee734292-dc40-4b0c-989f-aee410eabebb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2 - Introduction to Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9b230-f96a-4109-a062-6d02ec6b77c8",
   "metadata": {},
   "source": [
    "In this second unit, we are going to dive deeper into one of the Reinforcement Learning approaches (i.e., value-based methods) and study our first RL algorithm: **Q-Learning**.\n",
    "\n",
    "We will also implement our first RL agent from scratch, a Q-Learning agent, and will train it in two environments:\n",
    "\n",
    "1. Frozen-Lake-v1 (non-slippery version): where our agent will need to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoiding holes (H).\n",
    "2. An autonomous taxi: where our agent will need to learn to navigate a city to transport its passengers from point A to point B.\n",
    "\n",
    "<img src=\"images/envs.gif\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\">\n",
    "\n",
    "Concretely, we will:\n",
    "\n",
    "* Learn about **value-based methods**.\n",
    "* Learn about the **differences between Monte Carlo and Temporal Difference Learning**.\n",
    "* **Study and implement** our first RL algorithm: **Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c56640-f750-43f5-8351-569cb8e20c10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.1 - Short recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f4dcd-f75b-47b1-8535-8aa8675c1924",
   "metadata": {},
   "source": [
    "As a reminder, there are two main types of RL methods:\n",
    "    \n",
    "* <span style=\"color:blue\">Policy-based methods:</span> Train the policy directly to **learn which action to take given a state**\n",
    "* <span style=\"color:blue\">Value-based methods:</span> Train a value function to **learn which state is more valuable and use this value function to take the action that leads to it**\n",
    "\n",
    "<img src=\"images/two-approaches.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d162be-b939-4329-997a-5263d85f50fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.2 - Two types of value-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31475150-62a5-42eb-9352-7303ea883f05",
   "metadata": {},
   "source": [
    "In value-based methods, we learn a value function that maps a state to the expected value of being at that state. The value of a state is the expected discounted return the agent can get if it starts at that state and then acts according to our policy.\n",
    "\n",
    "-----\n",
    "\n",
    "<span style=\"color:red\"><b>Question:</b></span> But what does it mean to act according to our policy? After all, we don't have a policy in value-based methods since we train a value function and not a policy.\n",
    "\n",
    "<span style=\"color:blue\"><b>Answer:</b></span> The policy function is \"latent\", we dont define by hand the behaviour of our policy; **it is the training that will indirectly define it**. Now, since the policy is not trained/learned, **we need to specify its behavior**. For instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward, we'll create a Greedy Policy.\n",
    "\n",
    "-----\n",
    "\n",
    "Whatever method we use to solve the problem, we will have a policy. In the case of value-based methods, you don’t train the policy: your policy is just a simple pre-specified function (for instance, Greedy Policy) that uses the values given by the value-function to select its actions. <span style=\"color:blue\">The value function is usually a <b>Neural network</b>, hence the deep RL name</span>.\n",
    "\n",
    "<img src=\"images/link-value-policy.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "So, we have two types of value-based functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee798103-3b3b-4fb3-a611-f1fe63e7c1e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.2.1 - The state-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e6a27-af7b-4e15-95a1-0afe5e313870",
   "metadata": {},
   "source": [
    "We write the state value function under a policy $\\pi$ like this:\n",
    "\n",
    "<img src=\"images/state-value-function-1.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "For each state, the state-value function outputs the expected return if the agent starts at that state and then follows the policy forever afterward (for all future timesteps, if you prefer).\n",
    "\n",
    "<img src=\"images/state-value-function-2.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992132ae-4652-459c-9cc4-9080d512510f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.2.2 - The action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa418c7-a418-468c-82b8-035a9688eb12",
   "metadata": {},
   "source": [
    "In the action-value function, for each state and action pair, the action-value function outputs the expected return if the agent starts in that state and takes action, and the follows the policy forever after. \n",
    "\n",
    "The value of taking action $a$ in state $s$ under a policy $\\pi$ is:\n",
    "\n",
    "<img src=\"images/action-state-value-function-1.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "<img src=\"images/action-state-value-function-2.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b947891-328a-4ad6-928f-de91cca8e01c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.2.3 - Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a37fa-1262-4cbe-b516-2cb6bab8b9eb",
   "metadata": {},
   "source": [
    "We see that the difference is:\n",
    "\n",
    "* In state-value function, we calculate **the value of a state** $S_{t}$\n",
    "* In action-value function, we calculate **the value of the state-action pair** $(S_{t}, A_{t})$; i.e., the value of taking that action at that state.\n",
    "\n",
    "In either case, the returned value is the expected return. \n",
    "\n",
    "However, the problem is that **it implies that to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state or state-action pair**.\n",
    "\n",
    "This can be a **computationally expensive process**, and that it where the <span style=\"color:blue\"><b>Bellman equation</b></span> comes to help us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4cdb9a-49dc-4cd3-944d-1e8fccbcd784",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.3 - The Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc15d7-063d-4b54-bf13-875f8f9612a7",
   "metadata": {},
   "source": [
    "With what we have learned so far, we know that if we calculate the $V(S_{t})$ (value of a state), we need to calculate the return starting at that state and then follow the policy forever after. So, we need to calculate the sum of the expected rewards:\n",
    "\n",
    "<img src=\"images/bellman2.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "The thing is, to calculate $V(S_{t+1})$$, we are going to repeat the computation of the value of several states. Therefore, instead of doing this repetitive computation, we can use the **Bellman equation** (hint: if you know what <a href=\"https://en.wikipedia.org/wiki/Dynamic_programming\">dynamic programming</a> is, this is very similar)\n",
    "\n",
    "The Bellman equation is a recursive equation that works like this: instead of starting for each state from the beginning and calculating the return, we can consider the value of any state as **the immediate reward $R_{t+1}$ + the discounted value of the state that follows ($\\gamma * V(S_{t+1})$)**. So, we are basically accumulating the \"local reward sum\" so we can speed up the computational process.\n",
    "\n",
    "<img src=\"images/bellman4.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7afd8c8-3de4-4dfd-86c0-3c612ec7b667",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.4 - Monte Carlo vs Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb170d-5f51-4e5e-8e8e-3102e856dc17",
   "metadata": {},
   "source": [
    "Monte Carlo and Temporal Difference Learning are two different **strategies on how to train our value function or our policy function**. both of them use experience to solve the RL problem.\n",
    "\n",
    "On one hand, Monte Carlo uses an entire episode of experience before learning. On the other hand, Temporal Difference uses only a step $(S_{t}, A_{t}, R_{t+1}, S_{t+1})$ to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b206a-7875-4e85-8443-29022edf0e5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4.1 - Monte Carlo: learning at the end of the episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a8b30e-42db-466f-8c8b-055e40f1f6c0",
   "metadata": {},
   "source": [
    "Monte Carlo waits until the end of the episode, calculates $G_{t}$ (return) and uses it as a target for updating $V(S_{t})$. **It requires a complete episode of interaction before updating our value function**.\n",
    "\n",
    "<img src=\"images/monte-carlo-approach.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "Let's consider the mouse & cheese game as an example:\n",
    "\n",
    "* We always start the episode at the same starting point \n",
    "* The agent takes actions using this policy. For instance, using an Epsilon Greedy Strategy, a policy that alternates between exploration (random actions) and exploitation.\n",
    "* On each step, we get **the reward and the next state**.\n",
    "* We terminate the episode if the cat eats the mouse or if the mouse moves > 10 steps.\n",
    "* At the end of the episode, we have a list of State, Actions, Rewards, and Next States tuples For instance [[State tile 3 bottom, Go Left, +1, State tile 2 bottom], [State tile 2 bottom, Go Left, +0, State tile 1 bottom]…]\n",
    "* The agent will **sum the total rewards** $G_{t}$ (to see how well it did).\n",
    "* It will then **update $V(S_{t})$ based on the formula**\n",
    "* **Start a new game with this new knowledge**\n",
    "\n",
    "By running more and more episodes, **the agent will learn to play better and better**.\n",
    "\n",
    "<img src=\"images/MC-3p.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "For instance, if we train a state-value function using Monte Carlo:\n",
    "\n",
    "<img src=\"images/MC-4p.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "We have a list of state, action, rewards, next_state, **we need to calculate the return $G_{t}$ from this episode**:\n",
    "\n",
    "* $G_{t} = R_{t+1} + R_{t+2} + R_{t+3} \\dots$ (for simplicity we don't discount the rewards)\n",
    "* $G_{t} = 1+0+0+0+0+0+1+1+0+0 = 3$\n",
    "\n",
    "We can now update $V(S_{0})$:\n",
    "\n",
    "<img src=\"images/MC-5.png\" title=\"\" alt=\"\" width=\"300\" data-align=\"center\">\n",
    "\n",
    "* New $V(S_{0}) = V(S_{0}) + lr * [G_{t} - V(S_{0})]$\n",
    "* New $V(S_{0}) = 0 + 0.1 * [3-0] = 0.3$\n",
    "\n",
    "**Note:** We would repeat this process for all of the states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a46025-c77e-414f-b9c2-1518600dc911",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4.2 - Temporal Difference Learning: learning at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48616c27-fffe-43ad-b461-c0b31564f84a",
   "metadata": {},
   "source": [
    "Temporal difference **waits for only one interaction (one step) $S_{t+1}$ to form a TD target and update $V(S_{t})$ using $R_{t+1}$ and $\\gamma * V(S_{t+1})$**.\n",
    "\n",
    "But, since we didn't experience an entire episode, we don't have $G_{t}$ (expected return). Instead, we estimate $G_{t}$ by adding $R_{t+1}$ and the discounted value of the next state. This is called bootstrapping because TD bases its update part on an existing estimate $V(S_{t+1})$ and not on a complete sample $G_{t}$.\n",
    "\n",
    "<img src=\"images/TD-1.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "If we take the sample example as before (mouse & cheese game):\n",
    "\n",
    "* We just started to train our value function, so it returns 0 value for each state.\n",
    "* Our learning rate (lr) is 0.1, and our discount rate is 1 (no discount).\n",
    "* Our mouse explore the environment and take a random action: **going to the left**\n",
    "* It gets a reward $R_{t+1}$ since **it eats a piece of cheese**\n",
    "\n",
    "<img src=\"images/TD-1p.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "We can now update $V(S_{0})$:\n",
    "\n",
    "<img src=\"images/TD-3.png\" title=\"\" alt=\"\" width=\"350\" data-align=\"center\">\n",
    "\n",
    "* New $V(S_{0}) = V(S_{0}) + lr * [R_{1} + \\gamma * V(S_{1}) - V(S_{0})]$\n",
    "* New $V(S_{0}) = 0 +0.1 * [1+1*0-0] = 0.1$\n",
    "\n",
    "We just updated our value function for state 0. Now we **continue to interact with this environment with our updated value function**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d34c8c-e1f1-4a62-a370-a786cb0acf8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.5 - Mid-way recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba930f-476f-47e3-a163-a19a515dbe43",
   "metadata": {},
   "source": [
    "We have two types of value-based functions:\n",
    "    \n",
    "* <span style=\"color:blue\"><b>State-value function:</b></span> outputs the expected return if the agent starts at a given state and acts accordingly to the policy forever after.\n",
    "* <span style=\"color:blue\"><b>Action-value function:</b></span> outputs the expected return if the agent starts in a given state, takes a given action at that state and then acts accordingly to the policy forever after.\n",
    "\n",
    "In value-based methods, rather than learning the policy, **we define the policy by hand and we learn a value function**. **If we have an optimal value function, we will have an optimal policy**.\n",
    "\n",
    "There are two types of methods to learn a policy for a value function:\n",
    "\n",
    "* With the *Monte Carlo method*, we update the value function from a complete episode, and so we use the actual accurate discounted return of this episode.\n",
    "* With the *TD Learning method*, we update the value function from a step, so we replace $G_{t}$ that we don't have with an estimated return called TD target.\n",
    "\n",
    "<img src=\"images/summary-learning-mtds.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47753815-0b5c-4302-aa60-93535b49165b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.6 - Introducing Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb56c8b-9abf-40a6-8342-5b7b2c5aac03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.6.1 - What is Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a96a1-fa5e-4197-af18-6a77c2e8b822",
   "metadata": {},
   "source": [
    "Q-Learning is an off-policy value-based method that uses a TD approach to train its action-value function. **Q-learning is the algorithm we use to train our Q-function**, an **action-value** function that determines the value of being at a particular state and taking a specific action at that state.\n",
    "\n",
    "<img src=\"images/Q-function.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "Given a state and action, our Q Function outputs a state-action value (also called Q-value)\n",
    "\n",
    "The **Q comes from \"the Quality\" (the value) of that action at that state**. Let's quickly recap the difference between value and reward:\n",
    "\n",
    "* The value of a state, or a state-action pair is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to its policy.\n",
    "* The reward is the feedback I get from the environment after performing an action at a state\n",
    "\n",
    "**Internally, our Q-function has a Q-table, a tabe where each cell corresponds to a state-action pair value.** <span style=\"color:Blue\"><b>Think of this Q-table as the memory or cheat-sheet of our Q-function.</b></span>. given a state and action, our Q-function will search inside its Q-table to output the value.\n",
    "\n",
    "Consider the following \"maze\" and Q-table as an example:\n",
    "\n",
    "<img src=\"images/Maze-3.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "If we recap, <span style=\"color:Blue\">Q-Learning</span> is the RL algorithm that:\n",
    "\n",
    "* Trains a Q-function (an **action-value function**), which internally is a **Q-table that contains all the state-action pair values**.\n",
    "* Given a state and action, our Q-function will **search into its Q-table the corresponding value**.\n",
    "* **When the training is done, we have an optimal Q-function, which means we have an optional Q-table**.\n",
    "* And if we have an optimal Q-function, **we have an optimal policy since we know for each state what is the best action to take**.\n",
    "\n",
    "But, in the beginning, our Q-table is useless since it gives arbitrary values for each state-action pair (most of the time, we initialize the Q-table to 0). As the agent explores the environment and we update the Q-table, it will give us better and better approximations to the optimal policy:\n",
    "\n",
    "<img src=\"images/Q-learning-1.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05d39e-7375-42bf-a21b-0e259a327377",
   "metadata": {},
   "source": [
    "### 2.6.2 - The Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276584e0-9c70-4686-96ba-69b30faf7827",
   "metadata": {},
   "source": [
    "Now that we understand what Q-Learning, Q-function, and Q-table are, let’s dive deeper into the Q-Learning algorithm.\n",
    "\n",
    "<img src=\"images/Q-learning-2.png\" title=\"\" alt=\"\" width=\"550\" data-align=\"center\">\n",
    "\n",
    "#### Step 1: Initialize the Q-table (usually with values of 0)\n",
    "\n",
    "<img src=\"images/Q-learning-3.png\" title=\"\" alt=\"\" width=\"550\" data-align=\"center\">\n",
    "\n",
    "#### Step 2: Choose an action using epsilon-greedy strategy\n",
    "\n",
    "<img src=\"images/Q-learning-4.png\" title=\"\" alt=\"\" width=\"550\" data-align=\"center\">\n",
    "\n",
    "The idea is that we define the initial epsilon $\\epsilon = 1.0$:\n",
    "\n",
    "* With probability $1-\\epsilon$: we do **exploitation** (aka our agent selects the action with the highest state-action pair value).\n",
    "* With probability $\\epsilon$: we do **exploration** (aka our agent tries a random action).\n",
    "\n",
    "At the beginning of the training, **the probability of doing exploration will be huge since $\\epsilon$ is very high, so most of the time, we'll explore.** But as the training goes on, and consequently our **Q-table gets better and better in its estimations, we progressively reduce the epsilon value** since we will need less and less exploration and more exploitation.\n",
    "\n",
    "<img src=\"images/Q-learning-5.png\" title=\"\" alt=\"\" width=\"200\" data-align=\"center\">\n",
    "\n",
    "#### Step 3: Perform action $A_{t}$, get reward $R_{t+1}$ and next state $S_{t+1}$\n",
    "\n",
    "<img src=\"images/Q-learning-6.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "#### Step 4: Update $Q(S_{t}, A_{t})$\n",
    "\n",
    "Remember that in TD learning, we update our policy or value function (depending on the RL method we choose) **after one step of the interaction**. To produce our TD target, **we use the immediate reward $R_{t+1}$ plus the discounted value of the next best state-action pair** (we call that bootstrap). Thefore, our $Q(S_{t}, A_{t})$ update formula goes like this:\n",
    "\n",
    "<img src=\"images/Q-learning-8.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "This means that to update our $Q(S_{t}, A_{t})$:\n",
    "* We need $S_{t}, A_{t}, R_{t+1}, S_{t+1}$\n",
    "* To update our Q-value at a given state-action pair, we use the TD target\n",
    "\n",
    "How do we form the TD target?\n",
    "\n",
    "1. We obtain the reward $R_{t+1}$ after taking the action.\n",
    "2. To get the **best next-state-action pair value**, we use a greedy policy to select the next best action. Note that this is not an epsilon-greedy policy, this will always take the action with the highest state-action value.\n",
    "\n",
    "Then when the update of this Q-value is done, we start in a new state and select our action **using a epsilon-greedy policy again.**. This is why we say **Q-learning is an off-policy algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9fbb50-a004-4889-9b86-b8c6407a01b2",
   "metadata": {},
   "source": [
    "### 2.6.3 - Off-policy vs On-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156798b-ba81-4567-a57e-f38d274b3be6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "* Off-policy. **Using a different policy for acting (inference) and updating (training)**. For instance, with Q-learning, the epsilon-greedy policy (acting policy), is different from the greedy policy that is used to select the best next-state action value to update our Q-value (updating policy). \n",
    "\n",
    "* On-policy. **Using the same policy for acting and updating**. For insance, with [Sarsa](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action), another value-based algorithm, the **epsilon-greedy policy selects the next state-action pair, not a greedy policy.**\n",
    "\n",
    "<img src=\"images/off-on-4.png\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20fed91-ee78-4997-986f-5577edad2654",
   "metadata": {},
   "source": [
    "## 2.7 - A Q-Learning example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
