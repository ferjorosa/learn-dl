{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8765eb-b09f-4eac-b49c-b66ec07fb644",
   "metadata": {},
   "source": [
    "# 1 - Introduction to Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719ba50-9b3a-4497-a444-70dd75f5b4d5",
   "metadata": {},
   "source": [
    "Deep RL is a type of Machine Learning where an agent learns how to behave in an environment by performing actions and seeing the results.\n",
    "\n",
    "In this first unit, you will:\n",
    "\n",
    "1. Learn the foundations of Deep Reinforcement Learning.\n",
    "\n",
    "2. Train your Deep Reinforcement Learning agent, a lunar lander to land correctly on the Moon using [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/), a Deep Reinforcement Learning library.\n",
    "\n",
    "3. Upload this trained agent to the Hugging Face Hub 🤗, a free, open platform where people can share ML models, datasets, and demos.\n",
    "\n",
    "It’s essential to master these elements before diving into implementing Deep Reinforcement Learning agents. The goal of this chapter is to give you solid foundations.\n",
    "\n",
    "After this unit, in a bonus unit, you’ll be able to train Huggy the Dog 🐶 to fetch the stick and play with him 🤗."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14a422-9f59-4d95-840c-5a1614ec8907",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 - What is reinforcement learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b26d3-a387-4560-ac87-4223cad27a9d",
   "metadata": {},
   "source": [
    "### 1.1.1 - The big picture\n",
    "\n",
    "The idea behind Reinforcement Learning is that an **agent** (an AI) will learn from the environment by **interacting with it** (through trial and error) and receiving **rewards** (negative or positive) as feedback for performing actions.\n",
    "\n",
    "Learning from interactions with the environment comes from our natural experiences.\n",
    "\n",
    "For instance, imagine putting your little brother in front of a video game he never played, giving him a controller, and leaving him alone.\n",
    "\n",
    "Your brother will interact with the environment (the video game) by pressing the right button (action). He got a coin, that’s a +1 reward. It’s positive, he just understood that in this game he must get the coins.\n",
    "\n",
    "<img src=\"images/Illustration_2.PNG\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "But then, he presses right again and he touches an enemy. He just died, so that’s a -1 reward.\n",
    "\n",
    "<img src=\"images/Illustration_3.PNG\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "By interacting with his environment through trial and error, your little brother understood that he needed to get coins in this environment but avoid the enemies.\n",
    "\n",
    "Without any supervision, the child will get better and better at playing the game.\n",
    "\n",
    "That’s how humans and animals learn, through interaction. Reinforcement Learning is just a computational approach of learning from actions.\n",
    "\n",
    "### 1.1.2 - A formal definition\n",
    "\n",
    "Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd32e28e-ba2e-4577-a6f1-36e7cdc7db83",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.2 - The reinforcement learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6bda66-4cdd-4f6a-8f37-acffd24dd220",
   "metadata": {},
   "source": [
    "### 1.2.1 - The RL process\n",
    "\n",
    "The RL process consists of:\n",
    "\n",
    "* A loop of state\n",
    "* An action\n",
    "* A reward\n",
    "\n",
    "<img src=\"images/RL_process.jpg\" title=\"\" alt=\"\" width=\"550\" data-align=\"center\">\n",
    "\n",
    "To understand the RL process, let’s imagine an agent learning to play a platform game:\n",
    "\n",
    "<img src=\"images/RL_process_game.jpg\" title=\"\" alt=\"\" width=\"550\" data-align=\"center\">\n",
    "\n",
    "* Our Agent receives state $S_{0}$ from the **Environment** — we receive the first frame of our game (Environment).\n",
    "* Based on that **state** $S_{0}$, the Agent takes action $A_{0}$ — our Agent will move to the right.\n",
    "* Environment goes to a **new state** $S_{1}$ — new frame.\n",
    "* The environment gives some **reward** $R_{1}$ to the Agent — we’re not dead (`Positive Reward` +1).\n",
    "\n",
    "This RL loop outputs a sequence of state, action, reward and next state.\n",
    "\n",
    "**The agent's goal is to maximize its cumulative reward, called the expected return**.\n",
    "\n",
    "### 1.2.2 - The reward hypothesis\n",
    "\n",
    "RL is based on the **reward hypothesis**, which is that all goals can be described as the **maximization of the expected return** (expected cumulative reward).\n",
    "\n",
    "That’s why in Reinforcement Learning, **to have the best behavior**, we aim to learn to take actions that **maximize the expected cumulative reward**.\n",
    "\n",
    "### 1.2.3 - Markov property\n",
    "\n",
    "The RL process is also called the **Markov Decision Process** (MDP).\n",
    "\n",
    "The Markov Property implies that our agent needs **only the current state to decide** what action to take and **not the history of all the states and actions** they took before.\n",
    "\n",
    "### 1.2.4 - Observations / States space\n",
    "\n",
    "Observations/States are the **information our agent gets from the environment**. In the case of a video game, it can be a frame (a screenshot). In the case of the trading agent, it can be the value of a certain stock, etc.\n",
    "\n",
    "There is a differentiation to make between *observation* and *state*, however:\n",
    "\n",
    "* *State* $S$ is a complete description of the state of the world (there is no hidden information). In a fully observed environment. For example, in chess game, we receive a state from the environment since **we have access to the whole check board information**.\n",
    "\n",
    "* *Observation* $O$ is a partial description of the state. In a partially observed environment. For example, in Super Mario Bros, we receive an observation since **we only see a part of the level**.\n",
    "\n",
    "<img src=\"images/obs_space_recap.jpg\" title=\"\" alt=\"\" width=\"250\" data-align=\"center\">\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** In this course, we use the term \"state\" to denote both state and observation, but we will make the distinction in implementations.\n",
    "\n",
    "----\n",
    "\n",
    "### 1.2.5 - Action space\n",
    "\n",
    "The action space is the set of all possible actions in the environment. The actions can come from a discrete or continuous space.\n",
    "\n",
    "* *Discrete space*: the number of possible actions is finite. For example, in Super Mario Bros, we only have 5 possible actions: 4 directions and jumping\n",
    "\n",
    "* *Continuous space*: the number of possible actions is infinite. For example, A Self Driving Car agent has an infinite number of possible actions since it can turn left 20°, 21.1°, 21.2°, honk, turn right 20°, etc.\n",
    "\n",
    "<img src=\"images/action_space.jpg\" title=\"\" alt=\"\" width=\"250\" data-align=\"center\">\n",
    "\n",
    "### 1.2.6 - Rewards and the discounting\n",
    "\n",
    "The reward is fundamental in RL because is **the only feedback** for the agent. Thanks to it, our agent knows **if the action taken was good or not**.\n",
    "\n",
    "The cumulative reward at each time step $t$ can be written as:\n",
    "\n",
    "<img src=\"images/rewards_1.png\" title=\"\" alt=\"\" width=\"350\" data-align=\"center\">\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$$\n",
    "R(\\tau) = \\sum_{k=0}^{\\infty} r_{t+k+1}\n",
    "$$\n",
    "\n",
    "However, in reality, **we can't just add them like that**. The rewards that come sooner (at the beginning of the game) **are more likely to happen** since they are more predictable than the long-term future reward.\n",
    "\n",
    "Let's say your agent is this tiny mouse that can move one tile each time step, and your opponent is the cat (which can move too). The mouse's goal is **to eat the maximum amount of cheese before being eaten by the cat**.\n",
    "\n",
    "As we can see in the diagram, **it's more probable to eat the cheese near us than the cheese close to the cat** (the closer we are to the cat, the more dangerous it is).\n",
    "\n",
    "Consequently, **the reward near the cat, even if it is bigger (more cheese), will be more discounted** since we are not really sure we will be able to eat it.\n",
    "\n",
    "To discount the rewards, we proceed like this:\n",
    "\n",
    "1. We define a discount rate called $\\gamma$. **It must be between 0 and 1**. Most of the time between **0.99 and 0.95**.\n",
    "    * The larger the $\\gamma$, the smaller the discount. This means our agent cares more about the long-term reward.\n",
    "    * The smaller the gamme, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).\n",
    "    \n",
    "2. Then, each reward will be discounted by $\\gamma$ to the exponent of the time step. As the time step increases, the cat gets closer to us, so the future reward is less and less likely to happen.\n",
    "\n",
    "Our discounted expected cumulative reward is:\n",
    "\n",
    "<img src=\"images/rewards_4.png\" title=\"\" alt=\"\" width=\"400\" data-align=\"center\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
