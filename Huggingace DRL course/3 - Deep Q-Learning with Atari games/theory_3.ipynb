{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b824a8b-6ba8-4042-a556-4b6eb5893048",
   "metadata": {},
   "source": [
    "# 3 - Deep Q-Learning with Atari games\n",
    "\n",
    "In the last unit, we learned our first reinforcement learning algorithm: Q-Learning. We implemented it from scratch using Numpy and trained it in two environments, FrozenLake-v1 and Taxi-v3.\n",
    "\n",
    "We got excellent results with this simple algorithm, but these environments were relatively simple given **the state space was discrete and small** (14 different states for FrozenLake-v1 and 500 for Taxi-v3). For comparison, the state space in Atari games can contain $10^{9}$ to $10^{11}$ states.\n",
    "\n",
    "But as we'll see, producing and updating a **Q-table can become ineffective in large state space environments**. Instead of using a Q-table, **deep Q-Learning uses a neural network that takes a state and approximates Q-values for each action based on that state**.\n",
    "\n",
    "We'll train our agent to play space invaders and other Atari environments using RL-Zoo, a training framework for RL using Stable-Baselines that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results, and recording videos.\n",
    "\n",
    "<img src=\"images/atari-envs.gif\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea5bcf-4d1e-4c49-9d08-a71ba0002d71",
   "metadata": {},
   "source": [
    "## 3.1 - From Q-Learning to Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32496b-087a-4147-b1a1-ebfe48d964a1",
   "metadata": {},
   "source": [
    "We learned that Q-Learning is an algorithm we use to train our Q-Function, an action-value function that determines the value of being at a particular state and taking a specific action at that state. The Q comes from “the Quality” of that action at that state.\n",
    "\n",
    "Internally, our Q-function has a Q-table, a table where each cell corresponds to a state-action pair value. Think of this Q-table as the memory or cheat sheet of our Q-function.\n",
    "\n",
    "The problem is that Q-Learning is a *tabular method*. This raises a problem in which the states and actions spaces are **small enough to approximate value functions to be represented as arrays and tables**. Also, this is **not scalable.** Q-Learning worked well with small state space environments of 16 and 500 states. But in order to train an agent to play Space invadores or other Atari games, we are going to use the frames as input.\n",
    "\n",
    "A single frame in Atari is composed of an image of 210x160 pixels. Given the images are in color (RGB), there are 3 channels. As a result, Atari environments have an observation space with a shape of (210, 160, 3), where each pixel contains a value ranging from 0 to 255. That gives us a gigantic state space: $256^{210 \\times 160 \\times 3} = 256^{100800}$.\n",
    "\n",
    "Therefore, creating and updating a Q-table for this environment would not be efficient. In this case, the best idea is to approximate the Q-values using a parametrized Q-function $Q_{\\theta}(s,a)$.\n",
    "\n",
    "This neural network will approximate, given a state, the different Q-values for each possible action at that state. And that's exactly what Deep Q-Learning does.\n",
    "\n",
    "<img src=\"images/deep.jpg\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5e8a2-cc70-440a-a50d-f2b0d2652baf",
   "metadata": {},
   "source": [
    "## 3.2 - The Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13c25f-1100-42e5-9a5f-79e5512649a0",
   "metadata": {},
   "source": [
    "As input, we take a **stack of 4 frames** passed through the ntwork as a state and output a vector of Q-values for each possible action at that state. Then, like with Q-Learning, we just need to use our epsilon-greedy policy to select which action to take.\n",
    "\n",
    "Identically to the Q-table, **when the neural neural network is initialized, the Q-value estimation is terrible**. But during training, our Deep Q-Network agent will **associate a situation with appropriate action** and learn to play the game well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e342b-7319-422f-a902-5ca87b4f92d6",
   "metadata": {},
   "source": [
    "### 3.2.1 - Preprocessing the input and temporal limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f4bdf-b663-4623-834b-5c3e60ea1b72",
   "metadata": {},
   "source": [
    "We need to preprocess the input. It's an essential step since we want to reduce the complexity of our state to reduce the computation time needed for training.\n",
    "\n",
    "To achieve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
