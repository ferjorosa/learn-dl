{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "In this notebook we'll walk through how [LangChain](https://python.langchain.com/en/latest/index.html#) thinks about memory. Memory involves keeping a concept of state around throughout a user’s interactions with an language model. A user’s interactions with a language model are captured in the concept of ChatMessages, so this boils down to ingesting, capturing, transforming and extracting knowledge from a sequence of chat messages. There are many different ways to do this, each of which exists as its own memory type.\n",
    "\n",
    "List of main relevant tutorials/blog posts\n",
    "\n",
    "* [Getting started with chains (Langchain)](https://python.langchain.com/en/latest/modules/chains/getting_started.html)\n",
    "* [LangChain for LLM Application Development (DeepLearning.AI)](https://learn.deeplearning.ai/langchain/)\n",
    "* [Briggs and Ingham (2023)](https://www.pinecone.io/learn/langchain/). LangChain AI Handbook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Chains and Agents are stateless, meaning that they treat each incoming query independently (as are the underlying LLMs and chat models). However, in some applications (chatbots being a GREAT example) it is highly important to remember previous interactions, both at a short term but also at a long term level. The **Memory** does exactly that.\n",
    "\n",
    "LangChain provides memory components in two forms. First, LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular and useful regardless of how they are used. Secondly, LangChain provides easy ways to incorporate these utilities into chains.\n",
    "\n",
    "Four different kinds of memory:\n",
    "\n",
    "* Conversation buffer memory\n",
    "* Conversation entity memory\n",
    "* Conversation knowledge graph memory \n",
    "* VectorStore-backed memory\n",
    "\n",
    "We can use multiple memories at one time. For example, we can use conversation memory + entity memory to recall individuals.\n",
    "\n",
    "We can also use other types of memory no directly implemented in langchain. For instance, we can store the conversation in a conventional database (such as a key-value store or SQL)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Conversation buffer memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation buffer memory is the simplest form of memory. It just involves keeping a buffer of all prior messages. There are different types:\n",
    "\n",
    "* `ConversationBufferMemory`. Allows for storing of messages and then extracts the messages in a variable.\n",
    "* `ConversationBufferWindowMemory`. Keeps a list of the interactions of the conversation over time. It only uses the last $K$ interactions.\n",
    "* `ConversationTokenBufferMemory`. Keeps a buffer of recent interactions in memory, and uses token lenght rather than the number of interactions to determine when to flush iteractions.\n",
    "* `ConversationSummaryMemory`. Creates a summary of the conversation over time and appends it to the prompts with the `System` tag. This approach keeps the last interaction in memory an summarize the previous ones. So, when a new interaction comes in, the previous one is combined with the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://gpt3tests.openai.azure.com/\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "engine = \"Davinci003\"\n",
    "max_tokens = 1000\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=engine)\n",
    "llm.openai_api_key = openai.api_key\n",
    "llm.openai_api_base = openai.api_base \n",
    "llm.max_tokens = max_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - `ConversationBufferMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi Andrew! It's nice to meet you. My name is AI. What brings you here today?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI:  Hi Andrew! It's nice to meet you. My name is AI. What brings you here today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 1+1 is equal to 2.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI:  Hi Andrew! It's nice to meet you. My name is AI. What brings you here today?\n",
      "Human: What is 1+1?\n",
      "AI:  1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Your name is Andrew. It's nice to meet you.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access memory buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Andrew\n",
      "AI:  Hi Andrew! It's nice to meet you. My name is AI. What brings you here today?\n",
      "Human: What is 1+1?\n",
      "AI:  1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:  Your name is Andrew. It's nice to meet you.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Andrew\\nAI:  Hi Andrew! It's nice to meet you. My name is AI. What brings you here today?\\nHuman: What is 1+1?\\nAI:  1+1 is equal to 2.\\nHuman: What is my name?\\nAI:  Your name is Andrew. It's nice to meet you.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually append to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"What's up?\"}, \n",
    "                    {\"output\": \"Not much, just hanging\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Andrew\n",
      "AI:  Hi Andrew! It's nice to meet you. My name is AI. What brings you here today?\n",
      "Human: What is 1+1?\n",
      "AI:  1+1 is equal to 2.\n",
      "Human: What is my name?\n",
      "AI:  Your name is Andrew. It's nice to meet you.\n",
      "Human: What's up?\n",
      "AI: Not much, just hanging\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - `ConversationBufferWindowMemory`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConversationBufferWindowMemory` keeps a list of the interactions of the conversation over time. It only uses the last $K$ interactions. Ths can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi Andrew, my name is AI. It's nice to meet you. What can I do for you today?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI:  Hi Andrew, my name is AI. It's nice to meet you. What can I do for you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 1+1 is equal to 2. Is there anything else I can help you with?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI:  Hi Andrew, my name is AI. It's nice to meet you. What can I do for you today?\n",
      "Human: What is 1+1?\n",
      "AI:  1+1 is equal to 2. Is there anything else I can help you with?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, I don't know your name.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hi, my name is Andrew', additional_kwargs={}, example=False), AIMessage(content=\" Hi Andrew, my name is AI. It's nice to meet you. What can I do for you today?\", additional_kwargs={}, example=False), HumanMessage(content='What is 1+1?', additional_kwargs={}, example=False), AIMessage(content=' 1+1 is equal to 2. Is there anything else I can help you with?', additional_kwargs={}, example=False), HumanMessage(content='What is my name?', additional_kwargs={}, example=False), AIMessage(content=\" I'm sorry, I don't know your name.\", additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the buffer is the same, but the prompt is artificially trimmed when sent to the model to emulate the window approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - `ConversationTokenBufferMemory`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConversationTokenBufferMemory` mantiene un búfer de interacciones recientes en la memoria y utiliza la longitud de tokens en lugar del número de interacciones para determinar cuándo descartar interacciones.\n",
    "\n",
    "Al mirar el código, puede parecer contraintuitivo que necesitemos pasar el objeto LLM como argumento. La razón detrás de esto es que diferentes LLMs cuentan los tokens de manera diferente.\n",
    "\n",
    "**Nota:** Debes instalar el paquete `tiktoken` para contar los tokens de GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=20)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the \"memory window\" changes when we change the `max_token_limit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - `ConversationSummaryBufferMemory`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of limiting the number of exchanges or tokens, let's write a summary of the conversation so far and let that be the memory. This approach keeps the last interaction in memory an summarize the previous ones. So, when a new interaction comes in, the previous one is combined with the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/ainn-knowlege-sharing/Langchain/tutorials/3_memory.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/ainn-knowlege-sharing/Langchain/tutorials/3_memory.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmemory\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversationSummaryBufferMemory\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/ainn-knowlege-sharing/Langchain/tutorials/3_memory.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m agenda \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHay una reunión a las 8 a. m. con tu equipo de productos. \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/ainn-knowlege-sharing/Langchain/tutorials/3_memory.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mNecesitarás tener preparada tu presentación de PowerPoint. \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/ainn-knowlege-sharing/Langchain/tutorials/3_memory.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mDe 9 a. m. a 12 p. m., tendrás tiempo para trabajar en tu proyecto LangChain, \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/ainn-knowlege-sharing/Langchain/tutorials/3_memory.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdesde más de una hora de distancia para reunirse contigo y comprender lo último en IA. \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/ainn-knowlege-sharing/Langchain/tutorials/3_memory.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mAsegúrate de llevar tu computadora portátil para mostrar la última demostración de LLM.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/ainn-knowlege-sharing/Langchain/tutorials/3_memory.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m memory \u001b[39m=\u001b[39m ConversationSummaryBufferMemory(llm\u001b[39m=\u001b[39mllm, max_token_limit\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/ainn-knowlege-sharing/Langchain/tutorials/3_memory.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m memory\u001b[39m.\u001b[39msave_context({\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mHola\u001b[39m\u001b[39m\"\u001b[39m}, {\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m¿Qué tal?\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/ainn-knowlege-sharing/Langchain/tutorials/3_memory.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m memory\u001b[39m.\u001b[39msave_context({\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mNo mucho, solo pasando el rato\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/ainn-knowlege-sharing/Langchain/tutorials/3_memory.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m                      {\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mGenial\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: \\nThe human greets the AI and asks what is on the schedule today. The AI responds casually with \"Cool\".\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A good demo to show would be one that highlights the latest features of the LLM and how they can be used in a real-world setting. You could also demonstrate how the AI capabilities can be integrated with other systems.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: \\nThe human greets the AI and asks what is on the schedule today. The AI responds casually with \"Cool\" and goes on to explain that there is a meeting at 8am with the product team to prepare a powerpoint presentation, then from 9am-12pm there is time to work on the LangChain project, and then at noon lunch with a customer who is driving from over an hour away to understand the latest in AI, and to bring the laptop to show the latest LLM demo.\\nHuman: What would be a good demo to show?\\nAI:  A good demo to show would be one that highlights the latest features of the LLM and how they can be used in a real-world setting. You could also demonstrate how the AI capabilities can be integrated with other systems.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I recommend that you create between 8 and 10 slides for your demo. It is important to keep your presentation concise, but still have enough details to make the audience understand the features of the LLM and how it can be used in a real-world setting. Depending on the length of the demo, you may need to adjust the number of slides accordingly.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Do I have time to meet Harry at 10am? (according to my schedule)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: \\nThe human greets the AI and asks what is on the schedule today. The AI responds casually with \"Cool\" and goes on to explain that there is a meeting at 8am with the product team to prepare a powerpoint presentation, then from 9am-12pm there is time to work on the LangChain project, and then at noon lunch with a customer who is driving from over an hour away to understand the latest in AI. The AI suggests that a good demo to show would be one that highlights the latest features of the LLM and how they can be used in a real-world setting, and possibly demonstrate how the AI capabilities can be integrated with other systems. When asked if there was time to meet Harry at 10am, the AI confirmed that there was one hour from 9am to 10am to work on the LangChain project, so the human would be able to meet Harry at 10am.\\nHuman: How many slides do you recommend for my demo?\\nAI:  I recommend that you create between 8 and 10 slides for your demo. It is important to keep your presentation concise, but still have enough details to make the audience understand the features of the LLM and how it can be used in a real-world setting. Depending on the length of the demo, you may need to adjust the number of slides accordingly.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"How many slides do you recommend for my demo?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Entity memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This memory module extract information on entities (using LLMs) and builds up its knowledge about that entity over time (also using LLMs).\n",
    "\n",
    "**Note:** It would be interesting to understand how this works with well-known entities like \"Mount Everest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationEntityMemory\n",
    "\n",
    "memory = ConversationEntityMemory(llm=llm)\n",
    "_input = {\"input\": \"Deven & Sam are working on a hackathon project\"}\n",
    "memory.load_memory_variables(_input)\n",
    "memory.save_context(\n",
    "    _input,\n",
    "    {\"output\": \" That sounds like a great project! What kind of project are they working on?\"}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask the model to search for information about entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Deven & Sam are working on a hackathon project\\nAI:  That sounds like a great project! What kind of project are they working on?',\n",
       " 'entities': {'Sam': 'Sam is working on a hackathon project with Deven.'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": 'who is Sam'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Deven & Sam are working on a hackathon project\\nAI:  That sounds like a great project! What kind of project are they working on?',\n",
       " 'entities': {'Deven': 'Deven is working on a hackathon project with Sam.',\n",
       "  'Sam': 'Sam is working on a hackathon project with Deven.'}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": 'who is Deven'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Deven': 'Deven is working on a hackathon project with Sam.',\n",
       " 'Sam': 'Sam is working on a hackathon project with Deven.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.entity_store.store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Automatic usage in a Conversation\n",
    "\n",
    "In the following example, we have 4 entities:\n",
    "* Sam (unknown)\n",
    "* Deven (unknown)\n",
    "* Langchain (unknown in 2021, i.e., last model's data)\n",
    "* Everest (known)\n",
    "\n",
    "When generating the summary of the entities we can see how the model incorporates extra info to \"known\" entities given the given context, we can also see how it modifies all info about entities accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=ConversationEntityMemory(llm=llm)\n",
    ")\n",
    "\n",
    "conversation.memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Deven': '', 'Sam': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: Deven & Sam are working on a hackathon project\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' What kind of project are they working on?'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Deven & Sam are working on a hackathon project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Deven': 'Deven is working on a hackathon project with Sam.',\n",
       " 'Sam': 'Sam is working on a hackathon project with Deven.'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Deven': 'Deven is working on a hackathon project with Sam.', 'Sam': 'Sam is working on a hackathon project with Deven.', 'Langchain': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: Deven & Sam are working on a hackathon project\n",
      "AI:  What kind of project are they working on?\n",
      "Last line:\n",
      "Human: They are trying to add more complex memory structures to Langchain\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Interesting! What kind of memory structures are they trying to add?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"They are trying to add more complex memory structures to Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Deven': 'Deven is working on a hackathon project with Sam to add more complex memory structures to Langchain.',\n",
       " 'Sam': 'Sam is working on a hackathon project with Deven, attempting to add more complex memory structures to Langchain.',\n",
       " 'Langchain': 'Langchain is a project that is trying to add more complex memory structures.'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Deven': 'Deven is working on a hackathon project with Sam to add more complex memory structures to Langchain.', 'Sam': 'Sam is working on a hackathon project with Deven, attempting to add more complex memory structures to Langchain.', 'Everest': ''}\n",
      "\n",
      "Current conversation:\n",
      "Human: Deven & Sam are working on a hackathon project\n",
      "AI:  What kind of project are they working on?\n",
      "Human: They are trying to add more complex memory structures to Langchain\n",
      "AI:  Interesting! What kind of memory structures are they trying to add?\n",
      "Last line:\n",
      "Human: Deven & Sam are going to climb the Everest\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' That sounds like an ambitious goal! What challenges do you think they might face while attempting to climb the Everest?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Deven & Sam are going to climb the Everest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Deven': 'Deven is working on a hackathon project with Sam to add more complex memory structures to Langchain, and is attempting to climb the Everest.',\n",
       " 'Sam': 'Sam is working on a hackathon project with Deven, attempting to add more complex memory structures to Langchain, and is also planning to climb the Everest.',\n",
       " 'Langchain': 'Langchain is a project that is trying to add more complex memory structures.',\n",
       " 'Everest': 'Everest is a mountain that Deven and Sam are attempting to climb.'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Knowledge graph memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a knowledge graph to recreate memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"On Sam: Sam is one of Human's high-school friends. Sam favourite color blue.\"}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "# Recreate a fake conversation\n",
    "memory = ConversationKGMemory(llm=llm)\n",
    "memory.save_context({\"input\": \"say hi to sam\"}, {\"output\": \"who is sam\"})\n",
    "memory.save_context({\"input\": \"sam is one of my high-school friends\"}, {\"output\": \"hi Sam!\"})\n",
    "memory.save_context({\"input\": \"Sam's favourite color is blue\"}, {\"output\": \"it is a nice color\"})\n",
    "memory.save_context({\"input\": \"thanks :)\"}, {\"output\": \"you are welcome\"})\n",
    "\n",
    "memory.load_memory_variables({\"input\": 'who is sam'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** I have noticed that in order to properly show the system message on the following example, you have to execute it twice (don't know why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Sam: Sam is one of my high-school friends. Sam has favorite color. Sam favorite color is blue.', additional_kwargs={})]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate a fake conversation and return messages instead of model \"thoughts\"\n",
    "memory = ConversationKGMemory(llm=llm, return_messages=True)\n",
    "memory.save_context({\"input\": \"say hi to sam\"}, {\"output\": \"who is sam\"})\n",
    "memory.save_context({\"input\": \"sam is one of my high-school friends\"}, {\"output\": \"hi Sam!\"})\n",
    "memory.save_context({\"input\": \"Sam's favourite color is blue\"}, {\"output\": \"it is a nice color\"})\n",
    "memory.save_context({\"input\": \"thanks :)\"}, {\"output\": \"you are welcome\"})\n",
    "\n",
    "memory.load_memory_variables({\"input\": 'who is sam'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Automatic usage in a conversation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Interestingly, I tested this same example with the default prompt and the model hallucinated when asked about Will (it said that Will studied in Standford for 5 years and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=template\n",
    ")\n",
    "conversation_with_kg = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    prompt=prompt,\n",
    "    memory=ConversationKGMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! I'm doing well, just learning more about the world and the different data I can access. How about you?\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: My name is James and I'm helping Will. He's an engineer.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi James. It's nice to meet you. I'm an AI. It sounds like Will is a very interesting person. What type of engineering does he specialize in?\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(input=\"My name is James and I'm helping Will. He's an engineer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "On Will: Will is engineer.\n",
      "\n",
      "Conversation:\n",
      "Human: What do you know about Will?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Will is an engineer. I don't know any more information about him.\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(input=\"What do you know about Will?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - VectorStore-backed memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VectorStoreRetrieverMemory` stores memories in a VectorDB and queries the top-K most \"salient\" docs every time it is called. This differs from most of the other Memory classes in that it doesn’t explicitly track the order of interactions.\n",
    "\n",
    "In this case, the “docs” are previous conversation snippets. This can be useful to refer to relevant pieces of information that the AI was told earlier in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"TextEmbeddingAda002\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    openai_api_key=config.get(\"OPENAI_API_KEY\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "loader = TextLoader(\"./state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Split the documents into batches\n",
    "document_batches = [docs[i:i + batch_size] for i in range(0, len(docs), batch_size)]\n",
    "\n",
    "# Initialize the FAISS db with the first batch of documents\n",
    "initial_batch = document_batches.pop(0)\n",
    "db = FAISS.from_documents(initial_batch, embeddings)\n",
    "\n",
    "# Add the remaining batches\n",
    "for batch in document_batches:\n",
    "    db.add_documents(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs_back = db.similarity_search(query)\n",
    "docs_back"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
