{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The purpose of this notebook is to devise a strategy for identifying key entities and their attributes in finance PDF files such as Earning calls transcripts. The process should be reasonably fast (~a couple of seconds).\n",
    "\n",
    "Elements to be identified are:\n",
    "- Entities such as people and companies\n",
    "- Relevant attributes for these, such as roles or relationships\n",
    "\n",
    "In addition to this, **I have prepared a second task to identify \"business developments\" and classify them as positive or negative** with a score between -10 (very negative) and 10 (very positive). The reasoning behind this extra task is to show the flexibility of the selected approach and also have another task to outline that advantages and disadvantages of the approach.\n",
    "\n",
    "-----\n",
    "\n",
    "To achieve these objectives, we have chosen to leverage [Instruct GPT](https://openai.com/research/instruction-following) in conjunction with the [Langchain library](https://python.langchain.com/docs/get_started/introduction). The decision to use Instruct GPT, rather than a smaller Transformer model like [REBEL](https://huggingface.co/Babelscape/rebel-large), is based on several compelling reasons (which have been outlined by Instruct GPT itself):\n",
    "\n",
    "**Advantages of Instruct GPT:**\n",
    "\n",
    "1. **Contextual Understanding:** Instruct GPT, being a larger and more advanced language model, possesses a deep understanding of context and nuances. This positions it well to comprehend and generate text that accurately captures the desired information.\n",
    "\n",
    "2. **Higher Performance:** Instruct GPT consistently delivers superior results in various natural language understanding tasks due to its larger size and extensive training.\n",
    "\n",
    "3. **Reduced Ambiguity:** Instruct GPT excels at handling ambiguous queries or instructions by considering context and providing comprehensive responses.\n",
    "\n",
    "*Extra (outlined by me, text formatted by Chat-GPT)*\n",
    "\n",
    "4. **Speed of Proof-of-Concept Creation:** Leveraging Instruct GPT's extensive knowledge, we can achieve promising results **without the need for extensive training data**. We can reliably apply zero-shot or few-shot learning techniques for rapid development.\n",
    "\n",
    "5. **Task Flexibility:** Instruct GPT allows us to adapt our entity and relationship detection by making slight adjustments to the prompts. This means we can maintain a consistent pipeline while obtaining diverse results by varying input instructions.\n",
    "\n",
    "**Considerations for Smaller Transformer Models:**\n",
    "\n",
    "1. **Resource Constraints:** Smaller models demand fewer computational resources and memory, making them suitable for deployment in resource-constrained environments, especially if we plan to deploy our private model. `[Comment: This would only apply if we wanted to deploy the model privately]`\n",
    "\n",
    "2. **Inference Speed:** Smaller models generally offer faster inference times, which can be crucial for real-time or high-throughput applications.\n",
    "\n",
    "3. **Specificity:** If our information extraction task is highly specialized and domain-specific, a smaller model may suffice and prove to be a more cost-effective solution.\n",
    "\n",
    "*Extra (outlined by me, text formatted by Chat-GPT)*\n",
    "\n",
    "4.  **Data availability**. Related to point 3, but, if we have a lot of specific data, it would probably be interesting to try smaller models, because fine-tuning large LLMs can be very expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import PyPDF2\n",
    "import re\n",
    "import time\n",
    "import tiktoken\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://gpt3tests.openai.azure.com/\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Deployment name in my Azure OpenAI Studio is \"Davinci003\", model is \"text-davinci-003\"\n",
    "engine = \"Davinci003\" # uses p50k_base tokenizer\n",
    "model = \"text-davinci-003\"\n",
    "openai_api_version = \"2022-12-01\" # not sure about the openAI API version, did some tests\n",
    "\n",
    "# Deployment name in my Azure OpenAI Studio is \"GPT35Turbo\", model is \"gpt-35-turbo\"\n",
    "# engine = \"GPT35Turbo\" # uses cl100k_base tokenizer, but there seems to be issues: https://github.com/openai/openai-python/issues/304\n",
    "# model = \"gpt-35-turbo\"\n",
    "# openai_api_version = \"2023-05-15\"\n",
    "\n",
    "MAX_GENERATION_LENGTH = 1000\n",
    "MODEL_CONTEXT_LENGTH = 4097\n",
    "EXTRA_TOKENS_FOR_TOKENIZATION_VARIABILITY = 50 # Not sure why, but I have run the same process multiple times and I have seen different tokenizations, need to double check this\n",
    "TEMPERATURE = 0 # low temperature to avoid GPT's \"imagination\"\n",
    "\n",
    "llm = AzureOpenAI(deployment_name=engine, openai_api_version=openai_api_version, temperature=TEMPERATURE)\n",
    "llm.openai_api_key = openai.api_key\n",
    "llm.openai_api_base = openai.api_base \n",
    "llm.max_tokens = MAX_GENERATION_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Prepare data\n",
    "\n",
    "Our starting point is a PDF file containing text. To effectively use this text with our Language Model (LLM), we must adjust it to fit within the model's context length. Here's how we'll do it:\n",
    "\n",
    "1. **Load the PDF:** We'll begin by loading the content from the PDF file.\n",
    "2. **Divide into Paragraphs:** Next, we'll break down the text into paragraphs. This step helps us manage the text in smaller, more digestible chunks.\n",
    "3. **Count Tokens per Paragraph:** Finally, we'll calculate the number of tokens in each paragraph. This information is crucial for generating prompts that fit within the model's capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_name = \"East West Bancorp\"\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_file_path = Path(f'./data/{pdf_file_name}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Dividing Text into Paragraphs\n",
    "\n",
    "To make the most of our Language Model (LLM), we need to consider its context length limit, which is set at 4097 tokens in this case. Our goal is to extract and provide the LLM with as many complete paragraphs as possible while staying within this token limit. To this purpose we are first going to divide the text into the best possible paragraphs.\n",
    "\n",
    "#### Process Overview:\n",
    "\n",
    "1. **Opening the PDF File and Preparing the List for Paragraphs:**\n",
    "   - Start by opening the PDF file and initializing an empty list to store paragraphs.\n",
    "2. **Iterating Through PDF Pages:**\n",
    "   - We go through each page of the PDF document.\n",
    "3. **Extracting Text from Pages:**\n",
    "   - Extract the text content from each page.\n",
    "4. **Using a Regular Expression (`r'\\n(?=[A-Z])'`):**\n",
    "   - We employ a regular expression `r'\\n(?=[A-Z])'` to split the text. This splits the text at line breaks (`\\n`) that are followed by an uppercase letter, indicating the beginning of a new paragraph.\n",
    "5. **Filtering and Collecting Valid Paragraphs:**\n",
    "   - We remove empty paragraphs and add the valid ones to our list.\n",
    "6. **Cleaning Paragraphs:**\n",
    "   - To enhance readability, we remove any unnecessary line breaks within paragraphs.\n",
    "7. **Adding a Line Break at Paragraph Start:**\n",
    "   - For clear separation when using these paragraphs in prompts, we insert a line break at the beginning of each paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs_from_pdf(pdf_path):\n",
    "    paragraphs = []\n",
    "\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "        # Iterate through each page of the PDF\n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            page_text = page.extract_text()\n",
    "\n",
    "            # Use regular expressions to split the text into paragraphs\n",
    "            page_paragraphs = re.split(r'\\n(?=[A-Z])', page_text)\n",
    "\n",
    "            # Remove empty paragraphs\n",
    "            page_paragraphs = [p.strip() for p in page_paragraphs if p.strip()]\n",
    "\n",
    "            # Add the page paragraphs to the overall list of paragraphs\n",
    "            paragraphs.extend(page_paragraphs)\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "# Divide the text into paragraphs\n",
    "paragraphs = extract_paragraphs_from_pdf(pdf_file_path)\n",
    "\n",
    "# Now 'paragraphs' is a list of strings, where each string is a paragraph from the PDF. Let's remove the \"ignored\" line breaks\n",
    "cleaned_paragraphs = [paragraph.replace(\"\\n\", \"\") for paragraph in paragraphs]\n",
    "\n",
    "# Let's add one line break at the beginning of each paragraph to make sure in the prompt they are separately considered\n",
    "cleaned_paragraphs = [\"\\n\" + paragraph for paragraph in paragraphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Count number of tokens per paragraph\n",
    "\n",
    "We need this information to divide the text into the appropriate blocks that will be later ingested by the LLM. To this purpose, we are going to use the `tiktoken` library.\n",
    "\n",
    "Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you \n",
    "1. Whether the string is too long for a text model to process. \n",
    "2. How much an OpenAI API call costs (as usage is priced by token).\n",
    "\n",
    "`tiktoken` supports three encodings used by OpenAI models:\n",
    "\n",
    "| Encoding name           | OpenAI models                                       |\n",
    "|-------------------------|-----------------------------------------------------|\n",
    "| `cl100k_base`           | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`  |\n",
    "| `p50k_base`             | Codex models, `text-davinci-002`, `text-davinci-003`|\n",
    "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |\n",
    "\n",
    "You can retrieve the encoding for a model using `tiktoken.encoding_for_model()` as follows:\n",
    "\n",
    "```python\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "encoding = tiktoken.encoding_for_model('text-davinci-003')\n",
    "```\n",
    "\n",
    "[Official documentation example for counting tokens](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the file: 9052\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Token Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nEast West Bancorp, Inc. ( NASDAQ: EWBC ) Q3 ...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nCompany Participants</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nAdrienne Atkinson - Director of Investor Rel...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nDominic Ng - Chairman and Chief Executive Of...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nChristopher Del Moral -Niles - Chief Financi...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>\\nDominic Ng</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>\\nWell, I just want to thank everyone for list...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>\\nOperator</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>\\nThe conference has now concluded. Thank you ...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>\\nView all</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Paragraph  Token Count\n",
       "0    \\nEast West Bancorp, Inc. ( NASDAQ: EWBC ) Q3 ...           35\n",
       "1                               \\nCompany Participants            3\n",
       "2    \\nAdrienne Atkinson - Director of Investor Rel...           10\n",
       "3    \\nDominic Ng - Chairman and Chief Executive Of...           10\n",
       "4    \\nChristopher Del Moral -Niles - Chief Financi...           11\n",
       "..                                                 ...          ...\n",
       "223                                       \\nDominic Ng            4\n",
       "224  \\nWell, I just want to thank everyone for list...           37\n",
       "225                                         \\nOperator            3\n",
       "226  \\nThe conference has now concluded. Thank you ...           31\n",
       "227                                         \\nView all            3\n",
       "\n",
       "[228 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_tokens(encoding, text):\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "# Count tokens for each paragraph and store the results in a list of dictionaries\n",
    "token_counts = []\n",
    "for paragraph in cleaned_paragraphs:\n",
    "    num_tokens = count_tokens(encoding, paragraph)\n",
    "    token_counts.append({'Paragraph': paragraph, 'Token Count': num_tokens})\n",
    "\n",
    "# Create a Pandas DataFrame from the list of token counts\n",
    "df = pd.DataFrame(token_counts)\n",
    "print(f\"Total number of tokens in the file: {df['Token Count'].sum()}\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Tasks\n",
    "\n",
    "We are going to use GPT 3 for several different tasks:\n",
    "* Identify entities and relationships\n",
    "* Identify business developments and classify them as positive or negative with a score betwen -10 (very negative) and 10 (very positive)\n",
    "\n",
    "For the \"entities & relationships\" task, we are going to use a one-shot learning approach, where we provide an example of what we want the model to do.\n",
    "For the business developments task, we are going to use a zero-shot learning approach. `[Comment: I tried a one-shot approach, but it confused the model (probably needed more work)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Identifying Entities and Relationships\n",
    "\n",
    "In this task, we combine named-entity recognition and relation extraction in a generative approach. To accomplish this, we will utilize [Instruct GPT](https://openai.com/research/instruction-following) with Langchain, employing a straightforward [LLMChain](https://docs.langchain.com/docs/components/chains/llm-chain). An LLMChain represents a common type of chain, composed of a PromptTemplate, a model (either an LLM or a ChatModel), and, optionally, an output parser.\n",
    "\n",
    "Our objective involves identifying the following elements:\n",
    "- Entities, such as individuals and companies.\n",
    "- Pertinent attributes associated with these entities, such as roles or relationships.\n",
    "\n",
    "We will primarily focus on three types of relationships that exist between entities:\n",
    "\n",
    "1. `<is_a>`: This relationship is useful for defining the nature of companies, places, or assets.\n",
    "2. `<works_at>`: This relationship indicates the place of employment for a person.\n",
    "3. `<has_position>`: This relationship specifies the role or position held by a person within the company they are associated with.\n",
    "\n",
    "**Please note:** While our approach can be expanded to encompass more intricate relationships, we have chosen to concentrate on these three examples for clarity and demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relationships_task = \"entity_relationships\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 - Prepare the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load base prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899\n"
     ]
    }
   ],
   "source": [
    "# Base template for entity relationship extraction tasks\n",
    "base_template_path = Path(f\"./prompts/templates/{entity_relationships_task}/base_template.txt\")\n",
    "\n",
    "# File showing the relationships we are interested in extracting\n",
    "relationships_template_path = Path(f\"./prompts/templates/{entity_relationships_task}/pcpa.txt\")\n",
    "\n",
    "# Example prompt for the specific relationships defined above\n",
    "example_prompt_path = Path(f\"./prompts/examples/{entity_relationships_task}/pcpa_microsoft.txt\")\n",
    "\n",
    "# Read the template content from the file\n",
    "with open(base_template_path, 'r') as prompt_file:\n",
    "    base_template = prompt_file.read()\n",
    "\n",
    "# Read the template content from the file\n",
    "with open(relationships_template_path, 'r') as prompt_file:\n",
    "    relationships_template = prompt_file.read()\n",
    "\n",
    "# Read the template content from the file\n",
    "with open(example_prompt_path, 'r') as prompt_file:\n",
    "    example_prompt = prompt_file.read()\n",
    "\n",
    "prompt_text = relationships_template.format(example=example_prompt)\n",
    "prompt_text = base_template.format(prompt=prompt_text)\n",
    "\n",
    "n_prompt_tokens = count_tokens(encoding, prompt_text)\n",
    "print(n_prompt_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2133\n",
      "1: 2147\n",
      "2: 2133\n",
      "3: 2141\n",
      "4: 498\n",
      "Total: 9052\n"
     ]
    }
   ],
   "source": [
    "def generate_chunks(dataframe, max_tokens_per_chunk):\n",
    "    # Initialize variables\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    # Iterate through the DataFrame rows\n",
    "    for index, row in dataframe.iterrows():\n",
    "        paragraph = row['Paragraph']\n",
    "        num_tokens = row['Token Count']\n",
    "\n",
    "        # If adding the current paragraph to the current chunk does not exceed the limit\n",
    "        if current_token_count + num_tokens <= max_tokens_per_chunk:\n",
    "            current_chunk.append(paragraph)\n",
    "            current_token_count += num_tokens\n",
    "        else:\n",
    "            # Add the current chunk to the list of chunks\n",
    "            if current_chunk:\n",
    "                chunks.append(\"\".join(current_chunk))\n",
    "            # Start a new chunk with the current paragraph\n",
    "            current_chunk = [paragraph]\n",
    "            current_token_count = num_tokens\n",
    "\n",
    "    # Add the last chunk, if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "chunk_size = MODEL_CONTEXT_LENGTH - MAX_GENERATION_LENGTH - EXTRA_TOKENS_FOR_TOKENIZATION_VARIABILITY - n_prompt_tokens\n",
    "\n",
    "chunks = generate_chunks(df, chunk_size)\n",
    "\n",
    "total_token_count = 0\n",
    "for i in range(0, len(chunks)):\n",
    "    token_count = count_tokens(encoding, chunks[i])\n",
    "    print(f\"{i}: {token_count}\")\n",
    "    total_token_count += token_count\n",
    "\n",
    "print(f\"Total: {total_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 - Run prompt through the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n"
     ]
    }
   ],
   "source": [
    "def llm_run(chain, query):\n",
    "    start_time = time.time()  # Record the start time\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    end_time = time.time()  # Record the end time\n",
    "    execution_time = end_time - start_time  # Calculate the execution time in seconds\n",
    "    print(f'Time taken: {round(execution_time, 2)} seconds')\n",
    "\n",
    "    return result\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input_text\"],\n",
    "    template=prompt_text + \"\\n{input_text}\",\n",
    ")\n",
    "\n",
    "# Create the simple LLMChain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the prompts through the LLM\n",
    "er_outputs = []\n",
    "for i in range(len(chunks)):\n",
    "    print(f\"Chunk {i}\")\n",
    "    output = llm_run(llm_chain, chunks[i])\n",
    "    er_outputs.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 - Format output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid format for:  the \n",
      "Invalid format for: beginning of the year, and we expect this trend to continue. We will continue to focus on \n",
      "Invalid format for: managing our balance sheet prudently, and we will continue to focus on our sustainability \n",
      "Invalid format for: initiatives.\n",
      "Invalid format for:  We're not selling assets to reduce gearing.\n",
      "Invalid format for: Yeah. It's a good question. I think it's too early to comment on that. We do have a number of \n",
      "Invalid format for: long-term tenants in the portfolio, and we're in the process of renewing those leases. So I think \n",
      "Invalid format for: it's too early to comment on that.\n"
     ]
    }
   ],
   "source": [
    "def split_relationship_text(text):\n",
    "    # Split the text based on \"<\" and \">\" to separate the relationship part\n",
    "    parts = text.split(\"<\")\n",
    "    \n",
    "    if len(parts) == 2:\n",
    "        # Extract entities and relationship\n",
    "        entity1 = parts[0].strip()\n",
    "        relationship, entity2  = parts[1].split(\">\")\n",
    "        entity2 = entity2.strip()\n",
    "        return entity1, relationship, entity2\n",
    "    else:\n",
    "        # Handle invalid input gracefully\n",
    "        print(f\"Invalid format for: {text}\")\n",
    "        return None, None, None\n",
    "\n",
    "er_output_dfs = []\n",
    "for output in er_outputs:\n",
    "    er_output_dfs.append(pd.DataFrame({\"Triplet\": output.split(\"\\n\")}))\n",
    "\n",
    "er_output_df = pd.concat(er_output_dfs)\n",
    "er_output_df = er_output_df[er_output_df[\"Triplet\"] != \"\"]\n",
    "er_output_df = er_output_df.drop_duplicates([\"Triplet\"]).reset_index(drop=True).copy()\n",
    "\n",
    "# Apply the function to the DataFrame and create new columns\n",
    "er_output_df[['Entity 1', 'Relationship', 'Entity 2']] = er_output_df['Triplet'].apply(lambda x: pd.Series(split_relationship_text(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 - Store output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timestamp for the current date and time\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define the output directory path using pathlib\n",
    "er_output_directory = Path('./outputs') / pdf_file_name #/ timestamp\n",
    "er_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the output file path for the Parquet file\n",
    "er_output_file_path = er_output_directory / f'{entity_relationships_task}.parquet'\n",
    "\n",
    "# Save the DataFrame to Parquet format\n",
    "er_output_df.to_parquet(er_output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Triplet</th>\n",
       "      <th>Entity 1</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Entity 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Timothy Collyer &lt;has_position&gt; Managing Director</td>\n",
       "      <td>Timothy Collyer</td>\n",
       "      <td>has_position</td>\n",
       "      <td>Managing Director</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael Green &lt;has_position&gt; Chief Investment ...</td>\n",
       "      <td>Michael Green</td>\n",
       "      <td>has_position</td>\n",
       "      <td>Chief Investment Officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dion Andrews &lt;has_position&gt; Chief Financial Of...</td>\n",
       "      <td>Dion Andrews</td>\n",
       "      <td>has_position</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Timothy Collyer &lt;works_at&gt; Growthpoint Propert...</td>\n",
       "      <td>Timothy Collyer</td>\n",
       "      <td>works_at</td>\n",
       "      <td>Growthpoint Properties Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michael Green &lt;works_at&gt; Growthpoint Propertie...</td>\n",
       "      <td>Michael Green</td>\n",
       "      <td>works_at</td>\n",
       "      <td>Growthpoint Properties Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Woolworths &lt;is_a&gt; Company</td>\n",
       "      <td>Woolworths</td>\n",
       "      <td>is_a</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Linfox &lt;is_a&gt; Company</td>\n",
       "      <td>Linfox</td>\n",
       "      <td>is_a</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Erskine Park &lt;is_a&gt; Place</td>\n",
       "      <td>Erskine Park</td>\n",
       "      <td>is_a</td>\n",
       "      <td>Place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Woolworths &lt;works_at&gt; Western Australia</td>\n",
       "      <td>Woolworths</td>\n",
       "      <td>works_at</td>\n",
       "      <td>Western Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Linfox &lt;works_at&gt; Erskine Park</td>\n",
       "      <td>Linfox</td>\n",
       "      <td>works_at</td>\n",
       "      <td>Erskine Park</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Triplet         Entity 1  \\\n",
       "0    Timothy Collyer <has_position> Managing Director  Timothy Collyer   \n",
       "1   Michael Green <has_position> Chief Investment ...    Michael Green   \n",
       "2   Dion Andrews <has_position> Chief Financial Of...     Dion Andrews   \n",
       "3   Timothy Collyer <works_at> Growthpoint Propert...  Timothy Collyer   \n",
       "4   Michael Green <works_at> Growthpoint Propertie...    Michael Green   \n",
       "..                                                ...              ...   \n",
       "77                          Woolworths <is_a> Company       Woolworths   \n",
       "78                              Linfox <is_a> Company           Linfox   \n",
       "79                          Erskine Park <is_a> Place     Erskine Park   \n",
       "80            Woolworths <works_at> Western Australia       Woolworths   \n",
       "81                     Linfox <works_at> Erskine Park           Linfox   \n",
       "\n",
       "    Relationship                          Entity 2  \n",
       "0   has_position                 Managing Director  \n",
       "1   has_position          Chief Investment Officer  \n",
       "2   has_position           Chief Financial Officer  \n",
       "3       works_at  Growthpoint Properties Australia  \n",
       "4       works_at  Growthpoint Properties Australia  \n",
       "..           ...                               ...  \n",
       "77          is_a                           Company  \n",
       "78          is_a                           Company  \n",
       "79          is_a                             Place  \n",
       "80      works_at                 Western Australia  \n",
       "81      works_at                      Erskine Park  \n",
       "\n",
       "[82 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er_output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some comments (`text-davinci-003`)\n",
    "\n",
    "As observed, the model encounters difficulties due to the absence of well-defined complex relationships between entities, and the lack of a proper ontology exacerbates this issue. Consequently, the model occasionally misassigns `<works_at>` relationships to companies and places when, ideally, it should employ `<is_located_at>` or a similar relation. This confusion arises from the model's limited flexibility, stemming from our failure to provide clear constraints.\n",
    "\n",
    "One straightforward enhancement is to establish constraints dictating that certain types of relationships should only exist between specific types of entities. For instance, we can flag `<works_at>` relationships that connect a pair of entities that are not `(Person, Company)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Extract business developments and classify their sentiment\n",
    "\n",
    "This task is a mix of summarization and classification. We first \"summarize\" the text by extracting key business developments and then classify those as positive or negative. To accomplish this, we will utilize [Instruct GPT](https://openai.com/research/instruction-following) with Langchain, employing a straightforward [LLMChain](https://docs.langchain.com/docs/components/chains/llm-chain). An LLMChain represents a common type of chain, composed of a PromptTemplate, a model (either an LLM or a ChatModel), and, optionally, an output parser.\n",
    "\n",
    "\n",
    "**Note:** We could also try a different approach where we divide the process in two parts and use [SimpleSequentialChain](https://python.langchain.com/docs/modules/chains/foundational/sequential_chains), where the output of one step is the input to the next. This would allow us to use different models for differents parts of the process. It would be the approach I would have chosen if I had to work with less potent LLMs (also cheaper).\n",
    "\n",
    "* The first chain would generate a list of developments\n",
    "* The second chain would take the list of developments as input and classify them as either positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_developments_task = \"business_developments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 - Prepare the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load base prompt\n",
    "\n",
    "This template is much simpler because we are not providing example (zero-shot learning), we just provide a generic formatting guideline, so we just load a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n"
     ]
    }
   ],
   "source": [
    "# Base template for entity relationship extraction tasks\n",
    "base_template_path = Path(f\"./prompts/templates/{business_developments_task}/base_template.txt\")\n",
    "\n",
    "# Read the template content from the file\n",
    "with open(base_template_path, 'r') as prompt_file:\n",
    "    base_template = prompt_file.read()\n",
    "\n",
    "prompt_text = base_template\n",
    "\n",
    "n_prompt_tokens = count_tokens(encoding, prompt_text)\n",
    "print(n_prompt_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare chunks of text\n",
    "\n",
    "We have previously defined the `generate_chunks()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2798\n",
      "1: 2817\n",
      "2: 2687\n",
      "3: 284\n",
      "Total: 8586\n"
     ]
    }
   ],
   "source": [
    "chunk_size = MODEL_CONTEXT_LENGTH - MAX_GENERATION_LENGTH - EXTRA_TOKENS_FOR_TOKENIZATION_VARIABILITY - n_prompt_tokens\n",
    "\n",
    "chunks = generate_chunks(df, chunk_size)\n",
    "\n",
    "total_token_count = 0\n",
    "for i in range(0, len(chunks)):\n",
    "    token_count = count_tokens(encoding, chunks[i])\n",
    "    print(f\"{i}: {token_count}\")\n",
    "    total_token_count += token_count\n",
    "\n",
    "print(f\"Total: {total_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 - Run the prompt through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 3628 tokens\n",
      "Time taken: 17.57 seconds\n",
      "Chunk 1\n",
      "Spent a total of 3412 tokens\n",
      "Time taken: 10.44 seconds\n",
      "Chunk 2\n",
      "Spent a total of 3225 tokens\n",
      "Time taken: 9.18 seconds\n",
      "Chunk 3\n",
      "Spent a total of 612 tokens\n",
      "Time taken: 2.61 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input_text\"],\n",
    "    template=prompt_text + \"\\n{input_text}\",\n",
    ")\n",
    "\n",
    "# Create the simple LLMChain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the prompts through the LLM\n",
    "bd_outputs = []\n",
    "for i in range(len(chunks)):\n",
    "    print(f\"Chunk {i}\")\n",
    "    output = llm_run(llm_chain, chunks[i])\n",
    "    bd_outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\nBusiness Development Summary | Score | Reason for Score\\nAcquisition of GSO Building in Dandenong | 8 | The acquisition of the GSO Building in Dandenong provides Growthpoint with a 9.4-year WALE and a modern, long-term Victorian Government tenanted building located in a growing major urban center. \\nDivestment of 333 Ann Street, Brisbane | 8 | The divestment of 333 Ann Street, Brisbane provides Growthpoint with a 3.7-year WALE and a strong return on the asset acquired back in 2012 for AUD 109 million. \\nLeasing Performance | 9 | Growthpoint achieved a strong leasing performance with over 156,000 square meters leased equivalent to 11.2% of income, resulting in occupancy of 93% across the portfolio. \\nIntegration of Fortius Funds Management Platform | 8 | The integration of the Fortius Funds Management platform was successfully completed in September 2022, providing a long-term growth platform for the group. \\nGrowth in Metro and Fringe CBD Office Market | 8 | According to JLL data, the CBD office markets have grown by 1.7 million square meters or 10.2% over the last 10 years, while non-CBD markets have grown by 1.5 million square meters or 17% over the same period. \\nTenants Accommodating for Quick Feasible Occupancy | 8 | Tenants are accommodating for quick feasible occupancy, which is still below pre-pandemic levels, but more employers are mandating minimum days in the office. \\nPositive Momentum in Industrial Property Sector | 9 | The positive momentum in the industrial markets continued in financial year 2023 due to a shortage of modern warehouse space across all markets, underpinned by growth in e-commerce and demand for supply chain infrastructure. \\nHigh Quality Assets Attractive to Tenants | 9 | Growthpoint's high quality, energy efficient buildings are attractive for government tenants, and the WALE of their government office tenants is 10.3 years. \\nLeasing Activity | 8 | Growthpoint executed 33 leases, accounting for approximately 32,000 square meters of space, representing 9.2% of the office portfolio income, and 124,000 square meters of industrial space, representing 15.4% of the industrial portfolio income. \\nDefensive Characteristics of Portfolio | 9 | Growthpoint's portfolio WALE reduced slightly over the year from 6.3 years to 6 years, but remains one of the longest office portfolio WALE in the REIT sector at 6.3 years. The portfolio was 93% occupied with 2% of additional leasing in advanced negotiation. \\nSustainability | 9 | Growthpoint's portfolio continues to achieve a high average NABERS Energy rating of 5.2 stars, and they increased their GRESB score to 81, remaining a sector leader in their space. They have significantly increased their onsite solar rollout with seven additional installations progressed over the year.\",\n",
       " \" \\nNewstead with two government tenants.\\n\\n1. FFO decreased 3.2% in FY 2023 | -3 | FFO decreased due to rapid increase in interest rates and subsequent impact on interest costs.\\n2. NPI increased due to one-off payments | 8 | NPI increased due to two one-off payments, including a surrender of lease payment and a bank guarantee payment.\\n3. Gearing increased to 37.2% | -3 | Gearing increased from 31.6% to 37.2%, remaining within the group's target range of 35% to 45%.\\n4. Weighted average cost of debt increased to 4.6% | -5 | Weighted average cost of debt increased from 3.4% to 4.6%, reflecting the rapid increase in interest rates during the year.\\n5. Distribution payout ratio in target range of 75-85% | 8 | Distribution payout ratio was in the middle of the target payout ratio range of between 75% and 85% of FFO.\\n6. Share buyback program completed in May 2023 | 8 | Share buyback program was completed in May 2023, having purchased 19.3 million securities for a total consideration of AUD 63.4 million.\\n7. Moody's reaffirmed BAA2 rating with stable outlook | 8 | Moody's reaffirmed their BAA2 rating with a stable outlook for the group's debt.\\n8. Sustainability linked loans established | 8 | Sustainability linked loans were established, converting AUD 520 million of the group's existing debt arrangements and establishing an overarching sustainability finance governance framework.\\n9. Office occupancy decreased from 92% to 90% | -3 | Office occupancy decreased from 92% to 90% due to tenant exit and vacation of 7,500 square meters in Fortitude Valley.\",\n",
       " \"\\nYeah. It's a good question. I think it's too early to comment on that. We do have a number of \\ntenants that have been with us for a long time. We have a number of tenants that have been \\nwith us for a shorter period of time. So it's really hard to comment on that. We do have a \\nnumber of tenants that have been with us for a long time, and we do have a number of \\ntenants that have been with us for a shorter period of time. So it's really hard to comment on \\nthat. We do have a number of tenants that have been with us for a long time, and we do have \\na number of tenants that have been with us for a shorter period of time. So it's really hard to \\ncomment on that.\\n\\nBusiness Development Summary: Skyring Terrace unexpectedly became vacant | 0 | Neutral. \\nBusiness Development Summary: 5 Murray Rose getting good attention | 5 | Positive. The property is getting attention from potential tenants. \\nBusiness Development Summary: 95% of income from non-SME tenants | 8 | Positive. This indicates a strong portfolio of tenants with a low risk of default. \\nBusiness Development Summary: AUD 560 million sale of Korean pension fund to AustralianSuper | 9 | Positive. This indicates strong investor confidence in the market. \\nBusiness Development Summary: Bid/ask spread on office transactions is wide | -3 | Negative. This indicates a lack of liquidity in the office sector.\",\n",
       " '\\n\\n1. Linfox occupies three of our logistics properties at Erskine Park | 10 | Erskine Park is considered to be the hottest logistics market in the country right now, indicating a strong demand for the property and a high likelihood of renewal and extension.\\n\\n2. Woolworths is a major tenant across our portfolio for the last 15 years | 8 | Woolworths has a long-term relationship with the portfolio, indicating a strong commitment to the property and a high likelihood of renewal and extension.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 - Format output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid format for:  \n",
      "Invalid format for: Newstead with two government tenants.\n",
      "Invalid format for: Yeah. It's a good question. I think it's too early to comment on that. We do have a number of \n",
      "Invalid format for: tenants that have been with us for a long time. We have a number of tenants that have been \n",
      "Invalid format for: with us for a shorter period of time. So it's really hard to comment on that. We do have a \n",
      "Invalid format for: number of tenants that have been with us for a long time, and we do have a number of \n",
      "Invalid format for: tenants that have been with us for a shorter period of time. So it's really hard to comment on \n",
      "Invalid format for: that. We do have a number of tenants that have been with us for a long time, and we do have \n",
      "Invalid format for: a number of tenants that have been with us for a shorter period of time. So it's really hard to \n",
      "Invalid format for: comment on that.\n"
     ]
    }
   ],
   "source": [
    "def split_business_development_text(text):\n",
    "    parts = text.split('|')\n",
    "    if len(parts) == 3:\n",
    "        business_development_summary = parts[0].strip()\n",
    "        # score = int(parts[1].strip())\n",
    "        score = parts[1].strip()\n",
    "        reason_for_score = parts[2].strip()\n",
    "        return business_development_summary, score, reason_for_score\n",
    "    else:\n",
    "        print(f\"Invalid format for: {text}\")\n",
    "        return None, None, None\n",
    "\n",
    "bd_output_dfs = []\n",
    "for output in bd_outputs:\n",
    "    bd_output_dfs.append(pd.DataFrame({\"RAW Output\": output.split(\"\\n\")}))\n",
    "\n",
    "bd_output_df = pd.concat(bd_output_dfs)\n",
    "bd_output_df = bd_output_df[bd_output_df[\"RAW Output\"] != \"\"]\n",
    "bd_output_df = bd_output_df.drop_duplicates([\"RAW Output\"]).reset_index(drop=True).copy()\n",
    "\n",
    "# Apply the function to the DataFrame and create new columns\n",
    "bd_output_df[['Business development', 'Score', 'Explanation']] = bd_output_df['RAW Output'].apply(lambda x: pd.Series(split_business_development_text(x)))\n",
    "bd_output_df = bd_output_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 - Store output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timestamp for the current date and time\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define the output directory path using pathlib\n",
    "bd_output_directory = Path('./outputs') / pdf_file_name #/ timestamp\n",
    "bd_output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the output file path for the Parquet file\n",
    "bd_output_file_path = bd_output_directory / f'{business_developments_task}.parquet'\n",
    "\n",
    "# Save the DataFrame to Parquet format\n",
    "bd_output_df.to_parquet(bd_output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RAW Output</th>\n",
       "      <th>Business development</th>\n",
       "      <th>Score</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Development Summary | Score | Reason ...</td>\n",
       "      <td>Business Development Summary</td>\n",
       "      <td>Score</td>\n",
       "      <td>Reason for Score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acquisition of GSO Building in Dandenong | 8 |...</td>\n",
       "      <td>Acquisition of GSO Building in Dandenong</td>\n",
       "      <td>8</td>\n",
       "      <td>The acquisition of the GSO Building in Dandeno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Divestment of 333 Ann Street, Brisbane | 8 | T...</td>\n",
       "      <td>Divestment of 333 Ann Street, Brisbane</td>\n",
       "      <td>8</td>\n",
       "      <td>The divestment of 333 Ann Street, Brisbane pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leasing Performance | 9 | Growthpoint achieved...</td>\n",
       "      <td>Leasing Performance</td>\n",
       "      <td>9</td>\n",
       "      <td>Growthpoint achieved a strong leasing performa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Integration of Fortius Funds Management Platfo...</td>\n",
       "      <td>Integration of Fortius Funds Management Platform</td>\n",
       "      <td>8</td>\n",
       "      <td>The integration of the Fortius Funds Managemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Growth in Metro and Fringe CBD Office Market |...</td>\n",
       "      <td>Growth in Metro and Fringe CBD Office Market</td>\n",
       "      <td>8</td>\n",
       "      <td>According to JLL data, the CBD office markets ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tenants Accommodating for Quick Feasible Occup...</td>\n",
       "      <td>Tenants Accommodating for Quick Feasible Occup...</td>\n",
       "      <td>8</td>\n",
       "      <td>Tenants are accommodating for quick feasible o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Positive Momentum in Industrial Property Secto...</td>\n",
       "      <td>Positive Momentum in Industrial Property Sector</td>\n",
       "      <td>9</td>\n",
       "      <td>The positive momentum in the industrial market...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>High Quality Assets Attractive to Tenants | 9 ...</td>\n",
       "      <td>High Quality Assets Attractive to Tenants</td>\n",
       "      <td>9</td>\n",
       "      <td>Growthpoint's high quality, energy efficient b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Leasing Activity | 8 | Growthpoint executed 33...</td>\n",
       "      <td>Leasing Activity</td>\n",
       "      <td>8</td>\n",
       "      <td>Growthpoint executed 33 leases, accounting for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Defensive Characteristics of Portfolio | 9 | G...</td>\n",
       "      <td>Defensive Characteristics of Portfolio</td>\n",
       "      <td>9</td>\n",
       "      <td>Growthpoint's portfolio WALE reduced slightly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sustainability | 9 | Growthpoint's portfolio c...</td>\n",
       "      <td>Sustainability</td>\n",
       "      <td>9</td>\n",
       "      <td>Growthpoint's portfolio continues to achieve a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1. FFO decreased 3.2% in FY 2023 | -3 | FFO de...</td>\n",
       "      <td>1. FFO decreased 3.2% in FY 2023</td>\n",
       "      <td>-3</td>\n",
       "      <td>FFO decreased due to rapid increase in interes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2. NPI increased due to one-off payments | 8 |...</td>\n",
       "      <td>2. NPI increased due to one-off payments</td>\n",
       "      <td>8</td>\n",
       "      <td>NPI increased due to two one-off payments, inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3. Gearing increased to 37.2% | -3 | Gearing i...</td>\n",
       "      <td>3. Gearing increased to 37.2%</td>\n",
       "      <td>-3</td>\n",
       "      <td>Gearing increased from 31.6% to 37.2%, remaini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4. Weighted average cost of debt increased to ...</td>\n",
       "      <td>4. Weighted average cost of debt increased to ...</td>\n",
       "      <td>-5</td>\n",
       "      <td>Weighted average cost of debt increased from 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5. Distribution payout ratio in target range o...</td>\n",
       "      <td>5. Distribution payout ratio in target range o...</td>\n",
       "      <td>8</td>\n",
       "      <td>Distribution payout ratio was in the middle of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6. Share buyback program completed in May 2023...</td>\n",
       "      <td>6. Share buyback program completed in May 2023</td>\n",
       "      <td>8</td>\n",
       "      <td>Share buyback program was completed in May 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7. Moody's reaffirmed BAA2 rating with stable ...</td>\n",
       "      <td>7. Moody's reaffirmed BAA2 rating with stable ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Moody's reaffirmed their BAA2 rating with a st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8. Sustainability linked loans established | 8...</td>\n",
       "      <td>8. Sustainability linked loans established</td>\n",
       "      <td>8</td>\n",
       "      <td>Sustainability linked loans were established, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9. Office occupancy decreased from 92% to 90% ...</td>\n",
       "      <td>9. Office occupancy decreased from 92% to 90%</td>\n",
       "      <td>-3</td>\n",
       "      <td>Office occupancy decreased from 92% to 90% due...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Business Development Summary: Skyring Terrace ...</td>\n",
       "      <td>Business Development Summary: Skyring Terrace ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Business Development Summary: 5 Murray Rose ge...</td>\n",
       "      <td>Business Development Summary: 5 Murray Rose ge...</td>\n",
       "      <td>5</td>\n",
       "      <td>Positive. The property is getting attention fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Business Development Summary: 95% of income fr...</td>\n",
       "      <td>Business Development Summary: 95% of income fr...</td>\n",
       "      <td>8</td>\n",
       "      <td>Positive. This indicates a strong portfolio of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Business Development Summary: AUD 560 million ...</td>\n",
       "      <td>Business Development Summary: AUD 560 million ...</td>\n",
       "      <td>9</td>\n",
       "      <td>Positive. This indicates strong investor confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Business Development Summary: Bid/ask spread o...</td>\n",
       "      <td>Business Development Summary: Bid/ask spread o...</td>\n",
       "      <td>-3</td>\n",
       "      <td>Negative. This indicates a lack of liquidity i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1. Linfox occupies three of our logistics prop...</td>\n",
       "      <td>1. Linfox occupies three of our logistics prop...</td>\n",
       "      <td>10</td>\n",
       "      <td>Erskine Park is considered to be the hottest l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2. Woolworths is a major tenant across our por...</td>\n",
       "      <td>2. Woolworths is a major tenant across our por...</td>\n",
       "      <td>8</td>\n",
       "      <td>Woolworths has a long-term relationship with t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           RAW Output  \\\n",
       "0   Business Development Summary | Score | Reason ...   \n",
       "1   Acquisition of GSO Building in Dandenong | 8 |...   \n",
       "2   Divestment of 333 Ann Street, Brisbane | 8 | T...   \n",
       "3   Leasing Performance | 9 | Growthpoint achieved...   \n",
       "4   Integration of Fortius Funds Management Platfo...   \n",
       "5   Growth in Metro and Fringe CBD Office Market |...   \n",
       "6   Tenants Accommodating for Quick Feasible Occup...   \n",
       "7   Positive Momentum in Industrial Property Secto...   \n",
       "8   High Quality Assets Attractive to Tenants | 9 ...   \n",
       "9   Leasing Activity | 8 | Growthpoint executed 33...   \n",
       "10  Defensive Characteristics of Portfolio | 9 | G...   \n",
       "11  Sustainability | 9 | Growthpoint's portfolio c...   \n",
       "14  1. FFO decreased 3.2% in FY 2023 | -3 | FFO de...   \n",
       "15  2. NPI increased due to one-off payments | 8 |...   \n",
       "16  3. Gearing increased to 37.2% | -3 | Gearing i...   \n",
       "17  4. Weighted average cost of debt increased to ...   \n",
       "18  5. Distribution payout ratio in target range o...   \n",
       "19  6. Share buyback program completed in May 2023...   \n",
       "20  7. Moody's reaffirmed BAA2 rating with stable ...   \n",
       "21  8. Sustainability linked loans established | 8...   \n",
       "22  9. Office occupancy decreased from 92% to 90% ...   \n",
       "31  Business Development Summary: Skyring Terrace ...   \n",
       "32  Business Development Summary: 5 Murray Rose ge...   \n",
       "33  Business Development Summary: 95% of income fr...   \n",
       "34  Business Development Summary: AUD 560 million ...   \n",
       "35  Business Development Summary: Bid/ask spread o...   \n",
       "36  1. Linfox occupies three of our logistics prop...   \n",
       "37  2. Woolworths is a major tenant across our por...   \n",
       "\n",
       "                                 Business development  Score  \\\n",
       "0                        Business Development Summary  Score   \n",
       "1            Acquisition of GSO Building in Dandenong      8   \n",
       "2              Divestment of 333 Ann Street, Brisbane      8   \n",
       "3                                 Leasing Performance      9   \n",
       "4    Integration of Fortius Funds Management Platform      8   \n",
       "5        Growth in Metro and Fringe CBD Office Market      8   \n",
       "6   Tenants Accommodating for Quick Feasible Occup...      8   \n",
       "7     Positive Momentum in Industrial Property Sector      9   \n",
       "8           High Quality Assets Attractive to Tenants      9   \n",
       "9                                    Leasing Activity      8   \n",
       "10             Defensive Characteristics of Portfolio      9   \n",
       "11                                     Sustainability      9   \n",
       "14                   1. FFO decreased 3.2% in FY 2023     -3   \n",
       "15           2. NPI increased due to one-off payments      8   \n",
       "16                      3. Gearing increased to 37.2%     -3   \n",
       "17  4. Weighted average cost of debt increased to ...     -5   \n",
       "18  5. Distribution payout ratio in target range o...      8   \n",
       "19     6. Share buyback program completed in May 2023      8   \n",
       "20  7. Moody's reaffirmed BAA2 rating with stable ...      8   \n",
       "21         8. Sustainability linked loans established      8   \n",
       "22      9. Office occupancy decreased from 92% to 90%     -3   \n",
       "31  Business Development Summary: Skyring Terrace ...      0   \n",
       "32  Business Development Summary: 5 Murray Rose ge...      5   \n",
       "33  Business Development Summary: 95% of income fr...      8   \n",
       "34  Business Development Summary: AUD 560 million ...      9   \n",
       "35  Business Development Summary: Bid/ask spread o...     -3   \n",
       "36  1. Linfox occupies three of our logistics prop...     10   \n",
       "37  2. Woolworths is a major tenant across our por...      8   \n",
       "\n",
       "                                          Explanation  \n",
       "0                                    Reason for Score  \n",
       "1   The acquisition of the GSO Building in Dandeno...  \n",
       "2   The divestment of 333 Ann Street, Brisbane pro...  \n",
       "3   Growthpoint achieved a strong leasing performa...  \n",
       "4   The integration of the Fortius Funds Managemen...  \n",
       "5   According to JLL data, the CBD office markets ...  \n",
       "6   Tenants are accommodating for quick feasible o...  \n",
       "7   The positive momentum in the industrial market...  \n",
       "8   Growthpoint's high quality, energy efficient b...  \n",
       "9   Growthpoint executed 33 leases, accounting for...  \n",
       "10  Growthpoint's portfolio WALE reduced slightly ...  \n",
       "11  Growthpoint's portfolio continues to achieve a...  \n",
       "14  FFO decreased due to rapid increase in interes...  \n",
       "15  NPI increased due to two one-off payments, inc...  \n",
       "16  Gearing increased from 31.6% to 37.2%, remaini...  \n",
       "17  Weighted average cost of debt increased from 3...  \n",
       "18  Distribution payout ratio was in the middle of...  \n",
       "19  Share buyback program was completed in May 202...  \n",
       "20  Moody's reaffirmed their BAA2 rating with a st...  \n",
       "21  Sustainability linked loans were established, ...  \n",
       "22  Office occupancy decreased from 92% to 90% due...  \n",
       "31                                           Neutral.  \n",
       "32  Positive. The property is getting attention fr...  \n",
       "33  Positive. This indicates a strong portfolio of...  \n",
       "34  Positive. This indicates strong investor confi...  \n",
       "35  Negative. This indicates a lack of liquidity i...  \n",
       "36  Erskine Park is considered to be the hottest l...  \n",
       "37  Woolworths has a long-term relationship with t...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'f\\x89\\x03\\xaa\\xf2i\\nw\\xa5\\x1d\\xb0\\xd5|\\x86R\\xc2\\x87( \\xa2\\xf3\\x97Px%\\xf3\\xc1\\xb9\\xf5\\xcf[\\xd9h\\xe4v\\xf4t\"\\x05\\xcc\\xe6\\xee\\x08O\\x9f=n\\xff\\x1d9\\x85\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c']\n",
      "Bad pipe message: %s [b'\\x05\\x03\\x06', b'\\x07\\x08']\n",
      "Bad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'']\n",
      "Bad pipe message: %s [b'\\x93\\x8e\\x05\\x13\\x88&\\xf3\\xf5)\\xac\\nl3\\xfd\\xbf!g\\x94 \\xd6\\x80w;\\xf12tF-\\xc9\\xf0z\\xe3B\\xbe\\xb2\\xa3\\x9f\\xc8\\xb7\\x87\\x1bL\\x1bq7Be\\xaa1\\xb2\\xe0\\x00\\x08\\x13\\x02\\x13\\x03\\x13']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xc0l\\xcdV\\x8c\\xfc\\x1c\\xc8\\xbcC\\x18D\\xads\\xf1tb\\xed\\x91\\x14R\\xb5']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b\"\\x93\\xb0Z\\xe3\\x10U3\\xc4V9\\xdfX\\xea/\\x10\\xa2\\xae\\xd0\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\", b'\\x07\\x08']\n",
      "Bad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x11M\\x7f)\\xd3a\\xef\\xd8C)\\\\\\xeb\\x8f']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b\"^f\\xc6\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\"]\n",
      "Bad pipe message: %s [b'`\\xf5\\x85\\xe2\\x91\\x02\\x84\\xa2\\xba0s\\x1e\\xd2*\\xc6[J9\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b']\n",
      "Bad pipe message: %s [b'\\xab\\x1f\\xe7\\x9c\\x16d\\x0f\\x05\\xe7L\\x0bYI\\xd8\\xb4\\xc0T\\xfa\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff']\n",
      "Bad pipe message: %s [b\"\\x92\\x14\\x91\\x8a\\x12\\xb9B\\x8c\\xad\\x10\\x83\\xb5\\x07\\tz\\x1c\\xdc\\x96\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\"]\n",
      "Bad pipe message: %s [b\"bY|\\xe5\\x90\\xb8\\xbb\\x18<\\x84>\\x9e\\x03\\x9b\\xc5\\x8f\\x88\\x0f\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\"]\n"
     ]
    }
   ],
   "source": [
    "bd_output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments on Model Behavior (`text-davinci-003`):\n",
    "\n",
    "**Variability in Result Formatting:**\n",
    "It's noticeable that, even with a temperature setting of 0, the model generates answers in various formats. We can observe a few recurring patterns:\n",
    "- Some Business development\tanswers begin with \"Business development Summary,\" while others start with numbered lists like \"1.,\" \"2.,\" etc.\n",
    "- The explanations for scores are sometimes presented directly, and other times, they begin with terms like \"positive,\" \"negative,\" or \"neutral.\"\n",
    "\n",
    "To mitigate this variability and achieve more consistent formatting in the answers, we have a couple of options:\n",
    "* To improve the prompts we use (manually or with prompt fine-tuning)\n",
    "* Fine-tune the model. \n",
    "\n",
    "Personally, I believe that fine-tuning the model using pre-tested gold-standard examples is the most effective solution. This way, we can ensure that the model produces the desired output consistently.\n",
    "\n",
    "Alternatively, we can consider re-running sections of text that haven't been formatted as intended. We might use a higher temperature setting during these runs and verify if the output aligns with our expectations.\n",
    "\n",
    "**Extraneous Outputs (Deviation from Guidelines):**\n",
    "As evident, the model occasionally generates text that doesn't adhere to the instructions we provided. Fortunately, we disregarded these extraneous outputs during the formatting process. To address this issue, we should investigate why this occurs and explore potential improvements to prevent such deviations from our guidelines in the future.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earnings_entity_linking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
