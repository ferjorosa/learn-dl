{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33d08417-49c8-405e-b61f-dbcb240655a0",
   "metadata": {
    "id": "33d08417-49c8-405e-b61f-dbcb240655a0"
   },
   "source": [
    "# 7 - Question answering\n",
    "\n",
    "There are many flavours of question answering (QA), but the most common is extractive QA, which involves questions whose answer can be identified as a psan of text in a document, where the document might be a web page, legal contract, or news article. The two-stage process of first retrieving relevant documents and then extracting answers from them is also the basis for many modern QA systems, including:\n",
    "\n",
    "* semantic search engines\n",
    "* intelligent assistants\n",
    "* automated information extractors\n",
    "\n",
    "In this chapter, we'll apply this process to tackle a common problem facing ecommerce websites: helping consumers answer specific queries to evaluate a product. We'll see that customer reviews can be used as a rich and challenging source of information for QA, and along the way we'll learn how transformers act as a powerful *reading comprehension* models that can extract meaning from text. \n",
    "\n",
    "----\n",
    "\n",
    "**Note:** This chapter focuses on <span style=\"color:blue\"><b>extractive QA</b></span>, but other forms of QA may be more suitable for your use case. For example, <span style=\"color:blue\">community QA</span> involves gathering question-answer pairs that are generated by users on forums like Stack Overflow, and then using semantic similarity search to find the closest matching answer to a new question. There is also <span style=\"color:blue\">long-form QA</span>, which aims to generate complex paragraph-length answers to open-ended questions like \"Why is the sky blue?\" Remarkably, it is also possible to do QA over tables, and transformer models like TAPAS can even perform aggregations to produce the final answer!\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e7cee-d205-466c-968e-dbb76ceaf1f6",
   "metadata": {
    "id": "959e7cee-d205-466c-968e-dbb76ceaf1f6",
    "tags": []
   },
   "source": [
    "## 7.1 - Building a review-based QA system\n",
    "\n",
    "If you have ever purchased a product online, you probably relied on customer reviews to help inform your decision. These reviews can often help answer specific questions like \"Does this guitar come with a strap?\" or \"Can I use this camera at night?\" that may be hard to answer from the product description alone. However, popular products can have hundreds to thousands of reviews, so it can be a major drag to find one that is relevant. One alternative is to post your question on the community QA platform provided by websites like Amazon, but it usually takes days to get an answer (if you get one at all). Wouldn't it be nice if we could get an immediate answer like Google sometimes provides with its search engine?\n",
    "\n",
    "<img src=\"images/google_search_qa.PNG\" title=\"\" alt=\"\" width=\"700\" data-align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e963c-7313-4f07-a9c0-2355d7cfc113",
   "metadata": {
    "id": "db1e963c-7313-4f07-a9c0-2355d7cfc113"
   },
   "source": [
    "### 7.1.1 - The dataset\n",
    "\n",
    "To build our QA system we'll use the [SubjQA dataset](https://arxiv.org/abs/2004.14283), which consists of more than 10000 customer reviews in English about products and services in six domains: Trip-Advisor, Restaurants, Movies, Books, Electronics, and Grocery. As illustrated in the following figure, each review is associated with a question that can be answered using one or more sentences from the review.\n",
    "\n",
    "<img src=\"images/qa_product_example.PNG\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\">\n",
    "\n",
    "the interesting aspect of this dataset is that most of the questions ansd answers are *subjective*; that is, they depend on the personal experience of the users. This example shows why this feature makes the task potentially more difficult than finding answers to factual questions like \"What is the currency of the United Kingdom?\" First, the query is about \"poor quality\", which is subjective and depends on the user's definition of quality. Second, important parts of the query do not appear in the review at all, which means it cannot be answered with shortcuts like keyword search or paraphrasing the input question. These features make SubjQA a realistic dataset to benchmark our review-based QA models on, since user-generated content like that resembles what we might encounter in the wild.\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** QA systems are usually categorized by the domain of data that they have access to when responding to a query. *Closed-domain QA* deals with questions about a narrow topic (e.g., a single product category), while *open-domain QA* deals with questions about almost anything (e.g., Amazon's whole product catalog). In general, closed-domain QA involves searching through fewer documents than the open-domain case.\n",
    "\n",
    "----\n",
    "\n",
    "Let's begin by downloading the dataset from the Hugging Face Hub. We can use the `get_dataset_config_names()` function to find out which subsets are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "L6JhEvDQeXiQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6JhEvDQeXiQ",
    "outputId": "d5d3c195-1e7d-4c7a-c04d-814996c4ef0a"
   },
   "outputs": [],
   "source": [
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c47ccc-34aa-491e-b8f9-1449650c21b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85c47ccc-34aa-491e-b8f9-1449650c21b2",
    "outputId": "9a4160ce-d6ef-43b9-88da-a4c5e50ddd6f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02b513fa13043d58afb381404e5c615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73620b832a243799c1c93d248d87a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['books', 'electronics', 'grocery', 'movies', 'restaurants', 'tripadvisor']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "domains = get_dataset_config_names(\"subjqa\")\n",
    "domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf7b1ab-1e0c-4446-bef3-6d01755ab2b4",
   "metadata": {
    "id": "faf7b1ab-1e0c-4446-bef3-6d01755ab2b4"
   },
   "source": [
    "For our use case, we'll focus on building a QA system for the Electronics domain. To download the `electronics` subset, we just need to pass this value to the `name` argument of the `load_dataset()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "744d29d8-6a7a-4536-b113-9d5ddc3cdf2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169,
     "referenced_widgets": [
      "c408069e6b254ce0b06d094a39a10d28",
      "dba5b02276894c608dd9035123710de3",
      "4ca99bda608242a59beb532a3d8c6482",
      "9cb5bf1e7a624d0a91905191f02ec7f2",
      "3fbe7974762347859d18c3c769d21ab7",
      "09cb8e4897434b4b9087c6756770585c",
      "40120e26ed56426abc7fc61338efef67",
      "6408c6ac2a7d403784fbffcd9c73bd29",
      "5fdaa8dbf21d46ac8b02b8dfae855061",
      "a20a44b491494f85abdcf1ee5b79f098",
      "b62b3c63c7764ef29178f57c914a6f6a",
      "1fb00d8024a14315aeef8725271ab34e",
      "cd34cc9618fa42d4a8be4886ed05d709",
      "f0881884802e4834bd080da76d4b95a0",
      "7e714d40de5c4313a80263d807c5ab74",
      "2a8ca2302bb146728741ccbbb72069a6",
      "834450d288564c6996a60194c9a88f1b",
      "b2858888ee9c490e92c5040803d5b195",
      "dd55761a227d4d39921dfb576f2def62",
      "efc2c348cd294a1d9c586015ba805fe7",
      "44a8c66ae0d349d0ac19042137df75d8",
      "5b13c36af4b844f2a26c32ba5ac837b9",
      "b55490741be14cdbbd8c5065de713a40",
      "473d57885b314873ac6b2543d6d9403c",
      "139aba392d8949c5a5f5d1d1d7e0a463",
      "80f5ca64a99c4372964527c2bda71ad5",
      "021c6f4d24f74c6783af283912439813",
      "59c6ca1ff54a44b2978fe96a8cc79856",
      "1040b2c842784511a0840a5988dfcd33",
      "e62f7df17864460080d6ecacacc60dc3",
      "ed153b0f2f1940beaa4a608d3d5071f8",
      "ead9dbc63c974fb0a848d15d9c1912f2",
      "a3d8b596c9244a20bcc147f9e29e8d55",
      "9767ecf362a543dfb62545af8858e7de",
      "5e8eaf4db808453c84e8df40a9bfcc07",
      "ad0a82604a674f38a2417b6e96db226b",
      "fb38593dbdad4907823ef8c52a48ee3d",
      "cf0ee4170da34819bd62967f64b9add3",
      "0a9687f5dd364d0e897ce79bad1ef849",
      "fc8c9cffba5d4ee39fc97c49b2dd7d9b",
      "bc298d01c1ce4b80b4a650fe9464ca88",
      "7cbefd561b514010b09465dfde954aaa",
      "65c68cec9a7b470b8873128065130636",
      "0dbf96ddc26448e3946c17713bfe3217",
      "80fd55e915654ed1897ad632ebf98ad9",
      "53f617a451fd4f4b95ae29d5cb247390",
      "135f98725668424797ab514b80c7b006",
      "528593b9e5f64aaaa279bd46b19565ac",
      "f437ad9b0c11414ab54baae5885b6ab5",
      "4f0e02f9ed5245868dd01cb9640a3eed",
      "c9eceb3ad55f4c43b29fe28fe6726f30",
      "adc5bb0d6e5a438c82ad36cbdd6937a7",
      "fed1d080ff8b46679b62bff6b8335a63",
      "f45ebe9bd0da421b9ec9ab8ae13cb185",
      "ea73e00acc124536a8e8b02cd8a3a545"
     ]
    },
    "id": "744d29d8-6a7a-4536-b113-9d5ddc3cdf2c",
    "outputId": "f7fd33ea-c46e-4d4b-b652-05f67b728ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset subjqa/electronics (download: 10.86 MiB, generated: 3.01 MiB, post-processed: Unknown size, total: 13.86 MiB) to C:\\Users\\rofe2001\\.cache\\huggingface\\datasets\\subjqa\\electronics\\1.1.0\\e5588f9298ff2d70686a00cc377e4bdccf4e32287459e3c6baf2dc5ab57fe7fd...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9361000ce87a4e469065a3b0ab702291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1295 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset subjqa downloaded and prepared to C:\\Users\\rofe2001\\.cache\\huggingface\\datasets\\subjqa\\electronics\\1.1.0\\e5588f9298ff2d70686a00cc377e4bdccf4e32287459e3c6baf2dc5ab57fe7fd. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ac5c92240b4f84bc5518503e357242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subjqa = load_dataset(\"subjqa\", name=\"electronics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c5e62-b4ee-4704-bd18-febdd4f89368",
   "metadata": {
    "id": "e24c5e62-b4ee-4704-bd18-febdd4f89368"
   },
   "source": [
    "Like other question answering datasets on the Hub, SubjQA stores the answers to each question as a nested dictionary. For example, if we inspect one of the rows in the `answers` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1951b071-2e34-40d3-b1d6-b7a88d9af6eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1951b071-2e34-40d3-b1d6-b7a88d9af6eb",
    "outputId": "1915223a-dd88-427b-924b-4628868d9c82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Bass is weak as expected', 'Bass is weak as expected, even with EQ adjusted up'], 'answer_start': [1302, 1302], 'answer_subj_level': [1, 1], 'ans_subj_score': [0.5083333253860474, 0.5083333253860474], 'is_ans_subjective': [True, True]}\n"
     ]
    }
   ],
   "source": [
    "print(subjqa[\"train\"][\"answers\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c0986-f1c8-49f8-b55b-f6a6cde05b6a",
   "metadata": {
    "id": "b38c0986-f1c8-49f8-b55b-f6a6cde05b6a"
   },
   "source": [
    "we can see that the answers are stored in a `text` field, while the starting character indices are provided in `answer_start`. To explore tehe dataset more easily, we'll flatten these nested column with the `flatten()` method and convert each split to a Pandas `DataFrame` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aCqWZLRfbFNE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCqWZLRfbFNE",
    "outputId": "c6c4335a-953b-4ddc-c8af-af2391660e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in train: 1295\n",
      "Number of questions in test: 358\n",
      "Number of questions in validation: 255\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n",
    "for split, df in dfs.items():\n",
    "  print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_8ApJoRsbbwp",
   "metadata": {
    "id": "_8ApJoRsbbwp"
   },
   "source": [
    "Notice that the dataset is relatively small, with only 1908 examples in total. This simulates a real-world scenario, since getting domain experts to label extractive QA datasets is labor-intensive and expensive. For example, the CUAD dataset for extractive QA on legal contracts is estmated to have a value of 2$ million to account for the legal expertise needed to annotate its 13000 examples! (see [Hedrycks et al., 2021](https://arxiv.org/abs/2103.06268))\n",
    "\n",
    "There are quite a few columns in the SubjQA dataset, but the most interesting ones for building our QA system are shown in the following table:\n",
    "\n",
    "<img src=\"images/subjqa_data.PNG\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\">\n",
    "\n",
    "\n",
    "Let's focus on these columns and take a look at a few of the training examples. We can use the `sample()` method to select a random sample:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "iUPUoHmYdDH0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "iUPUoHmYdDH0",
    "outputId": "75c13fef-e86b-4f2e-ee3a-2a928580b230"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "      <th>answers.text</th>\n",
       "      <th>answers.answer_start</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>B005DKZTMG</td>\n",
       "      <td>Does the keyboard lightweight?</td>\n",
       "      <td>[this keyboard is compact]</td>\n",
       "      <td>[215]</td>\n",
       "      <td>I really like this keyboard.  I give it 4 star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>B00AAIPT76</td>\n",
       "      <td>How is the battery?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>I bought this after the first spare gopro batt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title                        question                answers.text  \\\n",
       "791   B005DKZTMG  Does the keyboard lightweight?  [this keyboard is compact]   \n",
       "1159  B00AAIPT76             How is the battery?                          []   \n",
       "\n",
       "     answers.answer_start                                            context  \n",
       "791                 [215]  I really like this keyboard.  I give it 4 star...  \n",
       "1159                   []  I bought this after the first spare gopro batt...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_cols = [\"title\", \"question\", \"answers.text\", \"answers.answer_start\", \"context\"]\n",
    "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gyrrMEUzdHGs",
   "metadata": {
    "id": "gyrrMEUzdHGs"
   },
   "source": [
    "From these examples we can make a few observations. First, the questions are not gramatically correct, which is quite common in the FAQ sections of ecommerce websites. Second, an empty `answers.text` entry denotes \"unanswerable\" questions whose answer cannot be found in the review. Finally, we can use the start index and length of the answer span to slice out the text in the review that corresponds to the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ZhIDoutEdrDi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ZhIDoutEdrDi",
    "outputId": "e9906e99-e252-4f58-d276-af98c77f1944"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this keyboard is compact'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\n",
    "end_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\n",
    "sample_df[\"context\"].iloc[0][start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NyYorXRUdteF",
   "metadata": {
    "id": "NyYorXRUdteF"
   },
   "source": [
    "Next, let's get a feel for what types of questions are in the training set by counting the questions that begin with a few common starting words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sw7f2nrvd2_T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "sw7f2nrvd2_T",
    "outputId": "b47baea5-8a21-4d4d-adcf-e55381b569d9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = {}\n",
    "question_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\n",
    "\n",
    "for q in question_types:\n",
    "  counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]\n",
    "\n",
    "pd.Series(counts).sort_values().plot.barh()\n",
    "plt.title(\"Frequency of Question Types\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A9JOTlphe2xJ",
   "metadata": {
    "id": "A9JOTlphe2xJ"
   },
   "source": [
    "We can see that questions beginning with \"How\", \"What\", and \"Is\" are the most common ones, so let's have a look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OUnhK4tufKB2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUnhK4tufKB2",
    "outputId": "953009d9-e4b2-47a4-adac-496e3a6d85fd"
   },
   "outputs": [],
   "source": [
    "for question_type in [\"How\", \"What\", \"Is\"]:\n",
    "  for question in (\n",
    "    dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)]\n",
    "    .sample(n=3, random_state=42)['question']):\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qDNrmXTvfWn5",
   "metadata": {
    "id": "qDNrmXTvfWn5"
   },
   "source": [
    "-----\n",
    "\n",
    "**Note:** The (question, review, [answer sentences]) format of SubjQA is commonly used in extractive QA datasets, and was pioneered in the Stanford Question Answering Dataset (SQuAD), which was presented in [Rajpurkar et al (2016)](https://arxiv.org/abs/1606.05250). This is a famous dataset that is often used to test the ability of machines to read a passage of text and answer questions about it. The dataset was created by sampling several hundred English articles from Wikipedia, partitioning each article into paragraphs, and then asking crowdworkers to generate a set of questions and answers for each paragraph. In the first version of SQuAD, each answer to a question was guaranteed to exist in the corresponding passage. Bt it wasn't long before sequence models started performing better than humans at extracting the correct span of text with the answer. To make the task more difficult, SQuAD 2.0 was created by agumenting SQuAD 1.1 with a set of adversarial questions that are relevant to a given passage but cannot be answered from the text alone ([Rajpurkar and Liang, 2018](https://arxiv.org/abs/1806.03822)). The state of the art as of mid 2021 is shown in the following figure, with most models since 2019 surpassing human performance\n",
    "\n",
    "<img src=\"images/squad_historical_performance.PNG\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "However, this superhuman performance does not appear to reflect genuine reading comprehension, since answers to the \"unanswerable\" questions can usually be identified through patterns in the passages like antonyms. To adress these problems Google released the Natural Questions dataset ([Kwiatkowski, 2019](dx.doi.org/10.1162/tacl_a_00276)), which involves fact-seeking questions obtained from Google search users. The answers in NQ are much longer than in SQuAD and present a more challenging benchmark.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48659256-5278-48ce-8207-5d474e5e802c",
   "metadata": {},
   "source": [
    "### 7.1.2 - Extracting answers from text\n",
    "\n",
    "The first thing we'll need for our QA system is to find a way to identify a potential answer as a span of text in a customer review. For example, if we have a question like \"Is it waterproof?\" and the review passage is \"This watch is waterproof at 30m depth\", then the model should output \"waterproof at 30m\". To do this we'll need to understand how to:\n",
    "\n",
    "* Fram the supervised learning problem\n",
    "* Tokenize and encode text for QA tasks\n",
    "* Deal with long passages that exceed a model's maximum context size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1804c8-babc-4e64-ba06-d5de1e04dc7b",
   "metadata": {},
   "source": [
    "#### Span classification\n",
    "\n",
    "<span style=\"color:blue\">The most common ay to extract answers from text is by framing the problem as a <b>span classification task</b>, where the start and end tokens of an answer span act as the labels that a model needs to predict.</span>\n",
    "\n",
    "<img src=\"images/span_classification.PNG\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "Since our training set is relatively small, with only 1295 examples, a good strategy is to start with a language model that has already been fine-tuned on a large-scale QA dataset like SQuAD. In general, these models have strong reading comprehension capabilities and serve as a good baseline upon which to build a more accurate system.\n",
    "\n",
    "This is a somewhat different approach to that taken in previous chapters, where we tipically started with a pretrained model and fine-tuned the task-specific head ourselves. For extractive QA, we can actually start with a fine-tuned model since the structure of the labels remains the same across datasets.\n",
    "\n",
    "There a more than 400 QA models to choose from in the HuggingFace Hub, so which one should we pick? In general, the answer depends on various factors like wether our corpus is mono- or mulitlingual and the constraints of running the model in a production environment. The following table lists a few models that provide a good foundation to build on:\n",
    "\n",
    "<img src=\"images/qa_models_table.PNG\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\">\n",
    "\n",
    "For the purposes of this chapter, we will use a fine-tuned MiniLM model since it is fast to train and will allow us to quickly iterate on the techniques that we'll be exploring. As usual, the first thing we need is a tokenizer to encode our texts, so let's take a look at how this works for QA tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a635f-2a2d-47cc-a067-b0f6c006f71a",
   "metadata": {},
   "source": [
    "#### Tokenizing text for QA\n",
    "\n",
    "To encode our texts, we'll load the [**MiniLM model**](https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384) ([Wang et al., 2020](https://arxiv.org/abs/2002.10957)) checkpoint from the 🤗 Hub as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a2991-b371-41af-a8e5-6580f85e5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537acb1c-3c74-4fac-a45a-183fbe7fe980",
   "metadata": {},
   "source": [
    "To see the model in action, let's first try to extract an answer from a short passage of text. In extractive QA tasks, the inputs are provided as (question, context) pairs, so we pass them both to the tokenizer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28775c5-002c-47e1-b42b-ef5e561228a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How much music can this hold?\"\n",
    "context = \"An MP3 is about 1 MB/minute, so about 6000 hours depending on file size.\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d32fd0-4db6-4e40-b01a-6be2821e4834",
   "metadata": {},
   "source": [
    "The novel thing here is the `token_type_ids` tensor, which indicates which part of the inputs corresponds to the question and context (a 0 indicates a question token, a 1 indicates a context token).\n",
    "\n",
    "To understand how the tokenizer formats the inputs for QA tasks, let's decode the `input_ids` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b1081-7bf3-48b5-acd1-cfe707a280c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b163f-e082-4144-9195-023fe9cf5ff1",
   "metadata": {},
   "source": [
    "We see that for each QA example, the inputs take the format:\n",
    "\n",
    "`[CLS] question tokens [SEP] context tokens [SEP]`\n",
    "\n",
    "where the location of the first [SEP] token is determined by the token_type_ids. Now that our text is tokenized, we just need to instantiate the model with a QA head and run the inputs through the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03eefea-3a35-4693-95fd-f68609f61bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f45a54-5c9c-484c-ab4a-9868916a8240",
   "metadata": {},
   "source": [
    "Here we can see that we get a `QuestionAnsweringModelOutput` object as the output of the QA head. As previously illustrated, the QA head corresponds to a linear layer that takes the hidden states from the encoder and computes the logits for the start and end spans. This means that we treat QA as a form of token classification, similar to what we encountered for named entity recognition in Chapter 4. To convert the outputs into an answer span, we first need to get the logits for the start and end tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f63cac-6c77-4188-9ffa-271633b7999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = output.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "print(f\"Input IDs shape: {inputs.input_ids.size()}\")\n",
    "print(f\"Start logits shape: {start_logits.size()}\")\n",
    "print(f\"End logits shape: {end_logits.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977fa47d-c37b-4f08-88e9-d14fc210793b",
   "metadata": {},
   "source": [
    "If we compare the shapes of these logits to the input IDs, we see that there are two logits (a start and end) associated with each input token. As illustrated in the following figure, larger, positive logits correspond to more likely candidates for the start and end tokens. In this example, we can see that the model assigns the highest start token logits to the numbers \"1\" and \"6000\", which makes sense since our question is asking about a quantity. Similarly, we see that the end tokens with the highest logits are \"minute\" and \"hours\":\n",
    "\n",
    "<img src=\"images/predicted_logits_start_end_tokens.PNG\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "To get the final answer, we can compute the argmax over the start and end token logits and then slice the span from the inputs. The following code performs these steps and decodes the result so we can print the resulting text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29559bd-eab4-494c-aa48-b166ec94fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "start_idx = torch.argmax(start_logits)\n",
    "end_idx = torch.argmax(end_logits) + 1\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "answer = tokenizer.decode(answer_span)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6251771-0860-477d-b1af-7a845d23a14e",
   "metadata": {},
   "source": [
    "In 🤗 Transformers, all of these preprocessing and postprocessing steps are conveniently wrapped in a dedicated pipeline. We can instantiate the pipeline by passing our tokenizer and fine-tuned model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8860f3-7ee9-4dee-a31b-de91e91881a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "pipe(question=question, context=context, topk=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a0dc49-81e0-45e4-b47c-115014ae4cbd",
   "metadata": {},
   "source": [
    "In addition to the answer, the pipeline also returns the model's probability estimate in the `score` field (obtained by taking a softmax over the logits). This is handy when we want to compare multiple answers within a single context. We have also shown that we can have the model predict multiple answers by specifying the `topk` parameter. Sometimes, it is possible to have questions for which no answer is possible, like the empty `answers.answer_start` examples in SubjQA. In these cases the model will assign a high start and end score to the [CLS] token, and the pipeline maps this output to an empty string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0faa16-6f18-4009-9ef4-60f4cc68d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(question=\"Why is there no data?\", context=context, handle_impossible_answer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16220cb6-f3a8-492c-ab62-a6492d0077df",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Note:** In our simple example, we obtained the start and end indices by taking the argmax of the corresponding logits. However, this heuristic can produce out-of-scope answers by selecting tokens that belong to the question instead of the context. In practice, the pipeline computes the best combination of start and end indices subject to various constraints such as being in-scope, requiring the start indices to precede the end indices, and so on.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d26ea-f3e2-42ae-aa77-efeecd6951ce",
   "metadata": {},
   "source": [
    "#### Dealing with long passages\n",
    "\n",
    "One subtlety faced by reading comprehension models is that the context often contains more tokens than the maximum sequence length of the model (which is usually a few hundred tokens at most). As illustrated in the following figure, a decent portion of the SubjQA training set contains question-context pairs that won't fit within MiniLM's context size of 512\n",
    "\n",
    "<img src=\"images/context_size_minilm.PNG\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "For other tasks, like text classification, we usually truncate long texts under the assumption that enough information is contained in the embedding of the [CLS] token to generate accurate predictions. **For QA, however, this strategy is problematic because the answer to a question could lie near the end of the context and thus would be removed by truncation**. The standard way to deal with this is to apply a *sliding window* across the inputs, where each window contains a passage of tokens that fit in the model's context. We can see an example in the following figure, where the first bar corresponds to the question, while the second bar is the context captured in each window\n",
    "\n",
    "<img src=\"images/sliding_window_question.PNG\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "In 🤗 Transformers, we can set `return_overflowing_tokens=True` in the tokenizer to enable the sliding window. The size of the sliding window is controlled by the `max_seq_length` argument, and the size of the stride is controlled by `doc_stride`. Let's grab the first example from our training set and define a small window to illustrate how this works. In thi case we now get a list of `input_ids`, one for each window. Let's check the number of tokens we have in each window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd16ebf-2810-43df-a31d-3dfa7bdfe577",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dfs[\"train\"].iloc[0][[\"question\", \"context\"]]\n",
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"], example[\"context\"],\n",
    "    return_overflowing_tokens=True, max_length=100, stride=25\n",
    ")\n",
    "\n",
    "for idx, window in enumerate(tokenized_example[\"input_ids\"]):\n",
    "print(f\"Window #{idx} has {len(window)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df02e6-5677-4243-86c3-784bfa0a1e24",
   "metadata": {},
   "source": [
    "We can also see where two windows overlap by decoding the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfeecdc-1209-466a-b262-8cd5d6d95ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in tokenized_example[\"input_ids\"]:\n",
    "    print(f\"{tokenizer.decode(window)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a7ab2-db54-4f06-8b26-5ffac733ebfc",
   "metadata": {},
   "source": [
    "Now that we have some intuition about how QA models can extract answers from text, let's look at the other components we need to build an end-to-end QA pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6db3f3-9d8c-4193-8b83-da946b5ae870",
   "metadata": {},
   "source": [
    "### 7.1.3 - Using Haystack to build a QA pipeline\n",
    "\n",
    "In our simple answer extraction example, we provided both the question and the context to the model. However, in reality our system's users will only provide a question about a product, so we need some way of selecting relevant passages from among all the reviews in our corpus. One way to do this would be to concatenate all the reviews of a given product together and feed them to the model as a single, long context. Although simple, the drawback of this approach is that the context can become extremely long and thereby introduce an unacceptable latency for our users' queries.\n",
    "\n",
    "For example, let's suppose that on average, each product has 30 reviews and each review takes 100 milliseconds to process. If we need to process all the reviews to get an answer, this would result in an average latency of 3 seconds per user query - much too long for ecommerce websites!\n",
    "\n",
    "To handle this, modern QA systems are typically based on the **retriever-reader** architecture, which has two main components:\n",
    "\n",
    "* **Retriever**. Responsible for retrieving relevant documents for a given query. Retrievers are usually categorized as sparse or dense. Sparse retrievers use word frequencies to represent each document and query as a sparse vector. The relevance of a query and a document is then determined by computing an inner product of the vectors. On the other hand, dense retrievers use encoders like transformers to represent the query and document as contextualized embeddings (which are dense vectors). These embeddings encode semantic meaning, and allow dense retrievers to improve search accuracy by understanding the content of the query.\n",
    "\n",
    "* **Reader**. Responsible for extracting an answer from the documents provided by the retriever. The reader is usually a reading comprehension model, although at the end of the chapter we'll see examples of models that can generate free-form answers.\n",
    "\n",
    "<img src=\"images/retriever_reader_architecture.PNG\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "To build our QA system, we'll use the [Haystack library](https://haystack.deepset.ai/) developed by deepset, a german company focused on NLP. Haystack is based on the retriever-reader architecture, abstracts much of the complexity involved in building these systems, and integrates tightly with 🤗 Transformers.\n",
    "\n",
    "In addition to the retriever and reader, there are two more ocmponents involved when building a QA pipeline with Haystack:\n",
    "\n",
    "* Document store. A document-oriented database that stores documents and metadata which are provided to the retirever at query time.\n",
    "* Pipeline. Combines all the components of a QA system to enable custom query flows, merging documents from multiple retrievers and more.\n",
    "\n",
    "We'll first look at how we can use these components to quickly build a prototype QA pipeline. Later, we'll examine how we can improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33da361-018c-4f5a-b35c-488d2d071e99",
   "metadata": {},
   "source": [
    "#### Initializing a document store\n",
    "\n",
    "In Haystack, there are various document stores to choose from and each one can be paired with a dedicated set of retrievers. This is illustrated in the following table, where the compatibility of sparse (TF-IDF, BM25) and dense (Embedding, DPR) retrievers is shown for each of the avalable document stores. We'll explain what all these acronyms mean later in this chapter.\n",
    "\n",
    "<img src=\"images/haystack_retrievers_compatibility.PNG\" title=\"\" alt=\"\" width=\"350\" data-align=\"center\">\n",
    "\n",
    "Since we'll be exploring both sparse and dense retrievers in this chapter, we'll use the `ElasticSearchDocumentStore`, which is compatible with both retriever types. Ealstic-search is a search engine that is capable of handling a diverse range of data types, including textual, numerical, geospatial, structured, and unstructured. It ability to store huge volumes of data and quickly filter it with full-text search features makes it especially well suited for developing QA systems. It also has the advantage of being the industry standard for infraestructure analytics, so there is a good chance you work/end up working in a company that already has a cluster you can work with.\n",
    "\n",
    "To initialize the document store, we first need to download and install Elasticsearch. By following Elasticsearch's guide, we can grab the latest release for Linux with `wget` and unpack it with the `tar` shell command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c978985-8c01-47eb-81f2-00655cf3ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"\"\"https://artifacts.elastic.co/downloads/elasticsearch/\\\n",
    "elasticsearch-7.9.2-linux-x86_64.tar.gz\"\"\"\n",
    "!wget -nc -q {url}\n",
    "!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff3275-32b2-423a-8527-7a0d6bf07f73",
   "metadata": {},
   "source": [
    "Next we need to start the Elasticsearch server. Since we are running all the code in this book within Jupyter notebooks, we'll need to use Python's `Popen()` function to spawn a new process. While we're at it, let's also run the subprocess in the background using the `chown` shell command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338aa3c6-8bd4-4177-bb01-5804a97bb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "# Run Elasticsearch as a background process\n",
    "!chown -R daemon:daemon elasticsearch-7.9.2\n",
    "es_server = Popen(args=['elasticsearch-7.9.2/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\n",
    "\n",
    "# Wait until Elasticsearch has started\n",
    "!sleep 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e76ef-1f17-46a2-9ba4-e97be96bec9c",
   "metadata": {},
   "source": [
    "In the `Popen()` function, the `args` specify the program we wish to execute, while `stdout=PIPE` creates a new pipe for the standard output and `stderr=STDOUT` collects the errors in the same pipe. The `preexec_fn` argument specifies the ID of the subprocess we wish to use. By default, Elasticsearch runs locally on port 9200, so we can test the connection by sending an HTTP request to localhost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cef7a-b586-4e9b-bf3a-3fc137856228",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET \"localhost:9200/?pretty\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8716dc6-b155-4a84-ab94-4531bb3f7e2e",
   "metadata": {},
   "source": [
    "Now that our Elasticsearch server is up and running, the next thing to do is instantialate the document store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fe8c0-ac4d-4c60-9711-ed947694d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "\n",
    "# Return the document embedding for later use with dense retriever\n",
    "document_store = ElasticsearchDocumentStore(return_embedding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8ac93-70ff-4aa8-a33c-c0e792ad428a",
   "metadata": {},
   "source": [
    "By default, `ElasticsearchDocumentStore` creates two indices on Elasticsearch: one called `document` for (you guessed it) storing documents, and another called `label` for storing the annotated answer spans. For now, we'll just populate the `document` index with the SubjQA reviews, and Haystack's document stores expect a list of dictionaries with `text` and `meta` keys as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"text\": \"<the-context>\",\n",
    "    \"meta\": {\n",
    "        \"field_01\": \"<additional-metadata>\",\n",
    "        \"field_02\": \"<additional-metadata>\",\n",
    "    ...\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "the fileds in `meta` cane be used for applying filters during retrieval. For our purposes we'll include the item_id and q_review_id columns of SubjQA so we can filter by product and question ID, along with the corresponding training split. We can then loop through the examples in each `DataFrame` and add them to the index with the `write_documents()` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f55f8e-1b5b-482b-9f04-4391695de808",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, df in dfs.items():\n",
    "    # Exclude duplicate reviews\n",
    "    docs = [{\"text\": row[\"context\"],\n",
    "             \"meta\":{\"item_id\": row[\"title\"], \"question_id\": row[\"id\"],\n",
    "                     \"split\": split}}\n",
    "            for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
    "    document_store.write_documents(docs, index=\"document\")\n",
    "\n",
    "print(f\"Loaded {document_store.get_document_count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc516bf8-02a1-47db-9e9a-59188c5b4727",
   "metadata": {},
   "source": [
    "Great, we have loaded all our reviews into an index! To search the index we'll need a retriever, so let's look at how can initialize one for Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4e6d9-5802-4f4d-8173-276804c57ef0",
   "metadata": {},
   "source": [
    "#### Initializing a retriever\n",
    "\n",
    "The Elasticsearch document store can be paired with any of the Haystack retrievers, so let's start by using a sparse retriever based on BM25 (short for \"Best Match 25\"). BM25 is an improved version of the classic Term Frequency-Inverse Document Frequency (TF-IDF) algorithm and represents the question and context as sparse vectors that can be searched efficiently on Elasticsearch. The BM25 score measures how much matched text is about a search query and improves on TF-IDF by saturating TF values quickly and normalizing the document length so that short documents are favored over long ones.\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** For an in-depth explanation of document scoring with TF-IDF and BM25 see Chapter 23 of [Speech and Language Processing, 3rd edition, by D. Jurafsky and J.H. Martin (Prentice Hall)](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf).\n",
    "\n",
    "----\n",
    "\n",
    "In Haystack, the BM25 retriever is used by default in ElasticsearchRetriever, so let's initialize this class by specifying the document store we wish to search over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205be02-7a20-478e-9f1b-d96da8eececf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "\n",
    "es_retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94291de-e1c5-4c8f-9e1c-80adbfc36e13",
   "metadata": {},
   "source": [
    "Next, let's look at a simple query for a single electronics product in the training set. For review-based QA systems like ours, it's important to restrict the queries to a single item because otherwise the retriever would source reviews about products that are not related to a user’s query. For example, asking “Is the camera quality any good?” without a product filter could return reviews about phones, when the user might be asking about a specific laptop camera instead. By themselves, the ASIN values in our dataset are a bit cryptic, but we can decipher them with online tools like [amazon ASIN](https://amazon-asin.com/) or by simply appending the value of item_id to the [www.amazon.com/dp/](www.amazon.com/dp/) URL. The following item ID corresponds to one of Amazon's Fire tablets, so let's use the retriever's `retrieve()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d60e9-f2c8-4772-a103-806fec0fee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = \"B0074BW614\"\n",
    "query = \"Is it good for reading?\"\n",
    "retrieved_docs = es_retriever.retrieve(\n",
    "query=query, top_k=3, filters={\"item_id\":[item_id], \"split\":[\"train\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a3a7b9-4676-4a9e-a296-04ffb11d564e",
   "metadata": {},
   "source": [
    "Here we have specified how many documents to return with the top_k argument and applied a filter on both the item_id and split keys that were included in the meta field of our documents. Each element of `retrieved_docs` in a Haystack `Document` object that is used to represent documents and includes the retriever's query score along with other metadata. Let's have a look at one of the retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411d635-82b3-44a2-b5df-79707ef60e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5d97d-f614-4011-a509-82414bae5c7d",
   "metadata": {},
   "source": [
    "In addition to the document's text, we can see the `score` that Elasticsearch computed for its relevance to the query (larger scores imply a better match). Under the hood, Elasticsearch relies on [Lucene](https://lucene.apache.org/) for indexing and search, so by default it uses Lucene's *practical scoring function*. You can find the nitty-gritty details behind the scoring function in the [Elasticsearch documentation](https://www.elastic.co/guide/en/elasticsearch/guide/current/practical-scoring-function.html), but in brief terms in first filters the candidate documents by applying a Boolean test (does the document match the query?), and then applies a similarity metric that's based on representing both the document and the query as vectors.\n",
    "\n",
    "Now that we have a way to retrieve relevant documents, the next thing we need is a way to extract answers from them. This is where the reader comes in, so let's take a look at how we can load our MiniLM model in Haystack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c3f81-c3f2-4f59-8e17-d853cec86580",
   "metadata": {},
   "source": [
    "#### Initializing a reader\n",
    "\n",
    "In Haystack, there are two types of readers one can use to extract answers from a given context:\n",
    "\n",
    "* `FARMReader`. Based on deepset's [FARM framework](https://farm.deepset.ai/) for fine-tuning and deploying transformers. Compatible with models trained using 🤗 Transformers and can load models directly from the Hugging Face Hub.\n",
    "\n",
    "* `TransformersReader`. Based on the QA pipeline from Transformers. Suitable for running inference only.\n",
    "\n",
    "Although both readers handle a model's weights in the same way, there are some differences in the way the predicitons are converted to produce answers:\n",
    "\n",
    "* In 🤗 Transformers, the QA pipeline normalizes the start and end logits with a softmax in each passage. This means that it is only meaningful to compare answer scores between answers extracted from the same passage, where the probabilities sum to 1. For example, an answer score of 0.9 from one passage is not necessarily better than a score of 0.8 in another. In FARM, the logits are not normalized, so inter-passage answers can be compared more easily.\n",
    "\n",
    "* The `TransformersReader` sometimes predicts the same answer twice, but with different scores. This can happen in long contexts if the answer lies across two overlapping windows. In FARM, these duplicates are removed.\n",
    "\n",
    "Since we will be fine-tuning the reader later in the chapter, we'll use the `FARMReader`. As with Transformers, to load the model we just need to specify the MiniLM checkpoint on the Hugging Face Hub along with some QA-specific arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422e0dc-57a3-4aef-b430-b7262ba50edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.reader.farm import FARMReader\n",
    "\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "max_seq_length, doc_stride = 384, 128\n",
    "reader = FARMReader(model_name_or_path=model_ckpt, progress_bar=False, max_seq_len=max_seq_length, doc_stride=doc_stride, return_no_answer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a017a-7d6d-4008-9100-5aee2f982673",
   "metadata": {},
   "source": [
    "In `FARMReader`, the behaviour of the sliding window is controlled by the same max_sq_length and doc_stride arguments that we saw for the tokenizer. Here we have used the values from the MiniLM paper. To confirm, let's now test the reader on our simple example from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d188c34-905a-4dcd-81a4-d4ae88b32e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reader.predict_on_texts(question=question, texts=[context], top_k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18648ede-793f-4f6d-a3d9-cca416992b4e",
   "metadata": {},
   "source": [
    "Great, the reader appears to be working as expected—so next, let’s tie together all our components using one of Haystack’s pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91344d2-6bca-44d0-b577-697296702840",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "\n",
    "Haystack provides a `Pipeline` abstraction that allows us to combine retrievers, readers, and other components together as a graph that can be easily customized for each use case. There are also predefined pipelines analogous to those in 🤗 Transformers, but specialized for QA systems. In our case, we are interested in extracting answers, so we'll use the `ExtractiveQAPipeline`, which takes a single retriever-reader pair as its arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82019ea7-7ac3-492f-9cbc-eccc3a0549e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipeline import ExtractiveQAPipeline\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, es_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1325519-4c5b-4114-be13-6f3ec8c20ade",
   "metadata": {},
   "source": [
    "Each `Pipeline` has a `run()` method that specifies how the query flow should be executed. For the `ExtractiveQAPipeline` we just need to pass the `query`, the number of documents to retrieve with `top_k_retriever`, and the number of answers to extract from these documents with `top_k_reader`. In our case, we also need to specify ta filter over the item ID which can be done using the `filters` argument as we did with the retriever earlier. Let's run a simple example using our question about the Amazon Fire tablet again, but this time returning the extracted answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87542573-595c-428d-961d-0813f5399f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_answers = 3\n",
    "preds = pipe.run(query=query, top_k_retriever=3, top_k_reader=n_answers, filters={\"item_id\": [item_id], \"split\":[\"train\"]})\n",
    "print(f\"Question: {preds['query']} \\n\")\n",
    "\n",
    "for idx in range(n_answers):\n",
    "    print(f\"Answer {idx+1}: {preds['answers'][idx]['answer']}\")\n",
    "    print(f\"Review snippet: ...{preds['answers'][idx]['context']}...\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3132a-982c-41da-b985-67218923aaba",
   "metadata": {},
   "source": [
    "Great, we now have an end-to-end QA system for Amazon product reviews! This is a good start, but notice that compared to the first answer returned by the system, the second and third answers are closer to what the question is actually asking. To do better, we'll need some metrics to quantify the performance of the retriever and reader. We'll take a look at that next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94806b84-eca8-460d-a3a9-49e7454e3fc2",
   "metadata": {},
   "source": [
    "## 7.2 - Improving our QA pipeline"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "021c6f4d24f74c6783af283912439813": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "09cb8e4897434b4b9087c6756770585c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a9687f5dd364d0e897ce79bad1ef849": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dbf96ddc26448e3946c17713bfe3217": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1040b2c842784511a0840a5988dfcd33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "135f98725668424797ab514b80c7b006": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adc5bb0d6e5a438c82ad36cbdd6937a7",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fed1d080ff8b46679b62bff6b8335a63",
      "value": 3
     }
    },
    "139aba392d8949c5a5f5d1d1d7e0a463": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e62f7df17864460080d6ecacacc60dc3",
      "max": 358,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed153b0f2f1940beaa4a608d3d5071f8",
      "value": 358
     }
    },
    "1fb00d8024a14315aeef8725271ab34e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cd34cc9618fa42d4a8be4886ed05d709",
       "IPY_MODEL_f0881884802e4834bd080da76d4b95a0",
       "IPY_MODEL_7e714d40de5c4313a80263d807c5ab74"
      ],
      "layout": "IPY_MODEL_2a8ca2302bb146728741ccbbb72069a6"
     }
    },
    "2a8ca2302bb146728741ccbbb72069a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "3fbe7974762347859d18c3c769d21ab7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40120e26ed56426abc7fc61338efef67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44a8c66ae0d349d0ac19042137df75d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "473d57885b314873ac6b2543d6d9403c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59c6ca1ff54a44b2978fe96a8cc79856",
      "placeholder": "​",
      "style": "IPY_MODEL_1040b2c842784511a0840a5988dfcd33",
      "value": "Generating test split:  96%"
     }
    },
    "4ca99bda608242a59beb532a3d8c6482": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6408c6ac2a7d403784fbffcd9c73bd29",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5fdaa8dbf21d46ac8b02b8dfae855061",
      "value": 1
     }
    },
    "4f0e02f9ed5245868dd01cb9640a3eed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "528593b9e5f64aaaa279bd46b19565ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f45ebe9bd0da421b9ec9ab8ae13cb185",
      "placeholder": "​",
      "style": "IPY_MODEL_ea73e00acc124536a8e8b02cd8a3a545",
      "value": " 3/3 [00:00&lt;00:00, 85.34it/s]"
     }
    },
    "53f617a451fd4f4b95ae29d5cb247390": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f0e02f9ed5245868dd01cb9640a3eed",
      "placeholder": "​",
      "style": "IPY_MODEL_c9eceb3ad55f4c43b29fe28fe6726f30",
      "value": "100%"
     }
    },
    "59c6ca1ff54a44b2978fe96a8cc79856": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b13c36af4b844f2a26c32ba5ac837b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e8eaf4db808453c84e8df40a9bfcc07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a9687f5dd364d0e897ce79bad1ef849",
      "placeholder": "​",
      "style": "IPY_MODEL_fc8c9cffba5d4ee39fc97c49b2dd7d9b",
      "value": "Generating validation split: 100%"
     }
    },
    "5fdaa8dbf21d46ac8b02b8dfae855061": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6408c6ac2a7d403784fbffcd9c73bd29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "65c68cec9a7b470b8873128065130636": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cbefd561b514010b09465dfde954aaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7e714d40de5c4313a80263d807c5ab74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44a8c66ae0d349d0ac19042137df75d8",
      "placeholder": "​",
      "style": "IPY_MODEL_5b13c36af4b844f2a26c32ba5ac837b9",
      "value": " 1295/1295 [00:10&lt;00:00, 470.56 examples/s]"
     }
    },
    "80f5ca64a99c4372964527c2bda71ad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ead9dbc63c974fb0a848d15d9c1912f2",
      "placeholder": "​",
      "style": "IPY_MODEL_a3d8b596c9244a20bcc147f9e29e8d55",
      "value": " 345/358 [00:03&lt;00:00, 233.39 examples/s]"
     }
    },
    "80fd55e915654ed1897ad632ebf98ad9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53f617a451fd4f4b95ae29d5cb247390",
       "IPY_MODEL_135f98725668424797ab514b80c7b006",
       "IPY_MODEL_528593b9e5f64aaaa279bd46b19565ac"
      ],
      "layout": "IPY_MODEL_f437ad9b0c11414ab54baae5885b6ab5"
     }
    },
    "834450d288564c6996a60194c9a88f1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9767ecf362a543dfb62545af8858e7de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e8eaf4db808453c84e8df40a9bfcc07",
       "IPY_MODEL_ad0a82604a674f38a2417b6e96db226b",
       "IPY_MODEL_fb38593dbdad4907823ef8c52a48ee3d"
      ],
      "layout": "IPY_MODEL_cf0ee4170da34819bd62967f64b9add3"
     }
    },
    "9cb5bf1e7a624d0a91905191f02ec7f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a20a44b491494f85abdcf1ee5b79f098",
      "placeholder": "​",
      "style": "IPY_MODEL_b62b3c63c7764ef29178f57c914a6f6a",
      "value": " 11.4M/? [00:02&lt;00:00, 5.38MB/s]"
     }
    },
    "a20a44b491494f85abdcf1ee5b79f098": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3d8b596c9244a20bcc147f9e29e8d55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad0a82604a674f38a2417b6e96db226b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc298d01c1ce4b80b4a650fe9464ca88",
      "max": 255,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7cbefd561b514010b09465dfde954aaa",
      "value": 255
     }
    },
    "adc5bb0d6e5a438c82ad36cbdd6937a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2858888ee9c490e92c5040803d5b195": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b55490741be14cdbbd8c5065de713a40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_473d57885b314873ac6b2543d6d9403c",
       "IPY_MODEL_139aba392d8949c5a5f5d1d1d7e0a463",
       "IPY_MODEL_80f5ca64a99c4372964527c2bda71ad5"
      ],
      "layout": "IPY_MODEL_021c6f4d24f74c6783af283912439813"
     }
    },
    "b62b3c63c7764ef29178f57c914a6f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc298d01c1ce4b80b4a650fe9464ca88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c408069e6b254ce0b06d094a39a10d28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dba5b02276894c608dd9035123710de3",
       "IPY_MODEL_4ca99bda608242a59beb532a3d8c6482",
       "IPY_MODEL_9cb5bf1e7a624d0a91905191f02ec7f2"
      ],
      "layout": "IPY_MODEL_3fbe7974762347859d18c3c769d21ab7"
     }
    },
    "c9eceb3ad55f4c43b29fe28fe6726f30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd34cc9618fa42d4a8be4886ed05d709": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_834450d288564c6996a60194c9a88f1b",
      "placeholder": "​",
      "style": "IPY_MODEL_b2858888ee9c490e92c5040803d5b195",
      "value": "Generating train split: 100%"
     }
    },
    "cf0ee4170da34819bd62967f64b9add3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dba5b02276894c608dd9035123710de3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09cb8e4897434b4b9087c6756770585c",
      "placeholder": "​",
      "style": "IPY_MODEL_40120e26ed56426abc7fc61338efef67",
      "value": "Downloading data: "
     }
    },
    "dd55761a227d4d39921dfb576f2def62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e62f7df17864460080d6ecacacc60dc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea73e00acc124536a8e8b02cd8a3a545": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ead9dbc63c974fb0a848d15d9c1912f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed153b0f2f1940beaa4a608d3d5071f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "efc2c348cd294a1d9c586015ba805fe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f0881884802e4834bd080da76d4b95a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd55761a227d4d39921dfb576f2def62",
      "max": 1295,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_efc2c348cd294a1d9c586015ba805fe7",
      "value": 1295
     }
    },
    "f437ad9b0c11414ab54baae5885b6ab5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f45ebe9bd0da421b9ec9ab8ae13cb185": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb38593dbdad4907823ef8c52a48ee3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65c68cec9a7b470b8873128065130636",
      "placeholder": "​",
      "style": "IPY_MODEL_0dbf96ddc26448e3946c17713bfe3217",
      "value": " 255/255 [00:13&lt;00:00, 206.46 examples/s]"
     }
    },
    "fc8c9cffba5d4ee39fc97c49b2dd7d9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fed1d080ff8b46679b62bff6b8335a63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
