{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc8784a-783a-4e6b-aaa4-70b4330b2074",
   "metadata": {
    "tags": []
   },
   "source": [
    "# How to build an open-domain Question Answering System?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645d65c-a349-40e5-9f0e-e0af794eb6f7",
   "metadata": {},
   "source": [
    "[Original article](https://lilianweng.github.io/posts/2020-10-29-odqa/)\n",
    "\n",
    "A model that can answer any question with regard to factual knowledge can lead to many useful and practical applications, such as working as a chatbot or an AI assistant. In this notebook, we will review several common approaches for building such an open-domain question answering system.\n",
    "\n",
    "Disclaimers given so many papers in the wild:\n",
    "\n",
    "* Assume we have access to a powerful pretrained language model.\n",
    "* We do not cover how to use structured knowledge base (e.g., Freebase, WikiData) here\n",
    "* We only focus on a single-turn QA instead of a multi-turn conversation style QA\n",
    "* We mostly focus on QA models that contain neural networks, specially Transformer-based language models.\n",
    "* This tutorial is based ona post from 2020, so there are probably many missing papers (from the \"future\" and from the \"past\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76dcbf-e883-447e-b5f0-7669e696e15c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 - What is open-domain question answering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099dbb6-5ced-47f6-bd7a-1981a2c037f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Open-domain Question Answering (ODQA)** is a type of language tasks, asking a model to produce answers to factoid questions in natural language. The true answer is objective, so it is simple to evaluate model performance. \n",
    "\n",
    "For example,\n",
    "\n",
    "```yaml\n",
    "Question: What did Albert Einstein win the Nobel Prize for?\n",
    "Answer: The law of the potoelectric effect.\n",
    "```\n",
    "\n",
    "The \"open-domain\" part refers to the lack of relevant context for any arbitrarily asked factual question. In the above case, the model only takes as the input the question but no article \"why Einstein didn't win a Nobel Prize for the theory of relativity\" is provided, where the term \"the law of the photoelectric effect\" is likely mentioned. In the case when both the question and the context are provided the task is known as **Reading comprehension (RC)**.\n",
    "\n",
    "When considering different types of open-domain questions, [Lewis et al. (2020)](https://arxiv.org/abs/2008.02637) provides a classification in order of difficulty:\n",
    "\n",
    "1. A model is able to correctly memorize and respond with the answer to a question that has been seen at training time\n",
    "2. A model is able to answer novel questions at test time and choose an answer from the set of answers it has seen during training\n",
    "3. A model is able to answer novel questions which have answers not contained in the training dataset\n",
    "\n",
    "<img src=\"images_odqa/QA-summary.png\" title=\"\" alt=\"\" width=\"750\" data-align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac51eb93-bb31-4882-b869-850bd52eba74",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 - Notation\n",
    "\n",
    "Given a question $x$ and a ground truth answer span $y$, the context passage containing the true answer is labelled as $z \\in \\mathcal{Z}$, where $\\mathcal{Z}$ is an external knowledge corpus. Wikipedia is a common choice for such an external knowledge source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1fcb7-eb76-4e9a-ad26-5cfb2a6be421",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 - Concerns of QA data fine-tuning\n",
    "\n",
    "Before we dive into the detail of many models below. I would like to point out one concern of fine-tuning a model with common QA datasets, which appears as one fine-tuning step in several ODQA models. It could be concerning, because there is a significant overlap between questions in the train and test sets in several public QA datasets.\n",
    "\n",
    "[Lewis et al. (2020)](https://arxiv.org/abs/2008.02637) ([code](https://github.com/facebookresearch/QA-Overlap)) found that 58-71% of test-time answers are also present somewhere in the training sets and 28-34% of test-set questions have a near-duplicate paraphrase in their corresponding training sets. In their experiments, several models performed notably worse when duplicated or paraphrased questions were removed from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1f2d6-f435-4691-b16c-ca8052a2756e",
   "metadata": {},
   "source": [
    "## 2 - Open-book QA: Retriever-reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee7894-cefb-4b98-b877-e3b067da9212",
   "metadata": {},
   "source": [
    "Given a factoid question, if a language model has no context or is not big neough to memorize the context which exists in the training dataset, it is unlikely to guess the correct answer. In an open-book exam, students are allowed to refer to external resources like notes and books while answering test questions. Similarly, a ODQA system can be paired with a rich knowledge base to identify relevant documents as evidence of answers.\n",
    "\n",
    "We can decompose the process of finding answers to given questions into two stages,\n",
    "\n",
    "1. Find the related context in an external repository of knowledge;\n",
    "2. Process the retrieved context to extract an answer.\n",
    "\n",
    "<img src=\"images_odqa/retriever_reader_framework.png\" title=\"\" alt=\"\" width=\"650\" data-align=\"center\">\n",
    "\n",
    "This architecture was first proposed in DrQA (\"Document retriever Question-Answering\" by [Chen et al., 2017](https://arxiv.org/abs/1704.00051); [code](https://github.com/facebookresearch/DrQA)). The retriever and the reader components can be set up and trained independently, or jointly trained end-to-end (explained further below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c3eeb-63f6-4176-be3f-680d31e55905",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 - Retriever model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b2cecb-20b0-46dd-88b0-ad52dd7b186f",
   "metadata": {
    "tags": []
   },
   "source": [
    "An information retrieval (IR) system is usually used for implementing the retriever. Retrievers are usually organized in two groups, sparse and dense:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece08f6c-332e-4a20-a4af-335a919bc897",
   "metadata": {},
   "source": [
    "#### 2.1.1 - Sparse IR\n",
    "\n",
    "Sparse retrievers use word frequencies to represent each document and query as a sparse vector. The relevance of a query and a document is then determined by computing an inner product of the vectors. The two most commont techniques are the following:\n",
    "\n",
    "1. The (non-learning) [term frequency-inverse document frequency (TF-IDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) technique, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The TF-IDF value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. TF-IDF is one of the most popular term-weighting schemes today.\n",
    "2. The (non-learning) [Best Match 25 (BM-25)](https://es.wikipedia.org/wiki/Okapi_BM25) technique, which is an improved version of TF-IDF that takes into account additional factors that can affect the relevance of a document to a given query. These factors include the length of the document, the length of the query, and the average length of documents in the collection. BM25 also includes a parameter called k1 that can be adjusted to tune the importance of these factors. As a result, BM25 is able to produce more accurate and relevant rankings than basic TF-IDF, particularly for longer queries and collections of documents.\n",
    "----\n",
    "\n",
    "**Note:** For an in-depth explanation of document scoring with TF-IDF and BM25 see Chapter 23 of [Speech and Language Processing, 3rd edition, by D. Jurafsky and J.H. Martin (Prentice Hall)](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf).\n",
    "\n",
    "----\n",
    "\n",
    "**DrQA** ([Chen et al., 2017](https://arxiv.org/abs/1704.00051); [code](https://github.com/facebookresearch/DrQA)) adopts an efficient non-learning based search engine based on the vector space model. Every query and document is modelled as a bag-of-word vector, where each term is weighted by TF-IDF\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\text{TF-IDF}(t,d,\\mathcal{D}) &=  \\text{TF}(t,d) \\times \\text{IDF}(t, \\mathcal{D})\\\\\n",
    "\\text{TF} &= \\log(1 + \\text{freq}(t,d))\\\\\n",
    "\\text{IDF} &= \\log(\\frac{|\\mathcal{D}|}{|d \\in \\mathcal{D}: t \\in d|})\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $t$ is a unigram or bigram term in a document $d$ from a collection of documents $\\mathcal{D}$. $\\text{freq}(t,d)$ measures how many times a term $t$ appears in $d$. Note that the term-frequency here includes bigram counts too, which is found  to be very helpful because the local word order is taken into consideration via bigrams. As part of the implementation, DrQA maps the bigrams of $2^{24}$ bins using unsigned murmur3 hash.\n",
    "\n",
    "Precisely, DrQA implemented Wikipedia as its knowledge source and this choice has became a default setting for many ODQA studies since then. The non-ML document retriever returns the top $k=5$ most relevant Wikipedia articles given a question.\n",
    "\n",
    "**BERTserini** ([Yang et al., 2019](https://arxiv.org/abs/1902.01718)) pairs the open-source Anserini IR toolkit as the retriever with a fine-tuned pre-trained BERT model as the reader. The top $k$ documents ($k=10$) are retrieved via Anserini, where the query is treated as a bag of words. The retrieved text segments are ranked by BM25. In terms of the effect of text granularity on performance, they found that paragraph retrieval > sentence retrieval > article retrieval.\n",
    "\n",
    "<img src=\"images_odqa/bertserini.png\" title=\"\" alt=\"\" width=\"650\" data-align=\"center\">\n",
    "\n",
    "**Multi-passage BERT** ([Wang et al., 2019]()) approaches this problem by combining ElasticSearch + BM25. The authors of this approach found that splitting articles into passages with the lenght of 100 words by *sliding window* (i.e., with some overlap) brings 4% improvements, since splitting documents into passages without overlap may cause some near-boundary evidence to lose useful contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a7343-0e97-47a2-aecd-4303631628a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1.2 - Dense IR\n",
    "\n",
    "There is a long history in learning a low-dimensional representation of text, denser than raw term-based vectors ([Deerwester et al., 1990](http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf); [Yih et al., 2011](https://aclanthology.org/W11-0329/)). Dense representations can be learned through matrix decomposition or some neural network architectures (e.g., MLP, LSTM, bidirectional LSTM, etc.). When involving neural networks, such approaches are referred to as \"Neural IR\", Neural IR is a new category of methods for retrieval problems, but it is not necessarily better/superior than classic IR ([Lim et al., 2018](https://sigir.org/wp-content/uploads/2019/01/p040.pdf))\n",
    "\n",
    "After the success of many large-scale general language models (e.g., BERT, GPT, T5, etc.), many QA models embrace the following approach:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "h_{x} &= E_{x}(x) \\\\\n",
    "h_{z} &= E_{z}(z) \\\\\n",
    "\\text{score}(x,z) &= h^{T}_{x} h_{z}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "1. Extract dense representations of a question $x$ and a context passage $z$ by feeding them into a language model (i.e., $E_{x}$ and $E_{z}$ respectively);\n",
    "2. Use the dot-product of these two representations as the retrieval score to rank and select most relevant passages.\n",
    "\n",
    "**ORQA** ([Lee et al., 2019](https://arxiv.org/abs/1906.00300)), **REALM** ([Guu et al., 2020](https://arxiv.org/abs/2002.08909)), and **DPR** ([Karpukhin et al., 2020](https://arxiv.org/abs/2004.04906)) all use such a scoring function for context retrieval, which will be described in detail in a later section on the end-to-end QA model.\n",
    "\n",
    "An extreme approach investigated by **DenseSPI** (\"Dense-Sparse Phrase Index\"; [Seo et al., 2019](https://arxiv.org/abs/1906.05807)), is to encode all the text in the knowledge corpus at the phrase level and then only rely on the retriever to identify the most relevant phrase as the predicted answer. **In this way, the retriever + reader pipeline is reduced to only retriever**. Of course, **the index would be much larger and the retrieval problem is more challenging.**\n",
    "\n",
    "**DenSPI** introduces a *query-agnostic* indexable representation of document phrases. Precisely it encodes query-agnostic representations of text spans in Wikipedia offline and looks for the answer at inference time by performing nearest neighbor search. It can drastically speed up the inference time, because there is no need to re-encode documents for every new query, which is often required by a reader model.\n",
    "\n",
    "Given a question $x$ and a fixed and a fixed set of $K$ (Wikipedia) documents, $z_{1}, \\dots, z_{K}$, where each document $z_{k}$ contains $N_{k}$ words, $z_{k} = \\langle z_{k}^{(1)}, \\dots, z_{k}^{(N_{k})} \\rangle$. An ODQA model is a scoring function $F$ for each candidate phrase span $z_{k}^{(i:j)}$, $1 \\leq i \\leq j \\leq N_{k}$, such that the truth answer is the phrase with maximum score: $y = \\text{arg max}_{k,i,j} F(x, z_{k}^{(i:j)})$.\n",
    "\n",
    "The phrase representation $z_{k}^{(i:j)}$ combines both dense and sparse vectors, $z_{k}^{(i:j)} = [d_{k}^{(i:j)}, s_{k}^{(i:j)}] \\in \\mathbb{R}^{\\text{dim}_{d} + \\text{dim}_{s}}$ (note that the dimensionality of the dense vector is much smaller than the dimensionality of the sparse vector, i.e., $\\text{dim}_{d} \\ll \\text{dim}_{s}$)\n",
    "\n",
    "* The dense vector $d_{k}^{(i:j)}$ is effective for encoding local *syntactic* and *semantic* cues, as what can be learned by a pretrained language model.\n",
    "* The sparse vector $s_{k}^{(i:j)}$ is superior at encoding precise *lexical* information. The sparse vector is term-frequency-based encoding. DenSPI uses 2-gram term-frequency (same as DrQA), resulting in a highly sparse representation ($\\text{dim}_{s} \\approx \\text{16M}$)\n",
    "\n",
    "The dense vector $d^{i:j}$ is further decomposed into three parts, $d(i:j) = [a_{i}, b_{j}, c_{ij}] \\in \\mathbb{R}^{2 \\ \\text{dim}_{b}+1}$ where $2 \\ \\text{dim}_{b}+1 = \\text{dim}_{d}$. All three components are learned based on different columns of the fine-tuned BERT representations.\n",
    "\n",
    "* A vector $a_{i}$ encodes the *start* position for the $i$-th word of the document;\n",
    "* A vector $a_{i}$ encodes the *start* position for the $i$-th word of the document;\n",
    "* A scalar $c_{ij}$ measures the coherency between the start and the end vectors, helping avoid non-constituent phrases during inference.\n",
    "\n",
    "For all possible $(i,j,k)$ tuples where $j - i < J$, the text span embeddings are precomputed and stored as a *phrase index*. The maximum span length $J$ is a predefined scalar constant.\n",
    "\n",
    "<img src=\"images_odqa/DenSPI.png\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\">\n",
    "\n",
    "At the inference time, the question is mapped into the same vector space $x = [d', s'] \\in \\mathbb{R}^{d^{d}+d^{s}}$, where the dense vector $d'$ is extracted from the BERT embedding of the special `[CLS]` symbol. The same BERT model is shared for encoding both questions and phrases. The final answer s predicted by $k^{*}, i^{*}, j^{*} = \\text{arg max} \\ x^{\\top} z_{k}^{(i:j)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167fc0ee-6928-47e3-b310-132db9d622c4",
   "metadata": {},
   "source": [
    "### 2.2 - Reader model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b7a0f-abbd-4dbb-a705-a4e244f8c49d",
   "metadata": {},
   "source": [
    "The reader model learns to solve the reading comprehension task (i.e., extract an answer for a given question from a given context document). Here we only discuss approaches for machine comprehension using neural networks.\n",
    "\n",
    "#### 2.2.1 - Bi-directional LSTM\n",
    "The reader model for answer detection of **DrQA** ([Chen et al., 2017](https://arxiv.org/abs/1704.00051); [code](https://github.com/facebookresearch/DrQA)) is a 3-layer bidirectional LSTM with hidden size of 128. Every relevant paragraph of retrieved Wikipedia articles is encoded by a sequence of feature vectors $\\{\\hat{\\mathbf{z}}_{1},\\dots,\\hat{\\mathbf{z}}_{m}\\}$. Each feature vector $\\hat{\\mathbf{z}}_{i} \\in \\mathbb{R}^{\\text{dim}_z}$ is expected to capture useful contextual information around one token $z_{i}$. The feature consists of several categories of features:\n",
    "\n",
    "1. **Word embeddings:** A 300d Glove word embedding trained from 800B Web Crawl data, f_{\\text{embed}} = E_{g}(z_{i}).\n",
    "\n",
    "2. **Exact match:** Wether a word appears in the question $x$, $f_{\\text{match}} = \\mathbb{I}(z_{i} \\in x)$\n",
    "\n",
    "3. **Token features:** This includes POS (part-of-speech) tagging, NER (named entity recognition) and TF (term-frequency), $f_{\\text{token}}(z_{i}) = \\text{POS}(z_{i}), \\text{NER}(z_{i}), \\text{TF}(z_{i})$.\n",
    "\n",
    "4. **Aligned question embedding:** The attention score $y_{ij}$ is designed to capture inter-sentence matching and similarity between the paragraph token $z_{i}$ and the question word $x_{j}$. This feature adds soft alignments between smilar but non-indentical words.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f_{\\text{align}}(z_{i}) &= \\sum_{j}y_{ij}E_{g}(x_{j}) \\\\\n",
    "y_{ij} = \\frac{\\text{exp}(\\alpha(E_{g}(z_{i}))^{\\top} \\alpha(E_{g}(x_{j})))}{\\sum_{j'}\\text{exp}(\\alpha(E_{g}(z_{i}))^{\\top} \\alpha(E_{g}(x_{j'})))}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a single dense layer with ReLU and $E_{g}(.)$ is the Glove word embedding.\n",
    "\n",
    "The feature vector of a paragraph of $m$ tokens is fed into LSTM to obtain the final paragraph vectors:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{z} = \\{\\mathbf{z}_{1}, \\dots, \\mathbf{z}_{m}\\} &= \\text{LSTM}(\\{\\hat{\\mathbf{z}}_{1}, \\dots, \\hat{\\mathbf{z}}_{m}\\}) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mathbf{z}}_{i} = \\{f_{\\text{embed}}, f_{\\text{match}}, f_{\\text{token}}, f_{\\text{align}}\\}$. The question is encoded as a weighted sum of the embeddings of every word in the question:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{j} b_{j} E(x_{j}) \\ b_{j} = \\text{softmax}(\\mathbf{w}^{\\top}E(x_{j}))\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}$ is a weight vector to learn.\n",
    "\n",
    "Once the feature vectors are constructed for the question and all the related paragraphs, the reader needs to predict the probabilities of each position in a paragraph to be the start and the end of an answer span, $p_{\\text{start}}(i_{s})$ and $p_{end}(i_{e})$, respectively. Across all the paragraphs, the optimal span is returned as the final answer with maximum $p_{\\text{start}}(i_{s}) \\times p_{end}(i_{e})$.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p_{\\text{start}}(i_{s}) &\\propto \\exp(\\mathbf{z}_{i_{s}} \\ \\mathbf{W}_{s} \\mathbf{x}) \\\\\n",
    "p_{end}(i_{e}) &\\propto \\exp(\\mathbf{z}_{i_{e} }\\mathbf{W}_{e} \\mathbf{x}) \\\\\n",
    "\\text{s.t.} \\ i_{s} \\leq i_{e} \\leq i_{s} + 15\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}_{s}$ and $\\mathbf{W}_{e}$ are learned parameters\n",
    "\n",
    "#### 2.2.2 - BERT uiverse\n",
    "\n",
    "Following the success of BERT, many QA models develop the machine comprehension component using BERT Let's define the BERT model as a function that can take one or multiple strings (concatenated by `[SEP]`) as input and outputs a set of BERT encoding vectors for the special `[CLS]` token and every input token:\n",
    "\n",
    "$$\n",
    "\\text{BERT}(s_{1}, s_{2}, \\dots) = [h^{[CLS]}, h^{(1)}, h^{(2)}, \\dots]\n",
    "$$\n",
    "\n",
    "where $h^{[CLS]}$ is the embeddin for the special `[CLS]` token and $h^{(i)}$ is the embedding vector for the i-th token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968c31e-34a0-40a6-9ab8-e26a890657c9",
   "metadata": {},
   "source": [
    "## 3 - Open-book QA: Retriever-generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ac26b-8607-48d1-9a87-e28931afb09f",
   "metadata": {},
   "source": [
    "## 4 - Open-book QA: Generative Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438d75d-d19d-4a66-bab9-5be005d169fe",
   "metadata": {},
   "source": [
    "Big language models have been pretrained on a large collection of unsupervised textual corpus. Given enough parameters, these models are able to memorize some factual knowledge within parameter weights. Therefore, we can use these models to do QA without explicit context, just like in a closed-book exam. The pre-trained language models produce *free text* to respond to questions, no explicit reading comprehension.\n",
    "\n",
    "[Roberts et al. (2020)](https://arxiv.org/pdf/2002.08910.pdf) measured the practical utility of a language model by fine-tuning a pre-trained model to answer questions without access to any external context or knowledge. They fine-tuned the T5 language model to answer questions without inputting any additional information or context. Such setupt enforces the language model to answer questions based on \"knowledge\" that it internalized during pre-training\n",
    "\n",
    "<img src=\"images_odqa/t5_qa.png\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\">\n",
    "\n",
    "The original T5 models were pre-trained on a multi-task mixture including an unsupervised masked language modeling (MLM) tasks on the C4 (Colossal Clean Crawled Corpus) dataset as well as fine-tuned altogether with supervised translation, summarization, classification, and reading comprehension tasks. [Roberts et al. (2020)](https://arxiv.org/pdf/2002.08910.pdf) took a pre-trained T5 model and continued pre-training with salient span masking over Wikipedia corpus, which has been found to substantially boost the performance for ODQA. Then they fine-tuned the model for each QA datasets independently.\n",
    "\n",
    "With a pre-trained T5 language model + continue pre-training with salient spans masking + fine-tuning for each QA dataset,\n",
    "\n",
    "* It can attain competitive results in open-domain question answering without access to external knowledge\n",
    "* A larger model can obtain better performance. For example, a T5 with 11B parameters is able to match the performance with dense page retriever with 3 BERT-base models, each with 330M parameters (**Note the difference in parameter size though, 11B vs 3 x 330M**)\n",
    "\n",
    "Interestingly, fine-tuning is not strictly necessary. GPT-3 (Brown et al., 2020) has been evaluated on the closed book question answering task without any gradient updates or fine-tuning. During evaluation, the few-shot, one-shot and zero-shot settings here only refer to how many demonstrations are provided as context in the text input:\n",
    "\n",
    "1. \"few-shot learning\": GPT3 is allowed to take as many demonstrations as what can fit into the model's context window (typically 10 to 100).\n",
    "2. \"one-shot learning\": only one demonstration is provided.\n",
    "3. \"zero-shot learning\": no demonstrations are allowed and only an instruction in natural language is given to the model.\n",
    "\n",
    "The performance grows with the model size. On the TriviaQA dataset, GPT3 evaluation with demonstrations can match or exceed the performance of SOTA baseline with fine-tuning.\n",
    "\n",
    "<img src=\"images_odqa/gpt3_performance_qa.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87286a1e-aa7f-4c62-8b12-421f9af51987",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5 - Related techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8373d80-68e9-40db-b62e-301a8ae37f8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1 - Fast maximum inner product search (MIPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74e537-0874-4e76-a5cc-1d0c5532ce28",
   "metadata": {},
   "source": [
    "MIPS (maximum inner product search) is a crucial component in many open-domain question answering models. In retriever + reader/generator framework, a large number of passages from the knowledge source are encoded and stored in a memory. A retrieval model is able to query the memory to identify the top relevant passages which have the maximum inner product with the question's embedding.\n",
    "\n",
    "We need fast MIPS because the number of precomputed passage representations can be gigantic. There are several ways to achieve fast MIPS ar run time\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** the first two summaries were provided by [Chat-gpt](https://chat.openai.com/chat)\n",
    "\n",
    "----\n",
    "\n",
    "#### 5.1.1 - Asymmetric Locality Sensitive Hashing (ALSH)\n",
    "\n",
    "[Asymmetric Locality Sensitive Hashing (ALSH)](https://proceedings.neurips.cc/paper/2014/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf) is a variant of Locality Sensitive Hashing (LSH) that is used for approximate nearest neighbor search in high-dimensional spaces. It is called \"asymmetric\" because it uses different hash functions for the query point and the points in the database, which can improve the accuracy of the search compared to traditional LSH.\n",
    "\n",
    "LSH is a technique for efficiently finding approximate nearest neighbors in large datasets by using hash functions to map data points to a lower-dimensional space. This allows the search to be performed in the lower-dimensional space, which is typically faster than searching in the original high-dimensional space. However, the hash functions used in LSH are designed to preserve the relative distances between points, so they are not necessarily optimal for finding the nearest neighbors.\n",
    "\n",
    "ALSH addresses this issue by using different hash functions for the query point and the points in the database. This allows the search to be more focused on finding the nearest neighbors, rather than just preserving the relative distances between points.\n",
    "\n",
    "ALSH is often used in applications such as information retrieval, recommendation systems, and image retrieval, where it is important to quickly find the nearest neighbors to a given query point. It is also used in machine learning algorithms that rely on nearest neighbor search, such as k-means clustering and kernel methods.\n",
    "\n",
    "#### 5.1.2 - Data-dependent hashing\n",
    "\n",
    "[Data-dependent hashing](https://arxiv.org/abs/1501.01062) is a method for constructing hash functions that are tailored to the specific characteristics of the data being hashed. It is a type of Locality Sensitive Hashing (LSH) that is used to efficiently find approximate nearest neighbors in large datasets.\n",
    "\n",
    "In traditional LSH, the hash functions are designed to preserve the relative distances between points in the dataset. This means that the hash functions are chosen independently of the data and are typically chosen to have certain desirable properties, such as being easy to compute or having a low collision rate.\n",
    "\n",
    "In contrast, data-dependent hashing constructs the hash functions based on the characteristics of the data itself. This can be done by using techniques such as neural networks or decision trees to learn the hash functions from the data. Data-dependent hashing can be more effective than traditional LSH for finding nearest neighbors because the hash functions are specifically designed to capture the structure of the data.\n",
    "\n",
    "Data-dependent hashing is often used in applications such as information retrieval, recommendation systems, and image retrieval, where it is important to quickly find the nearest neighbors to a given query point. It is also used in machine learning algorithms that rely on nearest neighbor search, such as k-means clustering and kernel methods.\n",
    "\n",
    "#### 5.1.3 - FAISS\n",
    "\n",
    "[FAISS](https://github.com/facebookresearch/faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed primarily at Meta's Fundamental AI Research group.\n",
    "\n",
    "FAISS contains several methods for similarity search. It assumes that the instances are represented as vectors and are identified by an integer, and that the vectors can be compared with L2 (Euclidean) distances or dot products. Vectors that are similar to a query vector are those that have the lowest L2 distance or the highest dot product with the query vector. It also supports cosine similarity, since this is a dot product on normalized vectors.\n",
    "\n",
    "Some of the methods, like those based on binary vectors and compact quantization codes, solely use a compressed representation of the vectors and do not require to keep the original vectors. This generally comes at the cost of a less precise search but these methods can scale to billions of vectors in main memory on a single server. Other methods, like HNSW and NSG add an indexing structure on top of the raw vectors to make searching more efficient.\n",
    "\n",
    "The GPU implementation can accept input from either CPU or GPU memory. On a server with GPUs, the GPU indexes can be used a drop-in replacement for the CPU indexes (e.g., replace IndexFlatL2 with GpuIndexFlatL2) and copies to/from GPU memory are handled automatically. Results will be faster however if both input and output remain resident on the GPU. Both single and multi-GPU usage is supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d863bb-6ca8-4c4d-b01c-89476097b9f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.2 - Language model pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f2ad3e-caed-4684-9479-ae65703a9268",
   "metadata": {},
   "source": [
    "Two pre-training tasks are especially helpful for QA tasks, as we have discussed above\n",
    "\n",
    "\n",
    "#### 5.2.2 - Inverse cloze task\n",
    "\n",
    "The goal of [Cloze Task](https://en.wikipedia.org/wiki/Cloze_test) is to predict masked-out text based on its context. The prediction of Inverse Cloze Task ([Lee et al., 2019](https://arxiv.org/abs/1906.00300)) is in the reverse direction, aiming to predict the context given a sentence. For example, given the context \"The cat sat on the couch\" and the prompt \"What did the cat do?\", the task would be to generate the missing text \"sat on the couch\". **In the context of QA tasks, a random sentence can be treated as a pseudo-question, and its context can be treated as pseudo-evidence**.\n",
    "\n",
    "#### 5.2.2 - Salient span making\n",
    "\n",
    "Salient spans masking ([Guu et al., 2020](https://arxiv.org/abs/2002.08909)) is a special case for MLM task in language model training. First, we find *salient spans* by using a tagger to identify named entities and a regular expression to identify dates. Then one of the detected salient spans is selected and masked. The task is to predict this masked salient span."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c6f55-ec74-4d48-a2ba-f048a093fc2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6 - Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5245f56-06ce-41fd-9d8b-df0f52f630bb",
   "metadata": {},
   "source": [
    "|        Model       |            Retriever           |          Reader / Generator         |            Pre-training / Fine-tuning            | End2end |\n",
    "|:------------------:|:------------------------------:|:-----------------------------------:|:------------------------------------------------:|:-------:|\n",
    "| DrQA               | TF-IDF                         | Bi-directional LSTM                 | –                                                | No      |\n",
    "| BERTserini         | Aserini + BM25                 | BERT without softmax layer          | Fine-tune with SQuAD                             | No      |\n",
    "| Multi-passage BERT | ElasticSearch + BM25           | Multi-passage BERT + Passage ranker |                                                  | No      |\n",
    "| R^3                | Classic IR + Match-LSTM        | Match-LSTM                          |                                                  | Yes     |\n",
    "| ORQA               | Dot product of BERT embeddings | BERT-RC                             | Inverse cloze task                               | Yes     |\n",
    "| REALM              | Dot product of BERT embeddings | BERT-RC                             | Salient span masking                             | Yes     |\n",
    "| DPR                | Dot product of BERT embeddings | BERT-RC                             | supervised training with QA pairs                | Yes     |\n",
    "| DenSPI             | Classic + Neural IR            | –                                   |                                                  | Yes     |\n",
    "| T5 + SSM           | –                              | T5                                  | SSM on CommonCrawl data + Fine-tuning on QA data | Yes     |\n",
    "| GPT3               | –                              | GPT3                                | NSP on CommonCrawl data                          | Yes     |\n",
    "| RAG                | DPR retriever                  | BART                                |                                                  | Yes     |\n",
    "| Fusion-in-Decoder  | BM25 / DPR retriever           | Tranformer                          |                                                  | No      |\n",
    "\n",
    "\n",
    "<img src=\"images_odqa/summary.png\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
