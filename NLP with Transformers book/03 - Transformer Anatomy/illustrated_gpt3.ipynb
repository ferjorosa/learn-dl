{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ffeeb2-93ca-4e7d-979c-3eaad72bcb04",
   "metadata": {},
   "source": [
    "# The illustrated GPT-3\n",
    "\n",
    "* <a href=\"https://jalammar.github.io/how-gpt3-works-visualizations-animations/\"><b>Original article</b></a>\n",
    "* <a href=\"https://arxiv.org/pdf/2005.14165.pdf\"><b>Brown et al., (2020)</b></a>\n",
    "\n",
    "\n",
    "The purpose of this article is to showcase some of the advantages that the MASSIVE increase in size from GPT-2 has produced. To better understand the model size increase:\n",
    "* It contains 175 billion parameters\n",
    "* Model dimensionality was increased to 2048 \n",
    "* It contains 96 transformer-decoder blocks\n",
    "\n",
    "<img title=\"\" src=\"images_igpt3/05-gpt3-generate-output-context-window.gif\" alt=\"\" width=\"700\" data-align=\"center\">\n",
    "\n",
    "In essence, GPT-3 is GPT-2 but bigger and with more data to train. [The difference with GPT-2 is the alternation of dense and sparse self-attention layers](https://arxiv.org/pdf/1904.10509.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
