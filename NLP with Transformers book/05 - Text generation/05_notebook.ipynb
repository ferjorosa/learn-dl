{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce7e23c-42e2-478a-92f1-9fb9665b60e6",
   "metadata": {},
   "source": [
    "# 5 - Text generation\n",
    "\n",
    "One of the most advertised features of transformer-based language models is their ability to generate text that is almost indistinguishable from human-written text. A famous example is OpenAi's GPT-2, which when given the prompt:\n",
    "\n",
    "```\n",
    "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
    "```\n",
    "\n",
    "was able to generate a compelling news article about talking unicorns:\n",
    "\n",
    "```\n",
    "The scientist named the population, after their distinctive horn, Ovid‚Äôs Unicorn. These four-horned, silver-white unicorns were previously unknown to science. Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved. Dr. Jorge P√©rez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. P√©rez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow. P√©rez and the others then ventured further into the valley. ‚ÄúBy the time we reached the top of one peak, the water looked blue, with some crystals on top,‚Äù said P√©rez. P√©rez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them‚Äîthey were so close they could touch their horns. While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English ‚Ä¶\n",
    "```\n",
    "\n",
    "What makes this example so remarkable is that it was generated without any explicit supervision! By simply learning to predict the next word in the text of millions of web pages, GPT-2 and its more powerful descendants like GPT-3 are able to acquire a broad set of skills and pattern recognition abilities that can be activated with different kinds of input prompts. \n",
    "\n",
    "The following image shows how language models are sometimes exposed during pretraining to sequences of tasks where they need to predict the following tokens based on the context alone. Some of this tasks include: arithmetics, translation, fixing misspellings, etc\n",
    "\n",
    "<img src=\"images/tasks_examples.png\" title=\"\" alt=\"\" width=\"700\" data-align=\"center\">\n",
    "\n",
    "The ability of transformers to generate realistic text has led to a diverse range of applications, auto-completition features like [Write With Transformer](https://transformer.huggingface.co/), text-based games such as [AI dungeon](https://play.aidungeon.io/), and conversational agents like [Google's Meena](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html)\n",
    "\n",
    "In this chapter, we'll use GPT-2 to illustrate how text generation works for language models and explore how different decoding strategies impact the generated texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec087d0b-7b71-41f9-90be-7d8ebe8e5420",
   "metadata": {},
   "source": [
    "## 5.1 - The challenge with generating coherent text\n",
    "\n",
    "Up to this point, we have focused on tackling NLP tasks via a combination of pretraining and supervised fine-tuning. As we have seen, for task-specific heads like sequence or token classification, generating predictions is fairly straightforward; the model produces some logits and we take the maximum value to get the predicted class. By contrast, converting the model's probabilistic output to text requires a **decoding method**, which introduces a few challenges that are unique to text generation:\n",
    "\n",
    "* The decoding is done **iteratively** and thus involves significantly more compute than simply passing inputs once through the forwards pass of a model.\n",
    "* The **quality** and **diversity** of the generated text depend on the choice of decoding method and associated hyperparameters.\n",
    "\n",
    "To understand how this decoding process works, let's start by examining how GPT-2 is pretrained and subsequently applied to generate text.\n",
    "\n",
    "Like other autoregressive or causal language models, GPT-2 is pretrained to estimate the probability $P(y|x)$ of a sequence of tokens $\\mathbf{y} = y_{1}, \\dots, y_{t}$, ocurring in the text, given some initial prompt or context sequence $\\mathbf{x} = x_{1}, \\dots, x_{k}$. Since it is impractical to adcquire enough training data to estimate $P(\\mathbf{y}|\\mathbf{x})$ directly, it is common to use the chain rule of probability to factorize it as a product of conditional probabilities:\n",
    "\n",
    "$$\n",
    "P(y_{1}, \\dots, y_{t}| \\mathbf{x}) = \\prod_{t=1}^{N} P(y_{t}|y_{1}, \\dots, y_{t-1}, \\mathbf{x})\n",
    "$$\n",
    "\n",
    "It is from these conditional probabilities that we pick up the intuition that autoregressive language modeling amounts to predicting each word given the preceding words in a sentence; this is exactly what the probability on the righthand side of the preceding equation describes. <span style=\"color:blue\">Notice that this pretraining objective is quite different from BERT's, which utilizes both <b>past</b> and <b>future</b> contexts to predict a *masked* token.</span>\n",
    "\n",
    "<img src=\"images/text_generation_example.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
    "\n",
    "As shown by the figure, we start with a prompt like \"Transformers are the\" and use the model to predict the next token. Once we have determined the next token, we append it to the prompt and then use the new input sequence to generate another token. We do this until we have reached a special end-of-sequence token or a predefined maximum length.\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** Since the output sequence is *conditioned* on the choice of input prompt, this type of text generation is often called <span style=\"color:blue\">conditional text generation</span>.\n",
    "\n",
    "----\n",
    "\n",
    "At the heart of this process lies a decoding method that determines which token is selected at each timestep. Since the language model head produces a logit $z_{t,i}$ per token in the vocabulary at each step, we can get the probability distribution over the next possible token $w_{i}$ by taking the softmax:\n",
    "\n",
    "$$\n",
    "P(y_{t} = w_{i} | y_{1}, \\dots, y_{t-1}, \\mathbf{x}) = \\text{softmax}(z_{t,i})\n",
    "$$\n",
    "\n",
    "The goal of most decoding methods is to search for the most likely overall sequence by picking a $\\hat{\\mathbf{y}}$ such that:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\underset{\\mathbf{y}}{argmax} \\ P(\\mathbf{y} | \\mathbf{x})\n",
    "$$\n",
    "\n",
    "<span style=\"color:blue\">Finding</span> $\\hat{\\mathbf{y}}$ <span style=\"color:blue\">directly would involve <b>evaluating every possible sequence</b> with the language model. Since there does not exist an algorithm that can do this in a reasonable amount of time, <b>we rely on approximations instead</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6d45d-8a3e-4015-88e7-9676950fe56a",
   "metadata": {},
   "source": [
    "## 5.2 - Decoding methods\n",
    "\n",
    "[**Great article from hugginface on decoding methods**](https://huggingface.co/blog/how-to-generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68044c6e-adf2-4cac-96ee-70ef06e6daea",
   "metadata": {},
   "source": [
    "### 5.2.1 - Greedy search decoding\n",
    "\n",
    "<img src=\"images/greedy_search_tree.png\" title=\"\" alt=\"\" width=\"400\" data-align=\"center\">\n",
    "\n",
    "This is the simplest decoding method to get discrete tokens from a model's continuous output is to greedily select the token with the highest probability at each timestep:\n",
    "\n",
    "$$\n",
    "\\hat{y_{t}} = \\underset{y_{t}}{argmax} \\ P(y_{t} | y_{1}, \\dots, y_{t-1}, \\mathbf{x})\n",
    "$$\n",
    "\n",
    "To see how greedy search works, let's start by loading the GPT-2 model with a language modeling head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23b3eb1-5754-44f2-b782-5a41ee269875",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/transformers/v2.2.0/pretrained_models.html\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/transformers/v2.2.0/pretrained_models.html\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2\" # 117M parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff9fea-b51a-4f1a-8e6a-8191a98426a9",
   "metadata": {},
   "source": [
    "Now, let's generate some text! Although ü§ó Transformers provides a `generate()` function for autoregressive models like GPT-2, we'll implement this decoding method ourselves to see what goes under the hood. To warm up, we'll use \"Transformers are the\" as the input prompt and run the decoding for eight timesteps. At each timestep, we pick out the model's logits for the last token in the prompt and wrap them with a softmax to get a probability distribution. We then pick the next token with the highest probability, add it to the input sequence, and run the process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d9074-a4be-42f7-a10b-246e4b243d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        \n",
    "        # Select logits of the first batch and the last token and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        \n",
    "        # Store tokens with highest probabilities\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "            \n",
    "        # Append predicted next token to input\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "\n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4fb37-0b7f-46fb-bee1-e2973565d079",
   "metadata": {},
   "source": [
    "With this simple method we were able to generate the sentence `Transformers are the most popular toy line in the world`. Interestingly, this indicates that GPT-2 has internalized some knowledge about the Transformers media franchise, which was created by two toy companies (Hasbro and Takara Tomy).\n",
    "\n",
    "Unlike other tasks such as sequence classification where a single forward pass suffices to generate predictions, with text generation we need to decode the output tokens one at a time.\n",
    "\n",
    "While implementing greedy search was not too hard, it would be better to use the built-in `generate()` function from ü§ó Transformers to explore more sophisticated decoding methods. To reproduce our example, let's make sure sampling is switched off (it‚Äôs off by default, unless the specific configuration of the model you are loading the checkpoint from states otherwise) and specify the `max_new_tokens` for the number of newly generated tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e8989-672a-4db2-bd38-e0e4dc4f1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c8b1d5-0292-4454-98bc-586eafb1bee1",
   "metadata": {},
   "source": [
    "Now, let's try something a bit more interesting: can we reproduce the unicorn story from OpenAI? As we did previously, we'll encode the prompt with the tokenizer, and we'll specify a larger value for `max_length` to generate a longer sequence of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65f2c6-d350-488c-bade-aa6f756b400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f85981b-c693-48a2-b869-1f069e5abe9b",
   "metadata": {},
   "source": [
    "From the resulting text we can see that one of the main drawbacks with greedy search decoding is that it tends to produce repetitive output sequences, which is certainly undesirable in a news article. This is a common problem with greedy search algorithms, which can fail to give you the optimal solution; in the context of decoding, <span style=\"color:blue\">they can miss word sequences whose overall probability is higher <b>just because words happen to be preceded by low probability ones.</b></span>\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** Although greedy search decoding is rarely used for text generation that require diversity (see this interesting video on that matter), <span style=\"color:blue\">it can be useful for producing short sequences like arithmetic where a determinisit and factually correct output is preferred.</span>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896984e-b82c-496f-8d51-e0647c8a6fca",
   "metadata": {},
   "source": [
    "### 5.2.2 - Beam search decoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
