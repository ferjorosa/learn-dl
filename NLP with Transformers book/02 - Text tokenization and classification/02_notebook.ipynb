{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027f18d9-b835-415a-b33b-5b050659127d",
   "metadata": {},
   "source": [
    "# 2 - TEXT TOKENIZATION AND CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccd29a-c76f-4f6e-a61f-b1d3b5e0e7ee",
   "metadata": {},
   "source": [
    "## 2.2 - From text to tokens\n",
    "\n",
    "Transformer models like DistilBERT cannot receive raw strings as input; instead, they assume the text has been *tokenized* and *encoded* as numerical vectors. Tokenization is the step of breaking down a string into the atomic units used in the model. There are several tokenization strategies one can adopt, and the optimal splitting of words into subunits is usually learned from the corpus. Before looking at the tokenizer used for DistilBERT, let's consider two extreme cases: *character* and *word* tokenization.\n",
    "\n",
    "### 2.2.1 - Character tokenization\n",
    "\n",
    "The simplest tokenization scheme is to feed each character individually to the model. In Python, `str` objects are really arrays under the hood, which allows us to quickly implement character-level tokenization with just one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664f2b16-4aa9-4616-955c-205aaec6aaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f52fd-fbc2-4ec7-a0f7-df3f58edf1c7",
   "metadata": {},
   "source": [
    "This is a good start, but we are not done yet. Our model expects each character to be converted to an integer, a process sometimes called *numericalization*. One simple way to do this is by encoding each unique token (which are characters in this case) with a unique integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "444d6d2a-6b8e-4d4d-a9a0-cfd9773a06df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cedd473-6203-47ed-a4e6-671bbee2ede6",
   "metadata": {},
   "source": [
    "This gives us a mapping from each character in our vocabulary to a unique integer. We can now use `token2idx` to transform the tokenized text to a list of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb579f9f-e21b-4846-aa9a-e5597708b00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8972f5-1ab3-4bb0-b754-01e6cb8ad251",
   "metadata": {},
   "source": [
    "Each token has now been mapped to a unique numerical identifier (hence the name `input_ids`). The last step is to convert `input_ids` to a 2D tensor of one-hot vectors. One-hot vectors are frequently used in machine learning to encode categorical data, which can be either ordinal or nominal. \n",
    "\n",
    "The problem with character-level tokenization ignores any structure in the text and treats the whole string as a stream of characters. Although this helps deal with misspellings and rare words, **the main drawback is that linguistic structures such as words need to be learned from the data**. This requires significant\n",
    "compute, memory, and data. For this reason, character tokenization is rarely used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd2398-02b0-4a44-a54b-5fad607359a9",
   "metadata": {},
   "source": [
    "### 2.2.2 - Word tokenization\n",
    "\n",
    "Instead of splitting the text into characters, we can split it into words and map each word to an integer. Using words from the outset enables the model to skip the step of learning words from characters, and thereby reduces the complexity of the training process.\n",
    "\n",
    "One simple class of word tokenizers uses whitespace to tokenize the text. We can do this by applying Python's `split()` function directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc527372-5af7-4722-a13e-6d25aa94c48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be49df0-9c45-49df-b684-a75de87cb3b1",
   "metadata": {},
   "source": [
    "From here we can take the same steps we took for the character tokenizer to map each word to an ID. However, we can already see one potential problem with this tokenization scheme: punctuation is not accounted for, so `NLP.` is treated as a single token. Given that words can include declinations, conjugations, or misspellings, the size of the vocabulary can easily grow into the millions!\n",
    "\n",
    "----\n",
    "\n",
    "<mark><b>Note:</mark> Some word tokenizers have extra rules for punctuation. One can also apply stemming or lemmatization, which normalizes words to their stem (e.g., \"great\", \"greater\", and \"greatest\" all become \"great\"), at the expense of losing some information in the text.\n",
    "    \n",
    "----\n",
    "    \n",
    "<span style=\"color:blue\"><b>Having a large vocabulary is a problem because it requires neural networks to have an enormous number of parameters</span>. To illustrate this, suppose we have 1 million unique words and want to compress the 1-million-dimensional input vectors to 1-thousanddimensional vectors in the first layer of our neural network. This is a standard step in most NLP architectures, and **the resulting weight matrix of this first layer would contain 1 million Ã— 1 thousand = 1 billion weights**. This is already comparable to the largest GPT-2 model, which has around 1.5 billion parameters in total!\n",
    "    \n",
    "Naturally, we want to avoid being so wasteful with our model parameters since models are expensive to train, and larger models are more difficult to maintain. <span style=\"color:blue\"><b>A common approach is to limit the vocabulary and discard rare words</span> by considering, say, the 100,000 most common words in the corpus. Words that are not part of the vocabulary are classified as \"unknown\" and mapped to a shared `UNK` token. This means that we lose some potentially important information in the process of word tokenization, since the model has no information about words associated with `UNK`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03f7b0-b362-4dd2-9370-58c07776d965",
   "metadata": {},
   "source": [
    "### 2.2.3 - Subword tokenization\n",
    "\n",
    "The basic idea behind subword tokenization is to combine the best aspects of character and word tokenization. On the one hand, we want to split rare words into smaller units to allow the model to deal with complex words and misspellings. On the other hand, we want to keep frequent words as unique entities so that we can keep the length of our inputs to a manageable size. The main distinguishing feature of subword tokenization (as well as word tokenization) is that it is learned from the pretraining corpus using a mix of statistical rules and algorithms.\n",
    "\n",
    "There are several subword tokenization algorithms that are commonly used in NLP, but let's start with WordPiece, which is used by **BERT** and **DistilBERT** tokenizers. To understand it, the best way is to see it in action!\n",
    "\n",
    "Thankfully, ðŸ¤— Transformers provides a convenient AutoTokenizer class that allows you to quickly load\n",
    "the tokenizer associated with a pretrained model by providing the ID of the model in the Hub or a local file path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bbcb1a3-dde8-41bc-813c-12012c15298f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c60c5cd704a4986811e4771cffb36e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e288640ccf6c46fcaa3e5626da79f0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4395c293075437e8c007cf1ae1c569f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7007fccfc024aae9b4e097d641f5314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bed0f4-e45b-4cd9-97ba-6d1bb8e29a01",
   "metadata": {},
   "source": [
    "The `AutoTokenizer` class belongs to a larger set of â€œautoâ€ classes whose job is to automatically retrieve the modelâ€™s configuration, pretrained weights, or vocabulary from the name of the checkpoint. This allows you to quickly switch between models, but if you wish to load the specific class manually you can do so as well. For example, we could have loaded the **DistilBERT** tokenizer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ffa236-b292-4f3e-8b49-742c0cce671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f600895-272d-4ebb-9410-41dab41c641e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<mark><b>Note:</mark> When you run the `AutoTokenizer.from_pretrained()` method for the first time you will see a progress bar that shows which parameters of the pretrained tokenizer are loaded from the Hugging\n",
    "Face Hub. When you run the code a second time, it will load the tokenizer from the cache, usually at `~/.cache/huggingface`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4b0cee1-55c6-464c-9eaa-c6209209a584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 26129, 2015, 5699, 1011, 14894, 6963, 2260, 4609, 2595, 17788, 2290, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'kellogg', '##s', 'sugar', '-', 'flavor', 'bars', '12', '##un', '##x', '##25', '##g', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text_2 = \"Kelloggs sugar-flavor bars 12unx25g\"\n",
    "\n",
    "encoded_text_2 = tokenizer(text_2)\n",
    "print(encoded_text_2)\n",
    "\n",
    "tokens_text_2 = tokenizer.convert_ids_to_tokens(encoded_text_2.input_ids)\n",
    "print(tokens_text_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
