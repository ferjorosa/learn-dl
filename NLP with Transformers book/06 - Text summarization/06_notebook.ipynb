{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f13b3d6-cf2e-4f6f-9789-0722bef152ac",
   "metadata": {},
   "source": [
    "# 6 - Summarization\n",
    "\n",
    "If you think about it, text summarization requires a range of abilities, such as understanding long passages, reasoning about the contents, and producing fluent text that incorporates the main topics from the original document. Moreover, accurately summarizing a news article is very different from summarizing a legal contract, so being able to do so requires a sophisticated degree of domain generalization. For these reasons, text summarization is a difficult task for neural language models, including transformers.\n",
    "\n",
    "Despite these challenges, text summarization offers the prospect for domain experts to significantly speed up their workflows and is used by enterprises to condense internal knowledge, summarize contracts, automatically generate content for social media releases, and more.\n",
    "\n",
    "In this chapter we will build our own encoder-decoder model to condense dialogues between several people into a crisp summary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0367129-11a5-4cd5-abf3-ebf41b313a0d",
   "metadata": {},
   "source": [
    "## 6.1 - The CNN/DailyMail Dataset\n",
    "\n",
    "Before we dive into the summarization process, let's begin by taking a look at one of the canonical datasets for summarization: the CNN/DailyMail corpus. This dataset consists of around 300000 pairs of news articles and their corresponding summaries, composed from the bullet points that CNN and the DailyMail attach to their articles. \n",
    "\n",
    "<span style=\"color:blue\">An important aspect of the dataset is that the summaries are <b>abstractive</b> and not <b>extractive</b>, which means that they consist of new sentences instead of simple excerpts.</span> [**The dataset is available on the Hub**](https://huggingface.co/datasets/cnn_dailymail); we'll use version 3.0.0, which is a nonanonymized version set up for summarization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eaaf72-a30d-483a-a59a-40d2167ecec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")\n",
    "print(f\"Features: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173ebce-f3bb-495b-aa5b-2b133b9b5327",
   "metadata": {},
   "source": [
    "The dataset has three columns: `article`, which contains the news articles, `highlights` with the summarise, and `id` to uniquely identify each article. Let's look at an excerpt from an article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4992c3-a695-40b5-889b-0bbdaa0c20c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e38ce83-a7c6-4de8-b4e4-591ba8273e7d",
   "metadata": {},
   "source": [
    "We see that the articles can be very long compared to the target summary; in this particular case the difference is 17-fold. Long articles pose a challenge to most transformer models since the context size is usually limited to 1000 tokens or so, which is equivalent to a few paragraphs of text. The standard, yet crude way to deal with this for summarization is to simply truncate the texts beyond the model's context size. Obviously there could be important information for the summary toward the end of the text, but for now we need to live with this limitation of the model architectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d3bf1-e8f9-48e9-9003-ffbe597d39d2",
   "metadata": {},
   "source": [
    "## 6.2 - Text summarization pipelines\n",
    "\n",
    "Let's see how a few of the most popular transformer models for summarization perform by first looking qualitatively at the outputs for the preceding example. Although the model architectures we will be exploring have varying maximum input sizes, let's restrict the input text to 2000 characters to have the same input for all models and thus make the outputs more comparable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2156ce1-c59c-4a3f-90be-571ba0901acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "# We'll collect the generated summaries of each model in a dictionary\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d2de4-c8df-4887-86bc-509a50c7ca83",
   "metadata": {},
   "source": [
    "A convention in summarization is to separate the summary sentences by a newline. We could add a newline token after each full stop, but this simple heuristic would fail for strings like \"U.S\" or \"U.N\". The Natural language toolkit (NLTK) package includes a more sophisticated algorithm that can differentiate the end of a sentence from punctuation that occurs in abbrevisations:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9edfaf-19af-49f5-95fc-337274be10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "string = \"The U.S. are a country. The U.N. is an organization.\"\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc0ee2-f5b5-434e-9bad-a54bfef60c5f",
   "metadata": {},
   "source": [
    "## 6.3 - Summarization baseline\n",
    "\n",
    "A common baseline for summarizing news articles is to simply take the first three sentences of the article. With NLTK's sentence tokenizer, we can easily implement such a baseline:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70891563-f54d-46b7-8ecb-cfb09f8ea1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b087d8-66b7-4b6a-9904-5dccc3f06a5c",
   "metadata": {},
   "source": [
    "### 6.3.1 - GPT-2\n",
    "\n",
    "We have already seen in Chapter 5 how GPT-2 ([Radford et al., 2019](https://openai.com/blog/better-language-models/)) can generate text given some prompt. One of the model's surprising features is that we can also use it to generate summaries by simply appending \"TL;DR\" at the end of the input text. This expression is often used on platforms like Reddit to indicate a short version of a long post. \n",
    "\n",
    "We will start our summarization experiment by re-creating the procedure of the original paper with the `pipeline()` function from ðŸ¤— Transformers. We create a text generation pipeline and load the GPT-2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47185575-8333-444c-88e5-1f1228392a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\", model=\"gpt2\") # 117M parameters\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "summaries[\"gpt2\"] = \"\\n\".join(\n",
    "sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08eb76-4dd5-4e9f-b83c-e269a7f97edf",
   "metadata": {},
   "source": [
    "Here we just store the summaries of the generated text by slicing off the input query and keep the result in a Python dictionary for later comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5755de-826c-4681-9f15-df3e15c99463",
   "metadata": {},
   "source": [
    "### 6.3.2 - T5\n",
    "\n",
    "Next let's try the T5 transformer ([Raffel et al., 2019](https://arxiv.org/abs/1910.10683)). As we saw in Chpater 3, the developers of this model performed a comprehensive study of transfer learning in NLP and found they could create a universal transformer architecture by formulating all tasks as text-to-text tasks. The T5 checkpoints are trained on a mixture of unsupervised data (to reconstruct masked words) and supervised data for several tasks, including summarization. These checkpoints can thus be directly used to perform summarization without fine-tuning by using the same prompts used during pretraining. In this framework, the input format for the model to summarize a document is `\"summarize:\" <ARTICLE>`, and for translation it looks like `\"translate English to German:\" <TEXT>`. This maskes T5 extremely versatile and allows us to solve many tasks with a single model.\n",
    "\n",
    "We can directly load T5 for summarization with the `pipeline()` function, which also takes care of formatting the inputs in the text-to-text format so we don't need to prepend them with `\"summarize:\"`\n",
    "\n",
    "<img src=\"images/t5_examples.png\" title=\"\" alt=\"\" width=\"700\" data-align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c92bf-c580-4f41-bbcb-87b3804f4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"t5-small\") # We could also try t5-base, with 220M parameters\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ca9f4-bdcb-4915-baae-891b23a88533",
   "metadata": {},
   "source": [
    "### 6.3.3 - BART\n",
    "\n",
    "BART ([Lewis et al., 2019](https://arxiv.org/abs/1910.13461)) also uses an encoder-decoder architecture and is trained to reconstruct corrupted inputs. It combines the pretraining schemes of BERT and GPT-2. We'll use the `facebook/bart-base-cnn` checkpoint, which has been specifically fine-tuned on the CNN/DailyMail dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d245c-19a0-43f3-9829-01b065e48514",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-base-cnn\") # 140M parameters\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024ec1c-02ea-4ad4-bd06-15cc6cff8c36",
   "metadata": {},
   "source": [
    "### 6.3.4 - PEGASUS\n",
    "\n",
    "PEGASUS ([Zhang et al., 2019](https://arxiv.org/abs/1912.08777)) is also an encoder-decoder transformer. Its pretraining objective is to is to predict masked sentences in multisentence texts. The authors argue that the closer the pretraining objective is to the downstream task, the more effective it is. With the aim of finding a pretraining objective that is closer to summarization than general language modeling, they automatically identified, in a very large corpus, sentences containing most of the content of their surrounding paragraphs (using summarization evaluation metrics as a heuristic for content overlap) and pretrained the PEGASUS model to reconstruct these sentences, thereby obtaining a state-of-the-art model for text summarization.\n",
    "\n",
    "<img src=\"images/pegasus_architecture.png\" title=\"\" alt=\"\" width=\"700\" data-align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c67bf76-d548-4799-8deb-700f7be4cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note on size: Pegasus is 568M parameters, which is considerably larger than BART-base and T5-base\n",
    "pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3539fa-e669-42e9-bc27-79d9734f3b23",
   "metadata": {},
   "source": [
    "## 6.4 - Comparing different summaries\n",
    "\n",
    "Now that we have generated summaries with four different models, let's compare the results. Keep in mind that one model has not been trained on the dataset at all (GPT-2), one model has been fine-tuned on this task among others (T5), and two models have exclusively been fine-tuned on this task (BART and PEGASUS). Let's have a look at the summaries these models have generated:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488f0ea-f68e-4e9d-b7cb-efe9b88930e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GROUND TRUTH\")\n",
    "print(dataset[\"train\"][1][\"highlights\"])\n",
    "print(\"\")\n",
    "for model_name in summaries:\n",
    "print(model_name.upper())\n",
    "print(summaries[model_name])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0758fc6-b7b9-4dd6-b4ac-dfa360d7cc51",
   "metadata": {},
   "source": [
    "## 6.5 - Measuring the quality of generated text\n",
    "\n",
    "Good evaluation metrics are important, since we use them to measure the performance of models not only when we train them but also later, in production. If we have bad metrics we might be blind to model degradation, and if they are misaligned with the business goals we might not create any value.\n",
    "\n",
    "Measuring performance on a text generation task is not as easy with standard classification tasks such as sentiment analysis or named entity recognition. Take the example of translation; given a sentence like \"I love dogs!\" in English and translating it to Spanish there can be multiple valid possibilities, like \"Â¡Me encantan los perros!\" or \"Â¡Me gustan los perros!\"\n",
    "\n",
    "<span style=\"color:blue\">Simply checking for an exact match to a reference translation is not optimal; even humans would fare badly on such a metric because we all write text slightly differently from each other (and even from ourselves, depending on the time of the day or year!). Fortunately, there are alternatives. </span>\n",
    "\n",
    "Two of the most common metrics used to evaluate generated text are **BLEU** and **ROUGE**. Let's take a look at how they are defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f3fb1-8c39-4afb-96f2-27df2d7ef663",
   "metadata": {},
   "source": [
    "### 6.5.1 - BLEU\n",
    "\n",
    "The idea of BLEU ([Papineni et al., 2002](https://dl.acm.org/doi/10.3115/1073083.1073135)) is simple: instead of looking at how many of the tokens in the generated texts are perfectly aligned with the reference text tokens, we look at words or $n$-grams. BLEU is a precision-based metric, which means that when we compare two texts we count the number of words in the generation that occur in the reference and divide it by the length of the reference.\n",
    "\n",
    "However, there is an issue with this vanilla precision. Assume the generated text just repeats the same word over and over again, and this word also appears in the reference. If it is repeated as many times as the length of the reference text, then we get perfect precision! \n",
    "\n",
    "For this reason, the authors of the BLEU paper introduced a slight modification: a word is only counted as many times as it occurs in the reference. To illustrate this point, suppose we have the reference text \"the cat is on the mat\" and the generated text is \"the the the the the the\". From this simple example, we can calculate the precision values as follows:\n",
    "\n",
    "$$\n",
    "p_{vanilla} = \\frac{6}{6}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{mod} = \\frac{2}{6}\n",
    "$$\n",
    "\n",
    "As we can see, that simple correction has pruced a much more reasonable value. Now let's extend this by not only looking at single words, but $n$-grams as well. Let's assume we have one generated sentence, $snt$, that we want to compare against a reference sentence $snt'$. We extract all possible $n$-grams of degree $n$ and do the accounting to get the precision $p_{n}$:\n",
    "\n",
    "$$\n",
    "p_{n} = \\frac{\\sum_{n\\text{-gram} \\ \\in \\ snt} \\text{Count}_{clip}(n\\text{-gram})}{\\sum_{n\\text{-gram} \\ \\in \\ snt'} \\text{Count}(n\\text{-gram})}\n",
    "$$\n",
    "\n",
    "In order to avoid rewarding repetitive generations, the count in the numerator is clipped. What this means is that the occurrence count of an $n$-gram is capped at how many times it appears in the reference sentence. Also note that the definition of a sentence is not very strict in this equation, and if you had a generated text spanning multiple sentences you would treat is as one sentence.\n",
    "\n",
    "In general, we have more than one sample in the test set we want to evaluate, so we need to slightly extend the equation by summing over all samples in the corpus $C$ (we are assumming that $C$ contains both the original sentences and the generated ones):\n",
    "\n",
    "$$\n",
    "p_{n} = \\frac{\\sum_{snt \\ \\in \\ C}\\sum_{n\\text{-gram} \\ \\in \\ snt} \\text{Count}_{clip}(n\\text{-gram})}{\\sum_{snt' \\ \\in \\ C} \\sum_{n\\text{-gram} \\ \\in \\ snt'} \\text{Count}(n\\text{-gram})}\n",
    "$$\n",
    "\n",
    "We are almost there. Since we are not looking at recall, all generated sequences that are short but precise have a benefit compared to sentences that are longer. Therefore, the precision score favors short generations. To compensate for that, authors of BLEU introduced an additional term, the brevity ($BR$) penalty:\n",
    "\n",
    "$$\n",
    "BR = \\text{min} \\left( 1, e^{1-l_{ref}/l_{gen}} \\right)\n",
    "$$\n",
    "\n",
    "By taking the minimum, we ensure that this penalty never exceeds 1 and the exponential term becomes exponentially small when the length of the generated text $l_{gen}$ is smaller than the reference text $l_{ref}$.\n",
    "\n",
    "At this point you may ask, why don't we just use something like an F1-score to account for recall as well? The answer is that often in translation datasets there are multiple reference sentences instead of just one, so if we also measured recall we would  incentivize translations that used all the words from all the references. Therefore, it is preferable to look for high precision in the translation and make sure the translation and reference have a similar length.\n",
    "\n",
    "Finally, we can put everything together and get the equation for the BLEU score, where the last term is the geometric mean of the modified precision up to $n$-gram $N$. \n",
    "\n",
    "$$\n",
    "\\text{BLEU-}N = BR \\times \\left( \\prod^{N}_{n=1} p_{n}\\right)\n",
    "$$\n",
    "\n",
    "In practice, the BLEU-4 score is often reported. Howver, you can probably already see that this metric has many limitations; for instance, it doesn't take synonyms into account, and many steps in the derivation seem like ad hoc and rather fragile heuristics  You can find a wonderful exposition of BLEU's flaws in Rachel Tatman's blog post [\"Evaluating Text Output in NLP: BLEU at Your Own Risk\"](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213).\n",
    "\n",
    "In general, the field of text generation is still looking for better evaluation metrics, and finding ways to overcome the limits of metrics like BLEU is an active area of research. Another weakness of the BLEU metric is that it expects the texto already be tokenized. This can lead to varying results if the exact same method for text tokenization is not used. The SacreBLEU metric addresses this issue by internalizing the tokenization step; for this reason, is the prefered metric for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ca0a7-565e-4577-9901-491e51169f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "bleu_metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aaa993-fa95-4be8-8e95-6a95977b9565",
   "metadata": {},
   "source": [
    "The `bleu_metric` object is an instance of the `Metric` class, and works like an aggregator: you can add single instances with `add()` or whole batches via `add_batch()`. Once you have added all the samples you need to evaluate, you then call `compute()` and the metric is calculated. This returns a dictionary with several values, such as the precision for each $n$-gram, the length penalty, as well as the final BLEU score. Let's look at the example from before:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0acc710-5bbc-4e36-b790-9760d4d794aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "bleu_metric.add(prediction=\"the the the the the the\", reference=[\"the cat is on the mat\"])\n",
    "\n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c83a5f-21c1-4320-8183-670ae6822c37",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Note:** The BLEU score also works if there are multiple reference translations. This is why `reference` is passed as a list. To make the metric smoother for zero counts in the $n$-grams, BLEU integrates methods to modify the precision calculation. One method is to add a constant to the numerator. That way, a missing $n$-gram does not cause the score to automatically go to zero. For the purpose of explaining the values, we turn it off by setting `smooth_value = 0`\n",
    "\n",
    "----\n",
    "\n",
    "We can see the precision of the 1-gram is ineed 2/6, whereas the precisions for the 2/3/4-grams are all 0 (for more information about the individual metrics, like `counts` and `bp`, see the [SacreBLEU repository](https://github.com/mjpost/sacrebleu)). This means the geometric mean is zero, thus also the BLEU score. Let's look at another example where the prediciton is almost correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce44a4-9002-452b-aa10-f01e8ece04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric.add(prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ead34-6abe-4435-9123-3e7847103c85",
   "metadata": {},
   "source": [
    "We observe that the precision scores are much better. The 1-grams in the prediciton all match, and only in the precision scores do we see that something is off. For the 4-gram there are only two candidates in the predicted text, i.e., `[\"the\", \"cat\", \"is\", \"on\"]` and `[\"cat\", \"is\", \"on\", \"mat\"]`, where the last one doesn't match, hence the precision of 0.5.\n",
    "\n",
    "<span style=\"color:blue\">The BLEU score is widely used for evaluating text, especially in machine translation, since precise translations are usually favored over translations that include all possible and appropriate words. There are other applications, such as summarization, where the situation is diferent. There we want all the important information in the generated text, so we favour high recall. This is where the ROUGE score is usually used.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e699d84-0756-4634-89bf-2c0122d41b61",
   "metadata": {},
   "source": [
    "### 6.5.2 - ROUGE\n",
    "\n",
    "The ROUGE score ([C-Y. Lin, 2004](https://aclanthology.org/W04-1013.pdf)) was specifically developed for applications like summarization where high recall is more important than just precision. The approach is very similar to the BLEU score in that we look at different $n$-grams and compare their occurrences in the generated text and the reference texts. \n",
    "\n",
    "* With ROUGE, we check how many $n$-grams in the reference text also occur in the generated text.\n",
    "* With BLEU, we check many $n$-grams in the generated text appear in the reference.\n",
    "\n",
    "Given their similar definitions, we can reuse the precision formula with the minor modification that we count the (unclipped) occurrence of reference $n$-grams in the generated text in the numerator:\n",
    "\n",
    "$$\n",
    "\\text{ROUGE-}N = \\frac{\\sum_{snt' \\ \\in \\ C} \\ \\sum_{n\\text{-gram} \\ \\in \\ snt'} \\text{Count}_{\\text{match}}(n\\text{-gram})}{\\sum_{snt' \\ \\in \\ C} \\ \\sum_{n\\text{-gram} \\ \\in \\ snt'} \\text{Count}(n\\text{-gram})}\n",
    "$$\n",
    "\n",
    "This was the original proposal for ROUGE. Subsequently, researchers have found that fully removing precision can have strong negative effects. Going back to the BLEU formula without the clipped counting, we can measure precision as well, and we can then combine both precision and recall ROUGE score in the harmonic mean to get an F1-score. The score is the metric that is nowadays commonly reported for ROUGE.\n",
    "\n",
    "There is a separate score in ROUGE to measure the longest common substring (LCS), called ROUGE-L. The LCS can be calculated for any pair of strings. For example, the LCS for \"abab\" and \"abc\" would be \"ab\", and its lenght would be 2. If we want to compare this value between two samples we need to somehow normalize it because otherwise a longer text would be at an advantage. To achieve this, the inventor of ROUGE came up with an F-score-like scheme where the LCS is normalized with the length of the reference and generated text, then the two normalized scores are mixed together:\n",
    "\n",
    "$$\n",
    "R_{LCS} = \\frac{LCS(X,Y)}{m} \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P_{LCS} = \\frac{LCS(X,Y)}{n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_{LCS} = \\frac{(1 + \\beta^{2})R_{LCS}P_{LCS}}{R_{LCS} + \\beta P_{LCS}}, \\text{where} \\beta = P_{LCS} / R_{LCS}\n",
    "$$\n",
    "\n",
    "**Note:** My assumption is that $m$ is the legnth of the reference text and $n$ refers to the length of the predicted text.\n",
    "\n",
    "This way, the LCS score is properly normalized and can be compared across samples. In the ðŸ¤— Datasets implementation, two variations of ROUGE are calculated: one calcuates the score per sentence and averages it for the summaries (ROUGE-L), and the other calcuates it directly over the whole summary (ROUGE-Lsum).\n",
    "\n",
    "We have already generated a set of summaries and now we have a metric to compare the summaries systematically. Let's apply the ROUGE score to all the summaries generated by the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d3fed-6f1d-44c1-a427-0d01aa60f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "for model_name in summaries:\n",
    "    rouge_metric.add(prediction=summaries[model_name], reference=reference)\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "    records.append(rouge_dict)\n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb398d-42e2-46aa-8495-09ca209b41e7",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Note:** The ROUGE metric in the ðŸ¤— Datasets library also calculates confidence intervals (by default, the 5th and 95th percentiles). The average value is stored in the attribute `mid` and the internval can be retrieved with `low` and `high`.\n",
    "\n",
    "----\n",
    "\n",
    "These results are obviously not very reliable as we only looked at a single sample, but we can compare the quality of the summary for that one example. \n",
    "\n",
    "<span style=\"color:red\"><b>TODO: Rellenar una vez ejecutado y poner conclusiones</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d41fc-e80b-4335-a7c4-b4ba92284061",
   "metadata": {},
   "source": [
    "## 6.6 - Evaluating PEGASUS on the CNN/DailyMail Dataset\n",
    "\n",
    "We now have all the pieces in place to evaluate the model properly:\n",
    "\n",
    "* We have a dataset with a test set from CNN/DailyMail\n",
    "* We have a metric with ROUGE\n",
    "* We have a summarization model\n",
    "\n",
    "Let's first evaluate the performance of the three-sentence baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067be341-cd95-40fa-93f2-8d7341f31bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(dataset, metric, column_text=\"article\", column_summary=\"highlights\"):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n",
    "    metric.add_batch(predictions=summaries, references=dataset[column_summary])\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5821d1b-2f9f-433e-abad-77cd6eb53520",
   "metadata": {},
   "source": [
    "Now we'll apply the function to a subset of the data. Since the test fraction of the CNN/DailyMail dataset consists of roughly 10000 samples, generating summaries for all these articles takes a lot of time. Recall that every generated token requires a forward pass through the model; generating just 100 tokens for each sample will thus require 1 million forward passes, and if we use beam search, this number is multiplied by the number of beams. \n",
    "\n",
    "For the purpose of keeping the calculations relatively fast, we'll subsample the test set and run the evaluation on 100 samples instead. This should give us a much more stable estimation while completing in less than one hour on a single GPU for the PEGASUS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a1f2a-0201-470f-9ffd-fbf1030de2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fbe114-32e0-47ca-bcf5-d01587d9bf5b",
   "metadata": {},
   "source": [
    "Now letâ€™s implement the same evaluation function for evaluating the PEGASUS model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3c1f7-b9a3-49fe-ac26-756a591f221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "        \n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size=16, device=device, column_text=\"article\", column_summary=\"highlights\"):\n",
    "    \n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n",
    "        inputs = tokenizer(article_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device), length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "        \n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e440176-95a2-460b-b9e4-6048b91400fd",
   "metadata": {},
   "source": [
    "Let's unpack this evaluation code a bit. First we split the dataset into smaller batches that we can process simultaneously. Then for each batch we tokenize the input articles and feed them to the `generate()` function to produce the summaries using beam search. We use the same generation parameters as proposed in the paper. The `length_penalty` parameter ensures that the model does not generate sequences that are too long. Finally, we decode the generated texts, replace the `<n>` token, and add the decoded texts with the references to the metric. At the end, we compute and return the ROUGE scores. Let's now load the model again with the `AutoModelForSeq2SeqLM` class, used for seq2seq generation tasks, and evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f875749-74bc-4201-8933-e079db4bdbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "score = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size=8)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae5e15-e350-484d-a0c8-d7a0ae3635a4",
   "metadata": {},
   "source": [
    "Since ROUGE and BLEU correlate better with human judgement than loss or accuracy, we should focus on them and carefully explore and choose the decoding strategy when building text generation models. These metrics are far from perfect, however, and one should always consider human judgments as well.\n",
    "\n",
    "Now that we are equipped with an evaluation function, it's time to train our own model for summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13e3692-2c5d-4ea9-82ef-fefc396a138f",
   "metadata": {},
   "source": [
    "## 6.7 - Training a summarization model\n",
    "\n",
    "We have worked through a lot of details on text summarization and evaluation, so let's put this to use to train a custom text summarization model! For our application, we'll use the [SAMSum dataset](https://huggingface.co/datasets/samsum) developed by Smasung, which consists of a collection of dialogues along with brief summaries. In an enterprise setting, these dialogues might represent the interactions between a customer and the support center, so generating accurate summaries can help improve customer service and detect common patterns among customer requests. Let's load it and look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb125bf-e5fb-48b0-aff2-b82f7d25126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogue:\")\n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36dc4b8-032f-4fe3-9a37-50121d3dff10",
   "metadata": {},
   "source": [
    "The dialogues look like what you would expect from a chat via SMS or WhatsApp, including emojis and placeholders for GIFs. The `dialogue` field contains the full text and the `summary` the summarized dialogue. Could a model that was fine-tuned on the CNN/DailyMail dataset deal with that? Let's find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66b992-493a-4eb5-8c08-06e7140a3484",
   "metadata": {},
   "source": [
    "### 6.7.1 - Evaluating PEGASUS on SAMSum\n",
    "\n",
    "First we'll run the same summarization pipeline with PEGASUS to see what the output looks like. We can reuse the code we used for the CNN/DailyMail summary generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f4832-65ea-448e-bb51-9a42641770ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"Summary:\")\n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa333c13-3e97-418e-9a89-a1f1ab156063",
   "metadata": {},
   "source": [
    "We can see that the model mostly tries to summarize by extracting the key sentences from the dialogue. This probably worked relatively well on the CNN/DailyMail dataset, but the summaries in SAMSum are more abstract. Letâ€™s confirm this by running the full ROUGE evaluation on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea0a06-d43a-4cbc-95bb-3c4e01269af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model, tokenizer, column_text=\"dialogue\", column_summary=\"summary\", batch_size=8)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349aaa9-c8ec-491f-8707-f05cd8457a07",
   "metadata": {},
   "source": [
    "Well, the results arenâ€™t great, but this is not unexpected since weâ€™ve moved quite a bit away from the CNN/DailyMail data distribution. Nevertheless, setting up the evaluation pipeline before training has two advantages: we can directly measure the success of training with the metric and we have a good baseline.\n",
    "\n",
    "Fine-tuning the model on our dataset should result in an immediate improvement in the ROUGE metric, and if that is not the case weâ€™ll know something is wrong with our training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd3d36-20e0-4106-a1f1-0a42156eea69",
   "metadata": {},
   "source": [
    "### 6.7.2 - Fine-tuning PEGASUS\n",
    "\n",
    "Before we process the data for training, let's have a quick look at the length distribution of the input and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697009ab-9623-4ecf-b41c-6d15b2563a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\n",
    "s_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "axes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "axes[0].set_title(\"Dialogue Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")\n",
    "axes[1].set_title(\"Summary Token Length\")\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adfdf0e-79fa-4e03-8c09-75cb52a18e51",
   "metadata": {},
   "source": [
    "We see that most dialogues are much shorter than the CNN/DailyMail articles, with 100â€“200 tokens per dialogue. Similarly, the summaries are much shorter, with around 20â€“40 tokens (the average length of a tweet).\n",
    "\n",
    "Letâ€™s keep those observations in mind as we build the data collator for the `Trainer`. First we need to tokenize the dataset. For now, weâ€™ll set the maximum lengths to 1024 and 128 for the dialogues and summaries, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ba263-ad4d-4c25-ad0a-2036db14001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True)\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": input_encodings[\"input_ids\"], \n",
    "        \"attention_mask\": input_encodings[\"attention_mask\"], \n",
    "        \"labels\": target_encodings[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True)\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcba2a61-f892-413f-99db-54de62dc8644",
   "metadata": {},
   "source": [
    "A new thing in the use of tokenization step is the `tokenizer.as_target_tokenizer()` context. Some models require special tokens in the decoder inputs, so it's important to differentiate between the tokenization of encoder and decoder inputs. In the `with` statement (called a *context manager*), the tokenizer knows that it is tokenizing for the decoder and can process sequences accordingly.\n",
    "\n",
    "Now, we need to create the data collator. This function is called in the `Trainer` just before the batch is fed through the model. In most cases we can use the default collator, which collects all the tensors from the batch and simply stacks them. For the summarization task we need to not only stack the inputs but also prepare the targets on the decoder side. \n",
    "\n",
    "PEGASUS is an encoder-decoder transformer and thus has the classic seq2seq architecture. In a seq2seq setup, a common approach is to apply teacher forcing in the decoder. With this strategy, the decoder receives input tokens that consists of the labels shifted by one in addition to the encoder output; so when making the prediction for the next token, the decoder gets the ground truth shifted by one as an input, as illustrated in the following table:\n",
    "\n",
    "<img src=\"images/teacher_forcing_example.png\" title=\"\" alt=\"\" width=\"450\" data-align=\"center\">\n",
    "\n",
    "We shift it by one so that the decoder only sees the previous ground truth labels and not the current or future ones. Shifting alone suffices since the decoder has masked self-attention that masks all inputs at present and in the future. So, when we prepare our batch, we set up, the decoder inputs by shifting the labels to the right by one. After that, we make sure the padding tokens in the labels are ignored by the loss function by setting them to `-100`. We actually dont' have to this manyally, though, since the DataCollatorForSeq2Seq comes to the rescue and takes care of all these steps for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491bd782-79c6-4255-a7d9-b82b7c16c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='pegasus-samsum', \n",
    "    num_train_epochs=1, \n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=1, \n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, \n",
    "    logging_steps=10, \n",
    "    push_to_hub=False, # Set to false\n",
    "    evaluation_strategy='steps', \n",
    "    eval_steps=500, \n",
    "    save_steps=1e6,\n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb61406-a790-4b62-ae47-aae8f39ffbf8",
   "metadata": {},
   "source": [
    "One thing that is different from the previous settings is the `gradient_accumulation_steps` argument. Since the model is quite big, we had to set the batch size to 1. However, a batch size that is too small can hurt convergence. To resolve that issue, we can use a nifty technique called [gradient accumulation](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation). As the name suggests, instead of calculating the gradients of the full batch at once, we make smaller batches and aggregate the gradients. When we have aggregated enough gradients we run the optimization step. Naturally this is a bit slower than doing it one pass, but **it saves us a log of GPU memory**.\n",
    "\n",
    "We now have everything we need to initialize the trainer with:\n",
    "* model\n",
    "* tokenizer\n",
    "* training arguments\n",
    "* data collator\n",
    "* training and evaluation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28959f7f-6615-4955-adfc-c964f769fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    tokenizer=tokenizer, \n",
    "    data_collator=seq2seq_data_collator,\n",
    "    train_dataset=dataset_samsum_pt[\"train\"],\n",
    "    eval_dataset=dataset_samsum_pt[\"validation\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee111d06-8ed9-4fd1-b770-a85cbb194792",
   "metadata": {},
   "source": [
    "After training, we can directly run the evaluation function on the test set to see how well the model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d9a9b-d264-47fc-8eba-df08d63a1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate_summaries_pegasus(\n",
    "    dataset_samsum[\"test\"], \n",
    "    rouge_metric, \n",
    "    trainer.model, \n",
    "    tokenizer,\n",
    "    batch_size=2, \n",
    "    column_text=\"dialogue\", \n",
    "    column_summary=\"summary\"\n",
    ")\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[f\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d29710-9352-4792-af75-0ed8b691b4cc",
   "metadata": {},
   "source": [
    "We see that the ROUGE scores improved considerably over the model without finetuning, so even though the previous model was also trained for summarization, it was not well adapted for the new domain. \n",
    "\n",
    "----\n",
    "\n",
    "**Note:** You can also evaluate the generations as part of the training loop: use the extension of `TrainingArguments` called `Seq2SeqTrainingArguments` and specify `predict_with_generate=True`. Pass it to the dedicated `Trainer` called `Seq2SeqTrainer`, which then uses the `generate()` function instead of the model's forward pass to create predictions for evaluation.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de5cfb-034d-4609-bae3-e0a4fb6845c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try this idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8d9fa-2cc9-41e0-bd52-4e1b7bb338e0",
   "metadata": {},
   "source": [
    "### 6.7.3 - Generating dialogue summaries\n",
    "\n",
    "Looking at the losses and ROUGE scores, it seems the model is showing a significant improvement over the original model trained on CNN/DailyMail only. Let's see what a summary generated on a sample from the test set looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14fe17a-6256-4fb6-9415-0c3112a612e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
    "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
    "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
    "pipe = pipeline(\"summarization\", model=\"transformersbook/pegasus-samsum\")\n",
    "\n",
    "print(\"Dialogue:\")\n",
    "print(sample_text)\n",
    "print(\"\\nReference Summary:\")\n",
    "print(reference)\n",
    "print(\"\\nModel Summary:\")\n",
    "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd70af6-b3a4-4f93-96f2-5a719ced3807",
   "metadata": {},
   "source": [
    "That looks much more like the reference summary. It seems the model has learned to synthesize the dialogue into a summary without just extracting passages. Now, the ultimate test: how well does the model work on a custom input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3779c9-8d81-480a-9ec9-59001b7b6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dialogue = \"\"\"\\\n",
    "Thom: Hi guys, have you heard of transformers?\n",
    "Lewis: Yes, I used them recently!\n",
    "Leandro: Indeed, there is a great library by Hugging Face.\n",
    "Thom: I know, I helped build it ;)\n",
    "Lewis: Cool, maybe we should write a book about it. What do you think?\n",
    "Leandro: Great idea, how hard can it be?!\n",
    "Thom: I am in!\n",
    "Lewis: Awesome, let's do it together!\n",
    "\"\"\"\n",
    "print(pipe(custom_dialogue, **gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaebc2e-4d45-4ebb-bd1c-a2f1c3afa269",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Text summarization poses some unique challenges compared to other tasks that can be framed as classification tasks, like sentiment analysis, named entity recognition, or question answering. Conventional metrics such as accuracy do not reflect the quality of the generated text. As we saw, the BLEU and ROUGE metrics can better evaluate generated texts; however, human judgment remains the best measure.\n",
    "\n",
    "A common question when working with summarization models is how we can summarize documents where the texts are longer than the model's context length. Unfortunately, there is no single strategy to solve this problem, and to date this is still an open and active research question. For example, [recent work by OpenAI showed how to scale summarization by applying it recursively to long documents and using human feedback in the loop](https://arxiv.org/abs/2109.10862).\n",
    "\n",
    "In the next chapter we'll look at question answering, which is the task of providing an answer to a question based on a text passage. In contrast to summarization, with this task there exist good strategies to deal with long or many documents, and we'll show how to scale question answering to thousands of documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
