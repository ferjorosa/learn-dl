{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - MULTILINGUAL ENTITY RECOGNITION\n",
        "\n",
        "So far in this book we have applied transformers to solve NLP tasks on English corpora - but what do you do when your documents are written in Greek, Swahili, or Klingon? One approach is to search the Hugging Face Hub for a suitable pretrained language model and fine-tune it on the task at hand. However, these pretrained models tend to exist only for \"high-resource\" languages like German, Russian, or Mandarin, where plenty of webtext is available for pretraining. Another common challenge arises when your corpus is multilingual: maintaining multiple monolingual models in production will not be any fun for you or your engineering team.\n",
        "\n",
        "Fortunately, there is a class of multilingual transformers that come to the resue. Like **BERT**, these models use masked language moeling as a pretraining objective, but they are trained jointly on texts in over one hundred languages. By pretraining on huge corpora across many languages, these multilingual transformers enable **zero-shot cross-lingual transfer**.This means that a model that is fine-tuned on one language can be applied to others without any further training! This also makes these models well suited for \"code-switching\", where a speaker  alternates between two or more languages or dialects in the context of a single conversation.\n",
        "\n",
        "In this chapter, we will focus on the encoder-only model **XLM-RoBERTa** ([Conneau et al., 2019](https://arxiv.org/abs/1911.02116)), which can be fine-tuned to perform named entity recognition (NER) across several languages. NER is a common NLP task that identifies entities like people, organizations, or locations in text. These entities can be used for various applications such as gaining insights from company documents, augmenting the quality of search engines, or simply building a structured database from a corpus.\n",
        "\n",
        "For this chapter, let's assume that we want to perform NER for a customer based in Switzerland, where there are four national languages (with English often serving as a bridge between them). Let's start by getting a suitable multilingual corpus for this problem."
      ],
      "metadata": {},
      "id": "078eeb9e-4604-4825-9594-90cb911f27d0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 - The dataset\n",
        "\n",
        "We are going to use a subset of the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark called WikiANN or PAN-X. This dataset consists of Wikipedia articles in many languages, including the four most commonly spoken languages in Switzerland: German (62.9%), French (22.9%), Italian (8.4%), and English (5.9%). Each article is annotated with <code>LOC</code> (location), <code>PER</code> (person), and <code>ORG</code> (organization) tags in the [\"inside-outside-beginning (IOB2) format\"](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)). \n",
        "\n",
        "In the IOB2 format, a <code>B-</code> prefix indicates the beginning of an entity, and consecutive tokens belonging to the same entity are given an <code>I-</code> prefix. An <code>O</code> tag indicates that the token does not belong to any entity. For example, the following sentence:\n",
        "\n",
        "`Jeff Dean is a computer scientist at Google in California`\n",
        "\n",
        "<img src=\"images/ner_example.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
        "\n",
        "To load one of the <code>PAN-X</code> subsets in <code>XTREME</code>, we'll need to know which dataset configuration to pass the `load_dataset()` function. Whenever you are dealing with a dataset that has multiple domains, you can use the <code>get_dataset_config_names()</code> function to find out which subsets are available:"
      ],
      "metadata": {},
      "id": "55ca5772-93d6-4845-a5c4-972db5785683"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import get_dataset_config_names\n",
        "\n",
        "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
        "print(f\"XTREME has {len(xtreme_subsets)} configurations\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/8.97k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "856ef1d551a14780bb8408ddf47015a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/22.9k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bc1fb8d907c488bab850068f15581a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "XTREME has 183 configurations\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1652656830429
        }
      },
      "id": "9300f9d9-13cf-406a-b107-7fe82ace84d1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whoa, that is a lot of configurations! Let's narrow the search by just looking for the configurations that start with \"PAN\":"
      ],
      "metadata": {},
      "id": "52cdeffd-d6b0-42b3-95b0-4702dbb4f983"
    },
    {
      "cell_type": "code",
      "source": [
        "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
        "print(len(panx_subsets))\n",
        "panx_subsets[:3]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "40\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1652656830832
        }
      },
      "id": "973e22e4-f6cc-4d0a-8f7c-a92ced439de0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that there are 40 different configurations for the `PAN-X` subset data, where each one has a two-letter suffix that appears to be an [ISO 639-1 language code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes). This means that to load the German corpus, we would have to do the following:"
      ],
      "metadata": {},
      "id": "44eddba2-a674-4570-b7c1-58288841b165"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ger_data = load_dataset(\"xtreme\", name=\"PAN-X.de\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading and preparing dataset xtreme/PAN-X.de (download: 223.28 MiB, generated: 9.08 MiB, post-processed: Unknown size, total: 232.36 MiB) to /home/azureuser/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/7bf67e71297af51aebad531f84e824cf5c995d9ce994485f7cd2e90d9cc4d555...\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/234M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77477520913c46babecb45aba35dbded"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f9bfaf4bfcf47699c2463fe6186f9ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7e0023f137e4b70b514459968206052"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "710dfb781ca14eab859f3fed8bf0fec0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset xtreme downloaded and prepared to /home/azureuser/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/7bf67e71297af51aebad531f84e824cf5c995d9ce994485f7cd2e90d9cc4d555. Subsequent calls will reuse this data.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1652656891962
        }
      },
      "id": "e7d3fd55-2043-4742-93bd-9a0abd3ffdea"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make a realisitc Swiss corpus, we'll sample the German (`de`), French (`fr`), Italian (`it`), and English (`en`) corpora according to their spoken proportions. this will create a language imbalance that is very common in real-world datasets. where adquiring labeled examples ina a minority language can be expensive due to the lack of domain expoerts who are fluent in that language. This imbalanced dataset will simulate a common situation when working on multilingual applications, and we'll see how we can build a model that works on all languages.\n",
        "\n",
        "To keep track of each language, let's create a Python `defaultdict` that stores the language code as the key and a `PAN-X` corpus of type [DatasetDict](https://huggingface.co/docs/datasets/v2.1.0/en/package_reference/main_classes#datasets.DatasetDict) as the value.\n",
        "\n",
        "---\n",
        "\n",
        "<mark><b>Note:</mark> Defaultdict is a sub-class of the dictionary class that returns a dictionary-like object. The functionality of `dict` and `defaultdict` are almost same [except for the fact that <code>defaultdict</code> never raises a <code>KeyError</code>](https://www.geeksforgeeks.org/defaultdict-in-python/). It provides a default value for the key that does not exists.\n",
        "\n",
        "---"
      ],
      "metadata": {},
      "id": "552fd9ac-b8a3-44e8-97ae-a60bfe5142c9"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from datasets import DatasetDict\n",
        "\n",
        "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
        "fracs = [0.629, 0.229, 0.084, 0.059]\n",
        "# Return a DatasetDict if a key doesn't exist\n",
        "panx_ch = defaultdict(DatasetDict)\n",
        "\n",
        "for lang, frac in zip(langs, fracs):\n",
        "    # Load monolingual corpus\n",
        "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
        "    # Shuffle and downsample each split according to spoken proportion\n",
        "    for split in ds:\n",
        "        panx_ch[lang][split] = (\n",
        "        ds[split]\n",
        "        .shuffle(seed=0)\n",
        "        .select(range(int(frac * ds[split].num_rows))))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Reusing dataset xtreme (/home/azureuser/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/7bf67e71297af51aebad531f84e824cf5c995d9ce994485f7cd2e90d9cc4d555)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading and preparing dataset xtreme/PAN-X.fr (download: 223.28 MiB, generated: 6.37 MiB, post-processed: Unknown size, total: 229.65 MiB) to /home/azureuser/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/7bf67e71297af51aebad531f84e824cf5c995d9ce994485f7cd2e90d9cc4d555...\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5806d5a48a148ff9d6398670ee64c19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61f7499744d4454ab480acc9c3501e67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd497d90d5d2453a8d80827d786f084b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset xtreme downloaded and prepared to /home/azureuser/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/7bf67e71297af51aebad531f84e824cf5c995d9ce994485f7cd2e90d9cc4d555. Subsequent calls will reuse this data.\nDownloading and preparing dataset xtreme/PAN-X.it (download: 223.28 MiB, generated: 7.35 MiB, post-processed: Unknown size, total: 230.63 MiB) to /home/azureuser/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/7bf67e71297af51aebad531f84e824cf5c995d9ce994485f7cd2e90d9cc4d555...\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c83bf7a7dbf04eb09e83210964c588d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5dce231c057f4818a3ed0656499641cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2426883bcfa341669fb2ba19b1b19cf2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset xtreme downloaded and prepared to /home/azureuser/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/7bf67e71297af51aebad531f84e824cf5c995d9ce994485f7cd2e90d9cc4d555. Subsequent calls will reuse this data.\nDownloading and preparing dataset xtreme/PAN-X.en (download: 223.28 MiB, generated: 7.30 MiB, post-processed: Unknown size, total: 230.59 MiB) to /home/azureuser/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/7bf67e71297af51aebad531f84e824cf5c995d9ce994485f7cd2e90d9cc4d555...\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ead5b6c52b64bc3a41de143604ecb43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce71327870a64ee4a1cf2dda77166240"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7d9adc7a8544d088b601cd69be60fe7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Dataset xtreme downloaded and prepared to /home/azureuser/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/7bf67e71297af51aebad531f84e824cf5c995d9ce994485f7cd2e90d9cc4d555. Subsequent calls will reuse this data.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1652656906258
        }
      },
      "id": "22835837-3226-4c61-a052-fb151a1255f5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we used the `shuffle()` method to make sure we don't accidentally bias our dataset splits, while `select()` allows us to downsample each corpus according to the values in `fracs`. Let's have a look at how many examples we have per language in the training sets by accessing the `Dataset.num_rows` attribute:"
      ],
      "metadata": {},
      "id": "ea0f49ee-3666-40b0-ae34-c6a2f92e8a4d"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs}, index=[\"Number of training examples\"])\n",
        "# panx_ch"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "                                de    fr    it    en\nNumber of training examples  12580  4580  1680  1180",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>de</th>\n      <th>fr</th>\n      <th>it</th>\n      <th>en</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Number of training examples</th>\n      <td>12580</td>\n      <td>4580</td>\n      <td>1680</td>\n      <td>1180</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1652656906634
        }
      },
      "id": "3746ea4e-3fe0-42c8-8fc5-ad4980d2c32c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look at `panx_ch` we can see that each language is splitted in `train`, `validation` and `test`. We can also see that we have more examples in German than all other languages combines, so we'll use it as a starting point from which to perform zero-shot cross-lingual transfer to French, Italian, and English. Let's inspect one of the examples in the German corpus:"
      ],
      "metadata": {},
      "id": "be9eda0c-3900-4879-b85a-79fb23bc9213"
    },
    {
      "cell_type": "code",
      "source": [
        "element = panx_ch[\"de\"][\"train\"][0]\n",
        "for key, value in element.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "langs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\nner_tags: [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\ntokens: ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der', 'polnischen', 'Woiwodschaft', 'Pommern', '.']\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1652656906957
        }
      },
      "id": "298c4370-d771-4a79-b168-7618c314330d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The keys of our example correspond to the column names of an Arrow table, while the values denote the entries in each column. The `ner_tags` column correspond to the mapping of each entity to a class ID. However, this is a bit cryptic to the human eye so let's transform those IDs into our familiar `LOC`, `PER`, and `ORG` tags. To do this, we can take advantage of the `features` attribute that specified the underlying data types associated with each column:"
      ],
      "metadata": {},
      "id": "9d30b327-e08b-44ee-8d90-3432ccf5b2c2"
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "langs: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\nner_tags: Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None), length=-1, id=None)\ntokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1652656907286
        }
      },
      "id": "ed93a576-2222-42f9-a433-29e722fef1e6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Sequence` class specifies that the field contains a list of features, which in the case of `ner_tags` corresponds to a list of `ClassLabel` features. Let’s pick out this feature from the training set as follows:"
      ],
      "metadata": {},
      "id": "d2be5322-2680-42e9-a695-65b2a715b316"
    },
    {
      "cell_type": "code",
      "source": [
        "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
        "print(tags)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None)\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1652656907604
        }
      },
      "id": "5f418a91-707e-4b1e-b370-ddb3a2f512d9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the `ClassLabel.int2str()` method to create a new column in our training set with class names for each tag. We'll use the `map()` method to return a `dict` with the key corresponding the new column name and the values as a `list` of class names:"
      ],
      "metadata": {},
      "id": "c9dd3aef-8c09-4fee-9200-6473cda98e00"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tag_names(batch):\n",
        "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
        "\n",
        "panx_de = panx_ch[\"de\"].map(create_tag_names)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/6290 [00:00<?, ?ex/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d17c3daa491c4cbeabc59db3443c850f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/6290 [00:00<?, ?ex/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd3772c7f27141adb6d8511ac0d7410e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/12580 [00:00<?, ?ex/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "798fcf0a2ee54fe1b7ec0b6a365dd37b"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1652656913775
        }
      },
      "id": "2099c50c-d220-4092-b638-819fdf61e6a3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our tags in human-readable format, let's see how the tokens and tags align for the first example in the training set"
      ],
      "metadata": {},
      "id": "5a23e1b0-e973-47ee-8a33-08f19e1a9270"
    },
    {
      "cell_type": "code",
      "source": [
        "de_example = panx_de[\"train\"][0]\n",
        "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],['Tokens', 'Tags'])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "           0           1   2    3         4      5   6    7           8   \\\nTokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \nTags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n\n                  9        10 11  \nTokens  Woiwodschaft  Pommern  .  \nTags           B-LOC    I-LOC  O  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>2.000</td>\n      <td>Einwohnern</td>\n      <td>an</td>\n      <td>der</td>\n      <td>Danziger</td>\n      <td>Bucht</td>\n      <td>in</td>\n      <td>der</td>\n      <td>polnischen</td>\n      <td>Woiwodschaft</td>\n      <td>Pommern</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1652656914120
        }
      },
      "id": "49448d46-22b7-4429-b704-b58b9fc75ebc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The presence of the LOC tags make sense since the sentence \"2,000 Einwohnern an der Danziger Bucht in der polnischen Woiwodschaft Pommern\" means \"2,000 inhabitants at the Gdansk Bay in the Polish voivodeship of Pomerania\" in English, and Gdansk Bay is a bay in the Baltic sea, while \"voivodeship\" corresponds to a state in Poland.\n",
        "\n",
        "As a quick check that we don’t have any unusual imbalance in the tags, let’s calculate the frequencies of each entity across each split:"
      ],
      "metadata": {},
      "id": "191b81b5-720a-40bf-a8c8-c73deb564a38"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "split2freqs = defaultdict(Counter)\n",
        "\n",
        "for split, dataset in panx_de.items():\n",
        "    for row in dataset[\"ner_tags_str\"]:\n",
        "        for tag in row:\n",
        "            if tag.startswith(\"B\"):\n",
        "                tag_type = tag.split(\"-\")[1]\n",
        "                split2freqs[split][tag_type] += 1\n",
        "                \n",
        "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "             ORG   LOC   PER\nvalidation  2683  3172  2893\ntest        2573  3180  3071\ntrain       5366  6186  5810",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ORG</th>\n      <th>LOC</th>\n      <th>PER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>validation</th>\n      <td>2683</td>\n      <td>3172</td>\n      <td>2893</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>2573</td>\n      <td>3180</td>\n      <td>3071</td>\n    </tr>\n    <tr>\n      <th>train</th>\n      <td>5366</td>\n      <td>6186</td>\n      <td>5810</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1652656914464
        }
      },
      "id": "e042d638-2033-4d8f-97b0-023863dac6c3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks good—the distributions of the `PER`, `LOC`, and `ORG` frequencies are roughly the same for each split, so the validation and test sets should provide a good measure of our NER tagger's ability to generalize. Next, let’s look at a few popular multilingual transformers and how they can be adapted to tackle our NER task."
      ],
      "metadata": {},
      "id": "a8f5f6f9-9195-4697-8309-e248d65ae809"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 - Multilingual Transformers\n",
        "\n",
        "Multilingual transformers involve similar architectures and training procedures as their monolingual counterparts, except that the corpus used for pretraining consists of documents in many languages. <span style=\"color:blue\">A remarkable feature of this approach is that despite receiving no explicit information to differentitate among the languages, the resulting linguistic representations are able to generalize well across languages for a variety of downstream tasks</span>. In some cases, this ability to perform cross-lingual transfer can produce results that are competitive with those of monolingual models, which circumvents the need to train one model per language!\n",
        "\n",
        "To measure the progress of cross-lingual transfer for NER, the [CoNLL-2002](https://huggingface.co/datasets/conll2002) and [CoNLL-2003](https://huggingface.co/datasets/conll2003) datasets are often used as benchmark for English, Dutch, Spanish, and German. This benchmark consists of news articles annotated with the same `LOC`, `PER` and `ORG` categories as PAN-X, but it contains an additional `MISC` label for miscellaneous entities that do not belong to the previous three groups. \n",
        "\n",
        "Multilingual transformer models are usually evaluated in three different ways:\n",
        "\n",
        "* `en`. Fine-tune on the English training data and then evaluate on each language's test set.\n",
        "* `each`. Fine-tune and evaluate on monolingual test data to measure per-language performance.\n",
        "* `all`. Fine-tune on all the training data to evaluate on each language's test set.\n",
        "\n",
        "We will adopt similar evaluation strategy for our NER task, but first we need to select a model to evaluate. One of the first multilingual transformers was **mBERT**, which uses the same architecture and pretraining objective as **BERT** but adds Wikipedia articles from many languages to the pretraining corpus. Since then, **mBERT** has been superseded by **XLM-RoBERTa**, or **XLM-R** for short, so that is the model we'll consider in this chapter.\n",
        "\n",
        "XLM-R uses only masked language modeling as its pretraining objective for 100 languages, but it is distinguished by the huge size of its pretraining corpus compared to its predecessors: Wikipedia dumps for each language and 2.5 terabytes of Common Crawl data from the web. This corpus is several orders of magnitude larger than the ones used in earlier models and provides a significant boost in signal for low-resource languages like Burmese and Swahili, where only a small number of Wikipedia articles exist.\n",
        "\n",
        "The RoBERTa part of the model's name refers to the fact that the pretrainig approach is the same as for the monolingual RoBERTa models. RoBERTa's developers improved on several aspects of BERT, in particular by removing the next sentence prediction task altogether. XLM-R also drops the language embeddings used in XLM and **uses SentencePiece** ([Kudo and Richardson, 2018](https://arxiv.org/abs/1808.06226)) **to tokenize the raw texts directly**. <span style=\"color:blue\">Besides its multilingual nature, a notable difference between XLM-R and RoBERTa is the size of the respective vocabularies: 250,000 tokens versus 55,000!</span>"
      ],
      "metadata": {},
      "id": "433fcdba-9f0b-4ee9-9bde-445451e2b7a0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 - A closer look at tokenization\n",
        "\n",
        "Instead of using a [Wordpiece](https://towardsdatascience.com/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7) or a [Fast-Wordpiece](https://ai.googleblog.com/2021/12/a-fast-wordpiece-tokenization-system.html) tokenizer (see Chapter 2) , XLM-R uses a tokenizer called Sentence Piece that is trained on the raw text of all one hundred languages. To get a feel for how SentencePiece comparse to WordPiece, let's load the BERT and XLM-R tokenizers in the usual way with 🤗 Transformers:"
      ],
      "metadata": {},
      "id": "15edcace-3092-4597-94d0-aad5d031bf0c"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "bert_model_name = \"bert-base-cased\"\n",
        "xlmr_model_name = \"xlm-roberta-base\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27056871633645fba119f72c955dafd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a702dbbd585846f4abd4d3c71c35971d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "939eecdee7e84c67a9ca7a377f3276e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "117e147437304348b47a2b6742806439"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb42694c303f4c22a46fe0e20ec924de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad2cde87ad414dc082757e80403873a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22ea4bae78c640ccb68fe9c2c95dfd00"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1652656923764
        }
      },
      "id": "098077b2-900b-47dc-b10c-49f079caf32b"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Jack Sparrow loves New York!\"\n",
        "\n",
        "bert_tokens = bert_tokenizer(text).tokens()\n",
        "print(bert_tokens)\n",
        "xlmr_tokens = xlmr_tokenizer(text).tokens()\n",
        "print(xlmr_tokens)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]']\n['<s>', '▁Jack', '▁Spar', 'row', '▁love', 's', '▁New', '▁York', '!', '</s>']\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1652656924076
        }
      },
      "id": "b915d6ac-e69b-4c40-8325-dbbfd6a6f0dd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Other examples:\n",
        "\n",
        "# text_2 = \"Kelloggs sugar-flavor bars 12unx25g\" # Eng\n",
        "# text_2 = \"Batom Extremo Conforto FPS 25 Una - 3,8 g\" # Portuguese\n",
        "text_2 = \"Chico Classic pañales desechables talla 4 con 40 piezas\" # Spanish\n",
        "\n",
        "bert_tokens_2 = bert_tokenizer(text_2).tokens()\n",
        "print(bert_tokens_2)\n",
        "xlmr_tokens_2 = xlmr_tokenizer(text_2).tokens()\n",
        "print(xlmr_tokens_2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['[CLS]', 'Chico', 'Classic', 'p', '##a', '##ña', '##les', 'des', '##ech', '##ables', 'tall', '##a', '4', 'con', '40', 'pie', '##zas', '[SEP]']\n['<s>', '▁Chi', 'co', '▁Classic', '▁', 'paña', 'les', '▁des', 'ech', 'ables', '▁talla', '▁4', '▁con', '▁40', '▁pieza', 's', '</s>']\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1652656924369
        }
      },
      "id": "ad6f07c3-ee95-49bc-b253-7d2a7c8085f2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that instead of the `[CLS]` and `[SEP]` tokens that **BERT** uses for sentence classification tasks, **XLM-R** uses `<s>` and `<\\s>` to denote the start and end of a sequence. These tokens are added in the final stage of tokenization, as we’ll see next.\n",
        "\n",
        "### 4.3.1 - The tokenizer pipeline\n",
        "\n",
        "So far we have treated tokenization as a single operation that transforms strings to integers we can pass through the model. This is not entirely accurate, and if we take a closer look we can see that it is actually a full processing pipeline that usually consists of four steps:\n",
        "\n",
        "<img src=\"images/tokenizer_pipeline.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
        "\n",
        "Let's take a look at each step of the pipeline:\n",
        "\n",
        "**Normalization**\n",
        "\n",
        "<span style=\"color:blue\">This step corresponds to the set of operations you apply to a raw string to make it more \"cleaner\"</span>. Common operations include stripping whitespace and removing accented characters. Unicode normalization is another common normalization operation applied by many tokenizers to deal with the fact that there often exist various ways to write the same character. This can make two versions of the “same” string (i.e., with the same sequence of abstract characters) appear different; Unicode normalization schemes like NFC, NFD, NFKC, and NFKD replace the various ways to write the same character with standard forms. Another example of normalization is lowercasing. If the model is expected to only accept and use lowercase characters, this technique can be used to reduce the size of the vocabulary it requires. After normalization, our example string would look like: `jack sparrow loves new york!`\n",
        "\n",
        "**Pretokenization**\n",
        "\n",
        "This step splits a text into smaller objects that give an upper bound to what your tokens will be at the end of training. A good way to think of this is that the pretokenizer will split your text into \"words\" and your final tokens will be parts of those words. For the languages that allow this (English, German, and many Indo-European languages), strings can typically be split into words on whitespace and punctuation. For example, this step might transform our `[\"jack\", \"sparrow\", \"loves\", \"new\", \"york\", \"!\"]`. These words are  then simpler to split into subwords with Byte-Pair Encoding (BPE) or Unigram algorithms in the next step\n",
        "of the pipeline. <span style=\"color:blue\">However, splitting into \"words\" is not always a trivial and deterministic operation, or even an operation that makes sense. For instance, in languages like Chinese, Japanese, or Korean, grouping symbols in semantic units like Indo-European words can be a nondeterministic operation with several equally valid groups. <b>In this case, it might be best to not pretokenize the text and instead use a language-specific library for pretokenization</span>.\n",
        "\n",
        "**Tokenizer model**\n",
        "    \n",
        "Once the input texts are normalized and pretokenized, the tokenizer applies a subword splitting model on the words. <span style=\"color:blue\">This is the part of the pipeline that needs to be trained on your corpus (or that has been trained if you are using a pretrained tokenizer). <b>The role of the tokenizer is to split the words into subwords to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary tokens</span>. Several subword tokenization algorithms exist, including BPE, Unigram, and WordPiece. For instance, our running example might look like [jack, spa, rrow, loves, new, york, !] after the tokenizer model is applied. Note that at this point we no longer have a list of strings but a list of integers (input IDs); to keep the example illustrative, we’ve kept the words but dropped the quotes to indicate the transformation.\n",
        "\n",
        "**Post-processing**\n",
        "    \n",
        "<span style=\"color:blue\">This is the last step of the tokenization pipeline, in which some additional transformations can be applied on the list of tokens. For instance, by adding special tokens at the beginning or end of the input sequence of token indices</span>. For example, a BERT-style tokenizer would add classifications and separator tokens: `[CLS, jack, spa, rrow, loves, new, york, !, SEP]`. This sequence (recall that this will be a sequence of integers, not the tokens you see here) can then be fed to the model."
      ],
      "metadata": {},
      "id": "9ec625ba-2cd4-497a-bf9a-ca2b24f605f5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.2 - The SentencePiece tokenizer\n",
        "\n",
        "The SentencePiece tokenizer is based on a type of subword segmentation called Unigram and encodes each input text as a sequence of Unicode characters. This last feature is especially useful for multilingual corpora since it allows SentencePiece to be agnostic about accents, punctuation, and the fact that many languages, like Japanese, do not have whitespace characters. Another special feature of SentencePiece is that whitespace is assigned the Unicode symbol `U+2581`, or the `▁` character, also called the lower one quarter block character. This enables SentencePiece to detokenize a sequence without ambiguities and without relying on language-specific pretokenizers. \n",
        "\n",
        "In our example from the previous section, for instance, we can see that WordPiece has lost the information that there is no whitespace between \"York\" and \"!\". By contrast, <span style=\"color:blue\">SentencePiece preserves the whitespace in the tokenized text <b>so we can convert back to the raw text without ambiguity</span>:\n",
        "\n",
        "* SentencePience: `[..., _New, _York, !]`\n",
        "* WordPiece: `[..., New, York, !]`"
      ],
      "metadata": {},
      "id": "b77db2cf-47fb-4fd0-aff0-92d106245686"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")\n",
        "'<s> Jack Sparrow loves New York!</s>'"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "'<s> Jack Sparrow loves New York!</s>'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1652656924696
        }
      },
      "id": "8b6c0260-5f58-4966-8f10-41836f7eb4c0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 - Transformers for named entity recognition\n",
        "\n",
        "In Chapter 2, we saw that for text classification BERT uses the special `[CLS]` token to represent an entire sequence of text. This representation is then fed through a fully connected or dense layer to output the distribution of all the discrete label values:\n",
        "\n",
        "<img src=\"images/text_classification_BERT.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
        "\n",
        "**BERT** and other encoder-only transformers take a similar approach for NER, except that the representation of each individual input token is fed into the same fully connected layer to output the entity of the token. For this reason, NER is often framed as a token classification task. This process looks something like this:\n",
        "\n",
        "<img src=\"images/named_entity_recognition_BERT.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\">\n",
        "\n",
        "So far, so good, but how should we handle subwords in a token classification task? For example, the first name \"Christa\" in the previous figure is tokenized into the subwords \"Chr\" and \"##ista\", so which one(s) should be assigned the `B-PER` label?\n",
        "\n",
        "In the **BERT** paper, the authors assigned this label to the first subword (\"Chr\" in our example) and ignored the following subword (\"##ista\"). This is the convention we'll adopt here, and we'll indicate the ignored subwords with `IGN`. We can later easily propagate the predicted label of the first subword to the subsequent subwords in the postprocessing step. We could also have chosen to include the representation of the \"##ista\" subword by assigning it a copy of the `B-LOC` label, but this violates the IOB2 format.\n",
        "\n",
        "Fortunately, all the architecture aspects we have seen in **BERT** carry over to **XLM-R** since its architecture is based on **RoBERTa**, which is identical to **BERT**! Next we’ll see how 🤗 Transformers supports many other tasks with minor modifications."
      ],
      "metadata": {},
      "id": "c215bb79-46b9-4a40-9a5d-dfde3d73b652"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 - The Anatomy of the Transformers class\n",
        "\n",
        "🤗 Transformers is organized around dedicated classes for each architecture and task. The model classes associated with different tasks are named according to a `<ModelName>For<Task>` convention, or `AutoModelFor<Task>` when using the `AutoModel` classes. However, it is uncommon to have a model for a very specific task!\n",
        "\n",
        "That is why 🤗 Transformers is designed to enable you to easily extend existing models for your specific use case. You can load the weights from pretrained models, and you have access to task-specific helper functions. This lets you build custom models for specific objectives with very little overhead. In this section, we’ll see how we can implement our own custom model.\n",
        "\n",
        "### 4.5.1 - Bodies and heads\n",
        "\n",
        "The main concept that makes **Transformers** so versatile is the split of the architecture into a *body* and *head*. When training for a specific task,  we need to replace the last layer of the model with one that is suitable for the task. This last layer is called the model head; it’s the part that is task-specific. The rest of the model is called the body; it includes the token embeddings and transformer layers that are task-agnostic. This\n",
        "structure is reflected in the 🤗 Transformers code as well: the body of a model is implemented in a class such as `BertModel` or `GPT2Model` that returns the hidden states of the last layer. Task-specific models such as `BertForMaskedLM` or `BertForSequenceClassification` use the base model and add the necessary head on top of the hidden states\n",
        "\n",
        "<img src=\"images/bert_body_head.png\" title=\"\" alt=\"\" width=\"200\" data-align=\"center\">\n",
        "\n",
        "### 4.5.2 - Creating a cusotm model for token classification (e.g., entity recognition)\n",
        "\n",
        "Let’s go through the exercise of building a custom token classification head for **XLM-R**. Since **XLM-R** uses the same model architecture as **RoBERTa**, we will use **RoBERTa** as the base model, but augmented with settings specific to **XLM-R**. <span style=\"color:blue\">Note that this is an educational exercise to show you how to build a custom model for your own task. <b>For token classification, an <code>XLMRobertaForTokenClassification</code> class already exists that you can import from Transformers</span>. If you want, you can skip to the next section and simply use that one.\n",
        "\n",
        "To get started, we need a data structure that will represent our **XLM-R** NER tagger. As a first guess, we’ll need a configuration object to initialize the model and a `forward()` function to generate the outputs. Let’s go ahead and build our **XLM-R** class for token classification:"
      ],
      "metadata": {},
      "id": "988fa939-2954-43c7-8701-4932838f871c"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
        "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
        "\n",
        "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "    \n",
        "    config_class = XLMRobertaConfig\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        \n",
        "        # Load model body\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        \n",
        "        # Set up token classification head\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        \n",
        "        # Load pretrained body weights and randomly initialize head weights\n",
        "        self.init_weights()\n",
        "        \n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
        "        \n",
        "        # Use model body to get encoder representations\n",
        "        outputs = self.roberta(input_ids, attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids, **kwargs)\n",
        "        \n",
        "        # Apply classifier to encoder representation\n",
        "        sequence_output = self.dropout(outputs[0])\n",
        "        logits = self.classifier(sequence_output)\n",
        "        \n",
        "        # Calculate losses\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        \n",
        "        # Return model output object\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss, logits=logits, \n",
        "            hidden_states=outputs.hidden_states, \n",
        "            attentions=outputs.attentions)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1652656925047
        }
      },
      "id": "fe3d208e-fe38-4b63-9efc-c5f4cde3f52c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The `config_class` ensures that the standard **XLM-R** settings are used when we initialize a new model\n",
        "* With the `super()` method we call the initialization function of the `RobertaPreTrainedModel` class\n",
        "* We use a `RoBertaModel` as the body of our model. Note that we set `add_pooling_layer=False` to ensure all hidden states are returned and not only the one associated with the `[CLS]` token (e.g., used in text classification)\n",
        "* We define a classification head with a `nn.Dropout` and a `nn.Linear` layers\n",
        "* Finally, we initialize all the weights by calling the `init_weights()` method we inherit from `RobertaPreTrainedModel`, which will load the pretrained weights for the model body and randomly initialize the weights of our token classification head.\n",
        "\n",
        "The only thing left to do is to define what the model should do in a forward pass with a `forward()` method:\n",
        "\n",
        "* During the forward pass, the data is first fed through the model *body*\n",
        "* The hidden state, which is part of the model body output, is then fed through the *dropout* and *classification* layers\n",
        "* If we also provide labels in the forward pass, we can directly calculate the loss. If there is an *attention\n",
        "mask* we need to do a little bit more work to make sure we only calculate the loss of the unmasked tokens\n",
        "* Finally, we wrap all the outputs in a `TokenClassifierOutput` object that allows us to access elements in a named tuple manner"
      ],
      "metadata": {},
      "id": "e301f6ae-e6bd-4531-934f-3e19cab1a1f1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5.3 - Loading a custom model\n",
        "\n",
        "Now that we have defined our custom token classification model, we are ready to load the model weights (since we are using a `PretrainedModel`) via the `from_pretrained()` function with the additional `config` argument.\n",
        "\n",
        "In order to do that, we’ll need to provide some additional information beyond the model name, including the tags that we will use to label each entity and the mapping of each tag to an ID and vice versa. All of this information can be derived from our tags variable, which as a ClassLabel object has\n",
        "a names attribute that we can use to derive the mapping:"
      ],
      "metadata": {},
      "id": "fa8429b0-ad7b-49d9-955c-3323973ce15e"
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
        "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1652656925349
        }
      },
      "id": "74a0c9a9-6c07-4ace-9bf7-7bb8e7cc8887"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `AutoConfig` class contains the blueprint of a model's architecture. When we load a model with `AutoModel.from_pretrained(model_ckpt)`, the configuration file associated with that model is downloaded automatically. However, if we want to modify something like the number of classes or label names, then we can load the configuration first with the parameters we would like to customize.\n",
        "\n",
        "We'll store previous mappings and the `tags.num_classes` attribute in the `AutoConfig` object. Passing keyword arguments to the `from_pretrained()` method overrides the default values."
      ],
      "metadata": {},
      "id": "014b99c0-e546-4c87-a173-6e956350af99"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, \n",
        "                                         num_labels=tags.num_classes, \n",
        "                                         id2label=index2tag, label2id=tag2index)\n",
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "xlmr_model = (XLMRobertaForTokenClassification\n",
        "              .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
        "              .to(device))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/1.12G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e556fb53289349edb47e046a5bdf8e4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1652656989677
        }
      },
      "id": "ba26e2f6-e322-4481-9060-0faf8558d457"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a quick check that we have initialized the tokenizer and model correctly, let's test the predictions on our small sequence of known entities:"
      ],
      "metadata": {},
      "id": "be7850ab-bcfa-40a3-b151-bebe42d62bce"
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
        "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "             0      1      2      3      4  5     6      7   8     9\nTokens     <s>  ▁Jack  ▁Spar    row  ▁love  s  ▁New  ▁York   !  </s>\nInput IDs    0  21763  37456  15555   5161  7  2356   5753  38     2",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jack</td>\n      <td>▁Spar</td>\n      <td>row</td>\n      <td>▁love</td>\n      <td>s</td>\n      <td>▁New</td>\n      <td>▁York</td>\n      <td>!</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Input IDs</th>\n      <td>0</td>\n      <td>21763</td>\n      <td>37456</td>\n      <td>15555</td>\n      <td>5161</td>\n      <td>7</td>\n      <td>2356</td>\n      <td>5753</td>\n      <td>38</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1652656990098
        }
      },
      "id": "3b5fc50a-9081-4219-9480-8adf2bdcb32d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see here, the start `<s>` and end `</s>` tokens are given the IDs 0 and 2, respectively.\n",
        "\n",
        "Finally, we need to pass the inputs to the model and extract the predictions by taking the argmax to get the most likely class per token:"
      ],
      "metadata": {},
      "id": "e5f9e0b5-4201-447c-8a59-e1e63cb94d06"
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = xlmr_model(input_ids.to(device)).logits\n",
        "predictions = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "print(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\n",
        "print(f\"Shape of outputs: {outputs.shape}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of tokens in sequence: 10\nShape of outputs: torch.Size([1, 10, 7])\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1652656990553
        }
      },
      "id": "9adcb1fb-7d3e-45cf-863c-075d81e66993"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that the logits have the shape `[batch_size, num_tokens, num_tags]`, with each token given a logit among the seven possible NER tags. By enumerating over the sequence, we can quickly see what the pretrained model predicts:"
      ],
      "metadata": {},
      "id": "1ebf6556-9bf0-44fd-b1b3-4366a9acdf74"
    },
    {
      "cell_type": "code",
      "source": [
        "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
        "pd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "            0      1      2      3      4      5      6      7      8      9\nTokens    <s>  ▁Jack  ▁Spar    row  ▁love      s   ▁New  ▁York      !   </s>\nTags    I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC  I-LOC",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jack</td>\n      <td>▁Spar</td>\n      <td>row</td>\n      <td>▁love</td>\n      <td>s</td>\n      <td>▁New</td>\n      <td>▁York</td>\n      <td>!</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>I-LOC</td>\n      <td>I-LOC</td>\n      <td>I-LOC</td>\n      <td>I-LOC</td>\n      <td>I-LOC</td>\n      <td>I-LOC</td>\n      <td>I-LOC</td>\n      <td>I-LOC</td>\n      <td>I-LOC</td>\n      <td>I-LOC</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1652656990957
        }
      },
      "id": "bbfac7fa-4bd3-411f-b97f-a8c7c9897114"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsurprisingly, our token classification layer with random weights leaves a lot to be desired; before fine-tuning the model, let's wrap preceding steps into a helper function for later use:"
      ],
      "metadata": {},
      "id": "d2cab254-34ac-471f-870b-8ab26d899d28"
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_text(text, tags, model, tokenizer):\n",
        "    # Get tokens with special characters\n",
        "    tokens = tokenizer(text).tokens()\n",
        "    # Encode the sequence into IDs\n",
        "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
        "    # Get predictions as distribution over 7 possible classes\n",
        "    outputs = model(input_ids)[0]\n",
        "    # Take argmax to get most likely class per token\n",
        "    predictions = torch.argmax(outputs, dim=2)\n",
        "    # Convert to DataFrame\n",
        "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
        "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1652657648987
        }
      },
      "id": "8bc2fc22-985a-4cdb-bc42-24d43c8a2818"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 - Tokenizing texts for NER\n",
        "\n",
        "Before we can train our model, we need to tokenize the inputs and prepare the labels. Our first objective is then to tokenize the whole dataset so that we can pass it to the XLM-R model for fine-tuning. 🤗 Datasets provides a fast way to tokenize a Dataset object with the `map()` operation\n",
        "\n",
        "Following the approach taken in the Transformers documentation, let’s look at how this works with our single German example by first collecting the words and tags as ordinary lists:"
      ],
      "metadata": {},
      "id": "30ccceef-1a4a-4f72-a3e1-0b6e7b7de88b"
    },
    {
      "cell_type": "code",
      "source": [
        "words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]\n",
        "words, labels"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "(['2.000',\n  'Einwohnern',\n  'an',\n  'der',\n  'Danziger',\n  'Bucht',\n  'in',\n  'der',\n  'polnischen',\n  'Woiwodschaft',\n  'Pommern',\n  '.'],\n [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1652656991620
        }
      },
      "id": "6ec00bce-b8e4-4d1e-a935-f7102dd2e3cb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we tokenize each word and use the `is_split_into_words` argument to tell the tokenizer that our input sequence has already been split into words:"
      ],
      "metadata": {},
      "id": "35db7957-21b5-42ab-babc-289f78a655eb"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "pd.DataFrame([tokens], index=[\"Tokens\"])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "         0       1           2  3    4     5     6   7    8      9   ...   15  \\\nTokens  <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...  ▁Wo   \n\n       16   17      18   19    20 21 22 23    24  \nTokens  i  wod  schaft  ▁Po  mmer  n  ▁  .  </s>  \n\n[1 rows x 25 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁2.000</td>\n      <td>▁Einwohner</td>\n      <td>n</td>\n      <td>▁an</td>\n      <td>▁der</td>\n      <td>▁Dan</td>\n      <td>zi</td>\n      <td>ger</td>\n      <td>▁Buch</td>\n      <td>...</td>\n      <td>▁Wo</td>\n      <td>i</td>\n      <td>wod</td>\n      <td>schaft</td>\n      <td>▁Po</td>\n      <td>mmer</td>\n      <td>n</td>\n      <td>▁</td>\n      <td>.</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 25 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1652656991974
        }
      },
      "id": "5c84b4b3-8192-4c14-b026-cfbc422a58e6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, and contrary to the previous BERT tokenization of section 4.1, we can see that the tokenizer has split “Einwohnern” into two subwords, `▁Einwohner` and `n`. Since we’re following the convention that only `▁Einwohner` should be associated with the `B-LOC` label, we need a way to mask the subword representations after the first subword. Fortunately, `tokenized_input` is a class that contains a `word_ids()` function that can help us achieve this:"
      ],
      "metadata": {},
      "id": "28c76a5a-e753-409b-8062-0973c611e83f"
    },
    {
      "cell_type": "code",
      "source": [
        "word_ids = tokenized_input.word_ids()\n",
        "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "            0       1           2  3    4     5     6   7    8      9   ...  \\\nTokens     <s>  ▁2.000  ▁Einwohner  n  ▁an  ▁der  ▁Dan  zi  ger  ▁Buch  ...   \nWord IDs  None       0           1  1    2     3     4   4    4      5  ...   \n\n           15 16   17      18   19    20  21  22  23    24  \nTokens    ▁Wo  i  wod  schaft  ▁Po  mmer   n   ▁   .  </s>  \nWord IDs    9  9    9       9   10    10  10  11  11  None  \n\n[2 rows x 25 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁2.000</td>\n      <td>▁Einwohner</td>\n      <td>n</td>\n      <td>▁an</td>\n      <td>▁der</td>\n      <td>▁Dan</td>\n      <td>zi</td>\n      <td>ger</td>\n      <td>▁Buch</td>\n      <td>...</td>\n      <td>▁Wo</td>\n      <td>i</td>\n      <td>wod</td>\n      <td>schaft</td>\n      <td>▁Po</td>\n      <td>mmer</td>\n      <td>n</td>\n      <td>▁</td>\n      <td>.</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Word IDs</th>\n      <td>None</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>...</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>10</td>\n      <td>10</td>\n      <td>10</td>\n      <td>11</td>\n      <td>11</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 25 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1652656992315
        }
      },
      "id": "9c1eb709-0e71-49ec-9cc6-dd8fedecc1c6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that word_ids has mapped each subword to the corresponding index in the words sequence, so the first subword, `▁2.000`, is assigned the index 0, while `▁Einwohner` and `n` are assigned the index 1 (since `Einwohnern` is the second word in words). We can also see that special tokens like `<s>` and `<\\s>` are mapped to None. Let’s set `–100` as the label for these special tokens and the subwords we wish to mask during training:"
      ],
      "metadata": {},
      "id": "0e17978b-3858-4348-905b-1daf6da735f6"
    },
    {
      "cell_type": "code",
      "source": [
        "previous_word_idx = None\n",
        "label_ids = []\n",
        "\n",
        "for word_idx in word_ids:\n",
        "    if word_idx is None or word_idx == previous_word_idx:\n",
        "        label_ids.append(-100)\n",
        "    elif word_idx != previous_word_idx:\n",
        "        label_ids.append(labels[word_idx])\n",
        "    previous_word_idx = word_idx\n",
        "    \n",
        "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
        "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
        "\n",
        "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "             0       1           2     3    4     5      6     7     8   \\\nTokens      <s>  ▁2.000  ▁Einwohner     n  ▁an  ▁der   ▁Dan    zi   ger   \nWord IDs   None       0           1     1    2     3      4     4     4   \nLabel IDs  -100       0           0  -100    0     0      5  -100  -100   \nLabels      IGN       O           O   IGN    O     O  B-LOC   IGN   IGN   \n\n              9   ...     15    16    17      18     19    20    21  22    23  \\\nTokens     ▁Buch  ...    ▁Wo     i   wod  schaft    ▁Po  mmer     n   ▁     .   \nWord IDs       5  ...      9     9     9       9     10    10    10  11    11   \nLabel IDs      6  ...      5  -100  -100    -100      6  -100  -100   0  -100   \nLabels     I-LOC  ...  B-LOC   IGN   IGN     IGN  I-LOC   IGN   IGN   O   IGN   \n\n             24  \nTokens     </s>  \nWord IDs   None  \nLabel IDs  -100  \nLabels      IGN  \n\n[4 rows x 25 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁2.000</td>\n      <td>▁Einwohner</td>\n      <td>n</td>\n      <td>▁an</td>\n      <td>▁der</td>\n      <td>▁Dan</td>\n      <td>zi</td>\n      <td>ger</td>\n      <td>▁Buch</td>\n      <td>...</td>\n      <td>▁Wo</td>\n      <td>i</td>\n      <td>wod</td>\n      <td>schaft</td>\n      <td>▁Po</td>\n      <td>mmer</td>\n      <td>n</td>\n      <td>▁</td>\n      <td>.</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Word IDs</th>\n      <td>None</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>...</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n      <td>10</td>\n      <td>10</td>\n      <td>10</td>\n      <td>11</td>\n      <td>11</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>Label IDs</th>\n      <td>-100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>6</td>\n      <td>...</td>\n      <td>5</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>6</td>\n      <td>-100</td>\n      <td>-100</td>\n      <td>0</td>\n      <td>-100</td>\n      <td>-100</td>\n    </tr>\n    <tr>\n      <th>Labels</th>\n      <td>IGN</td>\n      <td>O</td>\n      <td>O</td>\n      <td>IGN</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>I-LOC</td>\n      <td>...</td>\n      <td>B-LOC</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>I-LOC</td>\n      <td>IGN</td>\n      <td>IGN</td>\n      <td>O</td>\n      <td>IGN</td>\n      <td>IGN</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 25 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1652656992726
        }
      },
      "id": "bea48688-5c88-4544-825e-5a4bfdbfdecf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "<mark><b>Note:</mark> The reason we chose `–100` as the ID to mask subword representations is that in PyTorch `nn.CrossEntropyLoss` has an attribute called `ignore_index` whose value is `–100`. <span style=\"color:blue\">This index is ignored during training, so we can use it to ignore the tokens associated with consecutive subwords</span>.\n",
        "\n",
        "----\n",
        "    \n",
        "And that’s it! We can clearly see how the label IDs align with the tokens, so let's scale this out to the whole dataset by defining a single function that wraps all the logic:"
      ],
      "metadata": {},
      "id": "47d8fb4c-d46e-43c6-943c-5b863eceb854"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None or word_idx == previous_word_idx:\n",
        "                label_ids.append(-100)\n",
        "            else:\n",
        "                label_ids.append(label[word_idx])\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1652656993043
        }
      },
      "id": "7ff55826-61a1-4d7b-aa55-351ae7700872"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have all the ingredients we need to encode each split, so let’s write a function we can iterate over:"
      ],
      "metadata": {},
      "id": "9e49e2b0-707d-4a36-a2df-89ca403ecae7"
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_panx_dataset(corpus):\n",
        "    return corpus.map(tokenize_and_align_labels, batched=True, remove_columns=['langs', 'ner_tags', 'tokens'])"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1652656993338
        }
      },
      "id": "5ec736c1-7242-452b-95d1-6c7b25c50fe3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "By applying this function to a DatasetDict object, we get an encoded `Dataset` object per split. Let's use this to encode our German corpus:"
      ],
      "metadata": {},
      "id": "938b90b5-d621-4144-b106-ddd02e498f0f"
    },
    {
      "cell_type": "code",
      "source": [
        "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/7 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d39a12bc57f84aeb81f73737e00860fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/7 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07312f93907441fe8f35bf887e26c396"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/13 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac38664f12634828a6935b68894227ba"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1652656996199
        }
      },
      "id": "348f3cd6-57fd-4e0e-bd90-bfd004d8f765"
    },
    {
      "cell_type": "code",
      "source": [
        "# print(panx_ch[\"de\"])\n",
        "print(panx_de_encoded)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DatasetDict({\n    validation: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels'],\n        num_rows: 6290\n    })\n    test: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels'],\n        num_rows: 6290\n    })\n    train: Dataset({\n        features: ['attention_mask', 'input_ids', 'labels'],\n        num_rows: 12580\n    })\n})\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1652656996516
        }
      },
      "id": "c5fd2761-fe52-41c9-b8c7-6d9983badae7"
    },
    {
      "cell_type": "code",
      "source": [
        "# panx_ch[\"de\"][\"train\"][0]\n",
        "# panx_de_encoded[\"train\"][0]"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1652656996836
        }
      },
      "id": "4690b620-86a4-48af-93a3-45757911524f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7 - Performance measures\n",
        "\n",
        "Evaluating a NER model is similar to evaluating a text classification model, and it is common to report results for precision, recall, and F1-score. The only subtlety is that <span style=\"color:blue\">all words of an entity need to be predicted correctly in order for a prediction to be counted as correct</span>. Fortunately, there is a nifty library called [seqeval](https://github.com/chakki-works/seqeval) that is designed for these kinds of tasks. For example, given some placeholder NER tags and model predictions, we can compute the metrics via seqeval’s `classification_report()` function:"
      ],
      "metadata": {},
      "id": "4af3a88f-77a3-4415-a416-1897961ea566"
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
        "[\"B-PER\", \"I-PER\", \"O\"]]\n",
        "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"],\n",
        "[\"B-PER\", \"I-PER\", \"O\"]]\n",
        "\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "              precision    recall  f1-score   support\n\n        MISC       0.00      0.00      0.00         1\n         PER       1.00      1.00      1.00         1\n\n   micro avg       0.50      0.50      0.50         2\n   macro avg       0.50      0.50      0.50         2\nweighted avg       0.50      0.50      0.50         2\n\n"
        }
      ],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1652656997250
        }
      },
      "id": "bcf45f51-4d82-42f7-b2dc-be1de3899aa2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**seqeval** expects the predictions and labels as lists of lists, with each list corresponding to a single example in our validation or test sets. To integrate these metrics during training, we need a function that can take the outputs of the model and convert them into the lists that **seqeval** expects:"
      ],
      "metadata": {},
      "id": "660bf2c7-5fa8-4dd5-98ca-1e7b433ea15c"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def align_predictions(predictions, label_ids):\n",
        "    preds = np.argmax(predictions, axis=2)\n",
        "    batch_size, seq_len = preds.shape\n",
        "    labels_list, preds_list = [], []\n",
        "    \n",
        "    for batch_idx in range(batch_size):\n",
        "        example_labels, example_preds = [], []\n",
        "        for seq_idx in range(seq_len):\n",
        "            # Ignore label IDs = -100\n",
        "            if label_ids[batch_idx, seq_idx] != -100:\n",
        "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
        "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
        "                \n",
        "            labels_list.append(example_labels)\n",
        "            preds_list.append(example_preds)\n",
        "    return preds_list, labels_list"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1652656997548
        }
      },
      "id": "44b3f9a0-98bd-4706-b525-fdcf8be1ff42"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.8 - Fine-tuning XLM-RoBERTa\n",
        "\n",
        "We now have all the ingredients to fine-tune our model! Our first strategy will be to fine-tune our base model on the German subset of PAN-X and then evaluate its zeroshot cross-lingual performance on French, Italian, and English.\n",
        "\n",
        "We'll use 🤗 Transformers `Trainer` to handle our training loop, so first we need to define the training attributes using the `TrainingArguments` class:"
      ],
      "metadata": {},
      "id": "e124e718-44cc-43cf-bbd3-191971d4e660"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "num_epochs = 3\n",
        "batch_size = 24\n",
        "logging_steps = len(panx_de_encoded[\"train\"]) // batch_size\n",
        "model_name = f\"{xlmr_model_name}-finetuned-panx-de\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_name, num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size, evaluation_strategy=\"epoch\",\n",
        "    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n",
        "    logging_steps=logging_steps)"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1652657081149
        }
      },
      "id": "0ea13b7b-b026-4a27-b2c4-89d2c092412d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we evaluate the model's predictions on the validation set at the end of every epoch, tweak the weight decay, and set save_steps to a large number to disable checkpointing and thus speed up training.\n",
        "\n",
        "For that, we need to tell the `Trainer` how to compute metrics on the validation set, so here we can use the `align_predictions()` function that we defined earlier to extract the predictions and labels in the format needed by seqeval to calculate the F1-score:"
      ],
      "metadata": {},
      "id": "da699563-47e6-40ec-910c-dd44c603ac59"
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
        "    return {\"f1\": f1_score(y_true, y_pred)}"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1652657087930
        }
      },
      "id": "7bc5ea19-2fd3-4510-a99b-3197df47ea39"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<span style=\"color:blue\">The final step is to define a data collator so we can pad each input sequence to the largest sequence length in a batch</span>. 🤗 Transformers provides a dedicated data collator for token classification that will pad the labels along with the inputs:"
      ],
      "metadata": {},
      "id": "6fc2ced2-00f1-4b64-8c08-6a5f821a7f4f"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1652657106799
        }
      },
      "id": "d36c606e-cb6d-4f79-aa34-a7dc321a6801"
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "<mark><b>Note:</mark> Padding the labels is necessary because, unlike in a text classification task, the labels are also sequences. One important detail here is that the label sequences are padded with the value –100, which, as we’ve seen, is ignored by PyTorch loss functions.\n",
        "\n",
        "----\n",
        "    \n",
        "We will train several models in the course of this chapter, so we'll avoid initializing a new model for every `Trainer` by creating a `model_init()` method. This method loads an untrained model and is called at the beginning of the `train()` call:"
      ],
      "metadata": {},
      "id": "d9b5ed19-78f4-4e86-92be-411fca15a0c3"
    },
    {
      "cell_type": "code",
      "source": [
        "def model_init():\n",
        "    return (XLMRobertaForTokenClassification\n",
        "            .from_pretrained(xlmr_model_name, config=xlmr_config)\n",
        "            .to(device))"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1652657109671
        }
      },
      "id": "e21723ee-a949-4aef-b1e5-b5acef04c3fc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now pass all this information together with the encoded datasets to the `Trainer`:"
      ],
      "metadata": {},
      "id": "f838f92e-1541-4e75-add9-e515d57a0d0d"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(model_init=model_init, args=training_args,\n",
        "                  data_collator=data_collator, compute_metrics=compute_metrics,\n",
        "                  train_dataset=panx_de_encoded[\"train\"],\n",
        "                  eval_dataset=panx_de_encoded[\"validation\"],\n",
        "                  tokenizer=xlmr_tokenizer)\n",
        "\n",
        "# trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='1575' max='1575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1575/1575 05:06, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Runtime</th>\n      <th>Samples Per Second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.262700</td>\n      <td>0.162432</td>\n      <td>0.816127</td>\n      <td>25.388300</td>\n      <td>247.752000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.129500</td>\n      <td>0.136182</td>\n      <td>0.852984</td>\n      <td>25.373600</td>\n      <td>247.895000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.084700</td>\n      <td>0.134204</td>\n      <td>0.862839</td>\n      <td>25.499000</td>\n      <td>246.677000</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Attempted to log scalar metric loss:\n0.2627\nAttempted to log scalar metric learning_rate:\n3.3365079365079365e-05\nAttempted to log scalar metric epoch:\n1.0\nAttempted to log scalar metric eval_loss:\n0.16243162751197815\nAttempted to log scalar metric eval_f1:\n0.8161267769750641\nAttempted to log scalar metric eval_runtime:\n25.3883\nAttempted to log scalar metric eval_samples_per_second:\n247.752\nAttempted to log scalar metric epoch:\n1.0\nAttempted to log scalar metric loss:\n0.1295\nAttempted to log scalar metric learning_rate:\n1.673015873015873e-05\nAttempted to log scalar metric epoch:\n2.0\nAttempted to log scalar metric eval_loss:\n0.13618239760398865\nAttempted to log scalar metric eval_f1:\n0.8529844829539018\nAttempted to log scalar metric eval_runtime:\n25.3736\nAttempted to log scalar metric eval_samples_per_second:\n247.895\nAttempted to log scalar metric epoch:\n2.0\nAttempted to log scalar metric loss:\n0.0847\nAttempted to log scalar metric learning_rate:\n9.523809523809524e-08\nAttempted to log scalar metric epoch:\n2.99\nAttempted to log scalar metric eval_loss:\n0.1342041939496994\nAttempted to log scalar metric eval_f1:\n0.8628386437212472\nAttempted to log scalar metric eval_runtime:\n25.499\nAttempted to log scalar metric eval_samples_per_second:\n246.677\nAttempted to log scalar metric epoch:\n3.0\nAttempted to log scalar metric train_runtime:\n309.7593\nAttempted to log scalar metric train_samples_per_second:\n5.085\nAttempted to log scalar metric total_flos:\n2820100892001072.0\nAttempted to log scalar metric epoch:\n3.0\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 40,
          "data": {
            "text/plain": "TrainOutput(global_step=1575, training_loss=0.15888861334513105, metrics={'train_runtime': 309.7593, 'train_samples_per_second': 5.085, 'total_flos': 2820100892001072.0, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': 185802752, 'init_mem_gpu_alloc_delta': 1109908480, 'init_mem_cpu_peaked_delta': 1884442624, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 224169984, 'train_mem_gpu_alloc_delta': 3339727872, 'train_mem_cpu_peaked_delta': 1824706560, 'train_mem_gpu_peaked_delta': 2559183360})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 40,
      "metadata": {
        "gather": {
          "logged": 1652657441846
        }
      },
      "id": "daea5e6a-3300-4f1f-b11f-890872110d35"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./models/xlm_roberta_finetuned\")"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652657931786
        }
      },
      "id": "9679383a-0203-4ede-9943-e92a34b79f61"
    },
    {
      "cell_type": "code",
      "source": [
        "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\r\n",
        "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": "         0      1      2      3     4     5           6    7     8        9   \\\nTokens  <s>  ▁Jeff    ▁De     an  ▁ist  ▁ein  ▁Informati  ker  ▁bei  ▁Google   \nTags      O  B-PER  I-PER  I-PER     O     O           O    O     O    B-ORG   \n\n         10          11     12    13  \nTokens  ▁in  ▁Kaliforni     en  </s>  \nTags      O       B-LOC  I-LOC     O  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jeff</td>\n      <td>▁De</td>\n      <td>an</td>\n      <td>▁ist</td>\n      <td>▁ein</td>\n      <td>▁Informati</td>\n      <td>ker</td>\n      <td>▁bei</td>\n      <td>▁Google</td>\n      <td>▁in</td>\n      <td>▁Kaliforni</td>\n      <td>en</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>O</td>\n      <td>B-PER</td>\n      <td>I-PER</td>\n      <td>I-PER</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-ORG</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 45,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1652657653685
        }
      },
      "id": "5836360e-b083-4b9e-80c0-db8eabd5b3b2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}