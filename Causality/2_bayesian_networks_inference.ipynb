{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bayesian network inference",
   "id": "31fba8de1890fc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 - Probabilistic inference",
   "id": "394df29e7ce4bf61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Probabilistic inference is one of the most fundamental mechanisms for reasoning under uncertainty. Given a BN $\\mathcal{B}$ over a set of random variables $\\mathbf{X}$, probabilistic inference allows us to answer general queries of the form $P(\\mathbf{i}|\\mathbf{e})$ where $\\mathbf{e}$ are the values of the evidence variables $\\mathbf{E} \\subset \\mathbf{X}$ and $\\mathbf{i}$ are the values of the variables of interest $\\mathbf{I} \\subseteq \\{\\mathbf{X} \\setminus \\mathbf{E}\\}$, whose values we do not know. For example, in the medical domain, we can query the probability of a certain disease given the observed symptoms. The goal of inference can be formulated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "P(\\mathbf{i}|\\mathbf{e}) = \\frac{P(\\mathbf{i},\\mathbf{e})}{P(\\mathbf{e})} \\ .\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "The general problem of probabilistic inference in BNs was first tackled by [Kim and Pearl (1983)](https://koasas.kaist.ac.kr/bitstream/10203/17387/1/A%20Computational%20Model%20for%20Causal%20and%20Diagnostic%20Reasoning%20in%20Inference%20Systems.pdf). On a very high level, inference algorithms can be divided into two main classes: exact inference methods and approximate inference methods. We address each of these classes in the following sections. For additional information on this topic, [Salmer√≥n et al. (2018)](https://www.jair.org/index.php/jair/article/download/11228/26433) provide a recent review of the literature.\n"
   ],
   "id": "7c0a4b1d6d1cb8c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1 - Exact inference methods",
   "id": "a828a9f8803eddfd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Exact inference algorithms are designed to give an exact answer to the probabilistic query. While exact methods were originally designed for categorical BNs, they can be easily adapted for linear Gaussian BNs using the canonical form representation [(Laurtizen, 1992)](https://www.jstor.org/stable/pdf/2290647.pdf). There are three main strategies of exact inference in BNs:\n",
    "\n",
    "* **Variable elimination.** As the name suggests, the idea behind variable elimination is to successively remove variables from a BN while maintaining its ability to answer the query of interest. It was first formalized by [Zhang and Poole (1994)](https://repository.hkust.edu.hk/ir/bitstream/1783.1-757/1/canai94.pdf), although its origins go back to [Shachter et al. (1990)](https://cdn.aaai.org/AAAI/1990/AAAI90-019.pdf).\n",
    "\n",
    "* **Jointrees.** The jointree algorithm is a variation on the variable elimination algorithm that can be understood in terms of factor elimination. This algorithm improves on the complexity of variable elimination when answering multiple queries. There are two main approaches to the junction tree algorithm: (i) the Shenoy-Shafer architecture ([Shenoy and Shafer, 1990](https://kuscholarworks.ku.edu/bitstream/handle/1808/144/UAI90.pdf)), and (ii) the Hugin architecture [(Jensen, 1990)](https://cir.nii.ac.jp/crid/1572824499660884992). Both of them are based on the work of [Lauritzen and Spiegelhalter (1988)](https://www.jstor.org/stable/pdf/2345762.pdf), which introduced the first jointree algorithm.\n",
    "\n",
    "* **Conditioning.** The first incarnation of the conditioning algorithm was presented by [Pearl (1986)](https://arxiv.org/pdf/1304.3422) in the\tcontext of cutset conditioning, where the conditioning variables cut all loops in the network, forming a polytree. The general algorithm, under the name of global conditioning, was presented by [Shachter et al. (1994)](https://arxiv.org/pdf/1302.6843), which demonstrated the relation between conditioning and variable elimination. Finally recursive conditioning was developed by [Darwiche (2001)](https://www.sciencedirect.com/science/article/pii/S0004370200000692/pdf).\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** A polytree is a DAG whose underlying undirected graph is both connected and acyclic (i.e., a tree).\n",
    "\n",
    "----\n",
    "\n",
    "Although it is NP-hard to perform exact inference in general BNs ([Cooper, 1990](https://www.sciencedirect.com/science/article/pii/000437029090060D/pdf?md5=22871d7acedfc77015e8a4296cd3efbf&pid=1-s2.0-000437029090060D-main.pdf)), it becomes tractable in BNs with bounded treewidth [(Kwisthout et al., 2010)](https://ebooks.iospress.nl/pdf/doi/10.3233/978-1-60750-606-5-237). The notion of treewidth was introduced by [Robertson and Seymour (1986)](https://www.sciencedirect.com/science/article/abs/pii/0196677486900234) and can be understood as a measure of similarity between a graph and a tree (e.g., trees have a treewidth $\\leq$ $1$). However, this result only applies to categorical BNs and linear Gaussian BNs. Inference in conditional linear Gaussian BNs is much harder. Even in simple models like polytrees, exact inference is NP-hard [(Lerner and Parr, 2001)](https://arxiv.org/pdf/1301.2288).\n"
   ],
   "id": "1891730127d689f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2 - Approximate inference methods",
   "id": "a28ff30a663cabd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Approximate inference algorithms are designed to give an approximate answer to the probabilistic query, with the understanding that giving the exact probability is not crucial. Unfortunately, identical to the exact case, performing approximate inference has also been demonstrated to be NP-hard in general BNs ([Dagum and Luby, 1993](https://www.sciencedirect.com/science/article/pii/000437029390036B/pdf?md5=6662ccd95b0a88ff2e6f9637bfd8e083&pid=1-s2.0-000437029390036B-main.pdf); [Lerner and Parr, 2001](https://arxiv.org/pdf/1301.2288)). Despite these discouraging results, one can try to produce useful approximate algorithms with a computational performance that is in many cases far more manageable than that of exact algorithms. There are two main strategies of approximate inference in BNs:\n",
    "\n",
    "\n",
    "* **Sampling.** The idea behind sampling-based algorithms is to randomly pick assignments of the random variables, called samples, and then estimate the JPD of the query. It was first introduced for inference in BNs by [Henrion (1986)](https://www.academia.edu/download/55836968/Propagating_w_Logic_Sampling._Henrion_1988.pdf), who proposed a method based on probabilistic logic sampling. Several improvements have been done since then, such as likelihood weighting ([Shachter and Peot, 1990](https://arxiv.org/pdf/1304.1526)) and Markov chain Monte Carlo ([Pearl, 1987](https://hedibert.org/wp-content/uploads/2013/12/1987Pearl.pdf); [Hrycej, 1990](https://www.sciencedirect.com/science/article/abs/pii/000437029090020Z); [York, 1992](https://www.sciencedirect.com/science/article/abs/pii/0004370292900667)).\n",
    "\t\n",
    "* **Optimization.** The idea behind optimization-based algorithms is to construct an approximation to the target distribution, and then optimize a similarity function. This approach is usually referred to as variational inference ([Beli et al., 2016](https://arxiv.org/pdf/1601.00670)). Methods in this class fall into three main categories: belief propagation ([Pearl, 1988](https://dl.acm.org/doi/10.5555/534975); [Yedidia et al., 2000](https://proceedings.neurips.cc/paper_files/paper/2000/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf)), expectation propagation ([Minka, 2001](https://arxiv.org/pdf/1301.2294)) and mean-field variational inference ([Saul et al., 1996](https://www.jair.org/index.php/jair/article/download/10156/24075)). \n",
    "\n",
    "----\n",
    "\n",
    "**Note:** The mean-field approach is discussed in detail in `3_bayesian_network_learning.ipynb` since it is a central part of the Variational Bayes framework.\n",
    "\n",
    "----"
   ],
   "id": "465f84e1fefdb17b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 - PGMPy",
   "id": "161f8213b1fd9040"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6ab1eb94138c20ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
