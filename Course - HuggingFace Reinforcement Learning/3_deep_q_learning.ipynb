{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - From Q-Learning to Deep Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned that Q-Learning is an algorithm used to train our Q-Function, which is an action-value function that determines the value of being in a particular state and taking a specific action at that state. The \"Q\" in Q-Learning stands for the quality of the action at that state.\n",
    "\n",
    "Internally, our Q-function is represented by a Q-table, which is a table where each cell corresponds to a state-action pair value. We can think of this Q-table as the memory or cheat sheet of our Q-function.\n",
    "\n",
    "While Q-Learning has shown good performance in small state space environments with 16 and 500 states, it can become ineffective in large state space environments. For instance, if we want to use Q-Learning to train an agent that plays Atari games by using the frames as input, we would need a Q-table with a number of states ranging from $10^9$ to $10^{11}$ states.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_3/atari-envs.gif\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "A single frame in Atari is composed of an image of 210x160 pixels. Given the images are in color (RGB), there are 3 channels. As a result, Atari environments have an observation space with a shape of (210, 160, 3), where each pixel contains a value ranging from 0 to 255. That gives us a gigantic state space: $256^{210 \\times 160 \\times 3} = 256^{100800}$.\n",
    "\n",
    "Therefore, creating and updating a Q-table for this environment would not be efficient. In this case, the best idea is to approximate the Q-values using a parametrized Q-function $Q_{\\theta}(s,a)$. This is where deep Q-Learning comes in. Instead of using a Q-table, **deep Q-Learning uses a neural network that takes a state and approximates Q-values for each action based on that state**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_3/deep.jpg\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - The Deep Q-Network (DQN)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
