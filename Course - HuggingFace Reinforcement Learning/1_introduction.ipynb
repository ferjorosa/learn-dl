{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- What is Reinforcement learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - The big picture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind Reinforcement Learning  (RL) is that an **agent** (an AI) will learn from the environment by **interacting with it** (through trial and error) and receiving **rewards** (negative or positive) as feedback for performing actions.\n",
    "\n",
    "For instance, imagine putting your little brother in front of a video game he never played, giving him a controller, and leaving him alone. \n",
    "\n",
    "Your brother will interact with the environment (the video game) by pressing the right button (action). He got a coin, that’s a +1 reward. It’s positive, he just understood that in this game he must get the coins. But then, he presses right again and he touches an enemy. He just died, so that’s a -1 reward.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/brother_1.png\" width=\"350\" data-align=\"center\"></td>\n",
    "        <td><img src=\"images_1/brother_2.png\" width=\"350\" data-align=\"center\"></td>\n",
    "        <td><img src=\"images_1/brother_3.png\" width=\"350\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "By interacting with his environment through trial and error, your little brother understood that he needed to get coins in this environment but avoid the enemies. Without any supervision, the child will get better and better at playing the game.\n",
    "\n",
    "That’s how humans and animals learn, through interaction. **RL is just a computational approach of learning from actions**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - A formal definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - The RL framework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - The RL process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL process consists of:\n",
    "\n",
    "* A loop of state\n",
    "* An action\n",
    "* A reward\n",
    "\n",
    "To understand the RL process, let’s imagine an agent learning to play a platform game:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/RL_process_game.jpg\" title=\"\" alt=\"\" width=\"550\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* Our Agent receives state $S_{0}$ from the **Environment** — we receive the first frame of our game (Environment).\n",
    "* Based on that **state** $S_{0}$, the Agent takes action $A_{0}$ — our Agent will move to the right.\n",
    "* Environment goes to a **new state** $S_{1}$ — new frame.\n",
    "* The environment gives some **reward** $R_{1}$ to the Agent — we’re not dead (`Positive Reward` +1).\n",
    "\n",
    "This RL loop outputs a sequence of state, action, reward and next state.\n",
    "\n",
    "**The agent's goal is to maximize its cumulative reward, called the expected return**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - The reward hypothesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL is based on the **reward hypothesis**, which is that all goals can be described as the **maximization of the expected return** (expected cumulative reward).\n",
    "\n",
    "That’s why in Reinforcement Learning, **to have the best behavior**, we aim to learn to take actions that **maximize the expected cumulative reward**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Markov property"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL process is also called the **Markov Decision Process** (MDP).\n",
    "\n",
    "The Markov Property implies that our agent needs **only the current state to decide** what action to take and **not the history of all the states and actions** they took before."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Observations / States space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations/States are the **information our agent gets from the environment**. In the case of a video game, it can be a frame (a screenshot). In the case of the trading agent, it can be the value of a certain stock, etc.\n",
    "\n",
    "There is a differentiation to make between *observation* and *state*, however:\n",
    "\n",
    "* *State* $S$ is a complete description of the state of the world (there is no hidden information). In a fully observed environment. For example, in chess game, we receive a state from the environment since **we have access to the whole check board information**.\n",
    "\n",
    "* *Observation* $O$ is a partial description of the state. In a partially observed environment. For example, in Super Mario Bros, we receive an observation since **we only see a part of the level**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/obs_space_recap.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** In this course, we use the term \"state\" to denote both state and observation, but we will make the distinction in implementations.\n",
    "\n",
    "----\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Action space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is the set of all possible actions in the environment. The actions can come from a discrete or continuous space.\n",
    "\n",
    "* *Discrete space*: the number of possible actions is finite. For example, in Super Mario Bros, we only have 5 possible actions: 4 directions and jumping\n",
    "\n",
    "* *Continuous space*: the number of possible actions is infinite. For example, A Self Driving Car agent has an infinite number of possible actions since it can turn left 20°, 21.1°, 21.2°, honk, turn right 20°, etc.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/action_space.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Rewards and discounting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward is fundamental in RL because is **the only feedback** for the agent. Thanks to it, our agent knows **if the action taken was good or not**.\n",
    "\n",
    "The cumulative reward at each time step $t$ can be written as:\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/rewards_1.png\" title=\"\" alt=\"\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$$\n",
    "R(\\tau) = \\sum_{k=0}^{\\infty} r_{t+k+1}\n",
    "$$\n",
    "\n",
    "However, in reality, **we can't just add them like that**. The rewards that come sooner (at the beginning of the game) **are more likely to happen** since they are more predictable than the long-term future reward.\n",
    "\n",
    "Let's say your agent is this tiny mouse that can move one tile each time step, and your opponent is the cat (which can move too). The mouse's goal is **to eat the maximum amount of cheese before being eaten by the cat**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/rewards_3.jpg\" title=\"\" alt=\"\" width=\"450\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "As we can see in the diagram, **it's more probable to eat the cheese near us than the cheese close to the cat** (the closer we are to the cat, the more dangerous it is).\n",
    "\n",
    "Consequently, **the reward near the cat, even if it is bigger (more cheese), will be more discounted** since we are not really sure we will be able to eat it.\n",
    "\n",
    "To discount the rewards, we proceed like this:\n",
    "\n",
    "1. We define a discount rate called $\\gamma$. **It must be between 0 and 1**. Most of the time between **0.99 and 0.95**.\n",
    "    * The larger the $\\gamma$, the smaller the discount. This means our agent cares more about the long-term reward.\n",
    "    * The smaller the gamme, the bigger the discount. This means our agent cares more about the short term reward (the nearest cheese).\n",
    "    \n",
    "2. Then, each reward will be discounted by $\\gamma$ to the exponent of the time step. As the time step increases, the cat gets closer to us, so the future reward is less and less likely to happen.\n",
    "\n",
    "Our discounted expected cumulative reward is:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/rewards_4.png\" title=\"\" alt=\"\" width=\"450\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 - Types of tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task is an instance of a RL problem. We can have two types of tasks:\n",
    "\n",
    "* **Episodic tasks.** We have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and new States. **After each episode, the agent can learn how to choose the best actions taking its previous experience**. For instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ending when you are killed or you reached the end of the level.\n",
    "\n",
    "* **Continuing tasks.** Tasks that **continue forever** (no terminal state). In this case, the agent must **learn how to choose the best actions and simultaneously interact with the environment**. For instance, an agent that does automated stock trading. For this task, there is no starting point and terminal state. The agent keeps running until we decide to stop it.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/tasks.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 - The exploration/exploitation trade-off"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before looking at the different methods to solve Reinforcement Learning problems, we must cover one more very important topic: *the exploration/exploitation trade-off*.\n",
    "\n",
    "* *Exploration* is exploring the environment by trying random actions in order to **find more information about the environment**.\n",
    "* *Exploitation* is **exploiting known information to maximize the reward**.\n",
    "\n",
    "Remember, the goal of our RL agent is to maximize the expected cumulative reward. However, **we can fall into a common trap**.\n",
    "\n",
    "In the following game, our mouse can have an *infinite amount of small cheese (+1 each). But at the top of the maze there is a gigantic sum of cheese (+1000).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/exp_1.jpg\" title=\"\" alt=\"\" width=\"400\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "If we only focus on exploitation, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit the **nearest source of rewards**, even if this source is small (exploitation). But if our agent does a little bit of exploration, it can **discover the big reward** (the pile of big cheese).\n",
    "\n",
    "This is what we call the exploration/exploitation trade-off. We need to balance how much we explore the environment and how much we exploit what we know about the environment. Therefore, we must define a rule that helps to handle this trade-off. We’ll see the different ways to handle it in the future units.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/expexpltradeoff.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Two main approaches for solving RL problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we build an RL agent that can select the actions that maximize its expected cumulative reward?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - The policy $\\pi$: the agent's brain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The policy $\\pi$ **is the function that tells us what action to take given the state we are**. So it defines the agent's behavior at a given time.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/policy_1.png\" title=\"\" alt=\"\" width=\"450\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "This policy **is the function we want to learn**, our goal is to find the optimal policy $\\pi^{*}$, the policy that maximizes the expected return when the agent acts according to it. We find this $\\pi^{*}$ **through training**.\n",
    "\n",
    "There are two approaches to train our agent to find this optimal policy $\\pi^{*}$\n",
    "\n",
    "* Directly, **teach the agent to learn which action to take**, given the current state: **Policy-based methods**.\n",
    "* Indirectly, **teach the agent which state is more valuable** and then take the action that leads to the more valuable states: **Value-based methods**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Policy-based methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In policy-based methods, **we learn a policy function directly**.\n",
    "\n",
    "This function will define a mapping between each state and the best corresponding action. We can also say that **it'll define a probability distribution over the set of possible actions at that state**. \n",
    "\n",
    "We have two types of policies:\n",
    "\n",
    "* **Deterministic:** a policy at a given state will always return the same action. $a = \\pi(s)$\n",
    "\n",
    "* **Stochastic:** outputs a probability distribution over actions. $\\pi(a|s) = P(A|s)$ i.e., probability distribution over the set of actions given the state.\n",
    "\n",
    " <table>\n",
    "  <tr>\n",
    "    <th>Determininstic</th>\n",
    "    <th>Stochastic</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images_1/pbm_1.jpg\" title=\"\" alt=\"\" width=\"450\" data-align=\"center\"></td>\n",
    "    <td><img src=\"images_1/pbm_2.jpg\" title=\"\" alt=\"\" width=\"450\" data-align=\"center\"></td>\n",
    "  </tr>\n",
    "</table> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Value-based methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In value-based methods, instead of training a policy function, **we train a value function that maps a state to the expected value of being at that state**.\n",
    "\n",
    "The value of a state is the expected discounted return the agent can get if it starts in that state, and then act according to our policy, where \"act according to our policy\" just means that our policy is \"going to the state with the highest value\".\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/value_1.png\" title=\"\" alt=\"\" width=\"450\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/value_2.jpg\" title=\"\" alt=\"\" width=\"450\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Here we can see that **our value function defined value for each possible state**. Thanks to this function, at each step our policy will select the state with the biggest value defined by the value function: -7, then -6, then -5 (and so on) to attain the goal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Two types of value-based methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In value-based methods, we learn a value function that maps a state to the expected value of being at that state. The value of a state is the expected discounted return the agent can get if it starts at that state and then acts according to our policy.\n",
    "\n",
    "-----\n",
    "\n",
    "<span style=\"color:red\"><b>Question:</b></span> But what does it mean to act according to our policy? After all, we don't have a policy in value-based methods since we train a value function and not a policy.\n",
    "\n",
    "<span style=\"color:blue\"><b>Answer:</b></span> The policy function is \"latent\", we dont define by hand the behaviour of our policy; **it is the training that will indirectly define it**. Now, since the policy is not trained/learned, **we need to specify its behavior**. For instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward, we'll create a Greedy Policy.\n",
    "\n",
    "-----\n",
    "\n",
    "Whatever method we use to solve the problem, we will have a policy. In the case of value-based methods, you don’t train the policy: your policy is just a simple pre-specified function (for instance, Greedy Policy) that uses the values given by the value-function to select its actions. <span style=\"color:blue\">The value function is usually a <b>Neural network</b>, hence the deep RL name</span>.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/link-value-policy.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "So, we have two types of value-based functions:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - The state-value function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the state value function under a policy $\\pi$ like this:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/state-value-function-1.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "For each state, the state-value function outputs the expected return if the agent starts at that state and then follows the policy forever afterward (for all future timesteps, if you prefer).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/state-value-function-2.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - The action-value function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the action-value function, for each state and action pair, the action-value function outputs the expected return if the agent starts in that state and takes action, and the follows the policy forever after. \n",
    "\n",
    "The value of taking action $a$ in state $s$ under a policy $\\pi$ is:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/action-state-value-function-1.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/action-state-value-function-2.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Differences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the difference is:\n",
    "\n",
    "* In state-value function, we calculate **the value of a state** $S_{t}$\n",
    "* In action-value function, we calculate **the value of the state-action pair** $(S_{t}, A_{t})$; i.e., the value of taking that action at that state.\n",
    "\n",
    "In either case, the returned value is the expected return. \n",
    "\n",
    "However, the problem is that **it implies that to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state or state-action pair**.\n",
    "\n",
    "This can be a **computationally expensive process**, and that it where the <span style=\"color:blue\"><b>Bellman equation</b></span> comes to help us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - The Bellman Equation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With what we have learned so far, we know that if we calculate the $V(S_{t})$ (value of a state), we need to calculate the return starting at that state and then follow the policy forever after. So, we need to calculate the sum of the expected rewards:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/bellman2.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The thing is, to calculate $V(S_{t+1})$$, we are going to repeat the computation of the value of several states. Therefore, instead of doing this repetitive computation, we can use the **Bellman equation** (hint: if you know what <a href=\"https://en.wikipedia.org/wiki/Dynamic_programming\">dynamic programming</a> is, this is very similar)\n",
    "\n",
    "The Bellman equation is a recursive equation that works like this: instead of starting for each state from the beginning and calculating the return, we can consider the value of any state as **the immediate reward $R_{t+1}$ + the discounted value of the state that follows ($\\gamma * V(S_{t+1})$)**. So, we are basically accumulating the \"local reward sum\" so we can speed up the computational process.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/bellman4.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Monte Carlo vs Temporal Difference Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo and Temporal Difference Learning are two different **strategies on how to train our value function or our policy function**. both of them use experience to solve the RL problem.\n",
    "\n",
    "On one hand, Monte Carlo uses an entire episode of experience before learning. On the other hand, Temporal Difference uses only a step $(S_{t}, A_{t}, R_{t+1}, S_{t+1})$ to learn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 - Monte Carlo: learning at the end of the episode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo waits until the end of the episode, calculates $G_{t}$ (return) and uses it as a target for updating $V(S_{t})$. **It requires a complete episode of interaction before updating our value function**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/monte-carlo-approach.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Let's consider the mouse & cheese game as an example:\n",
    "\n",
    "* We always start the episode at the same starting point \n",
    "* The agent takes actions using this policy. For instance, using an Epsilon Greedy Strategy, a policy that alternates between exploration (random actions) and exploitation.\n",
    "* On each step, we get **the reward and the next state**.\n",
    "* We terminate the episode if the cat eats the mouse or if the mouse moves > 10 steps.\n",
    "* At the end of the episode, we have a list of State, Actions, Rewards, and Next States tuples For instance [[State tile 3 bottom, Go Left, +1, State tile 2 bottom], [State tile 2 bottom, Go Left, +0, State tile 1 bottom]…]\n",
    "* The agent will **sum the total rewards** $G_{t}$ (to see how well it did).\n",
    "* It will then **update $V(S_{t})$ based on the formula**\n",
    "* **Start a new game with this new knowledge**\n",
    "\n",
    "By running more and more episodes, **the agent will learn to play better and better**.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/MC-3p.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "For instance, if we train a state-value function using Monte Carlo:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/MC-4p.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "We have a list of state, action, rewards, next_state, **we need to calculate the return $G_{t}$ from this episode**:\n",
    "\n",
    "* $G_{t} = R_{t+1} + R_{t+2} + R_{t+3} \\dots$ (for simplicity we don't discount the rewards)\n",
    "* $G_{t} = 1+0+0+0+0+0+1+1+0+0 = 3$\n",
    "\n",
    "We can now update $V(S_{0})$:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/MC-5.png\" title=\"\" alt=\"\" width=\"300\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* New $V(S_{0}) = V(S_{0}) + lr * [G_{t} - V(S_{0})]$\n",
    "* New $V(S_{0}) = 0 + 0.1 * [3-0] = 0.3$\n",
    "\n",
    "**Note:** We would repeat this process for all of the states"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - Temporal Difference Learning: learning at each step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal difference **waits for only one interaction (one step) $S_{t+1}$ to form a TD target and update $V(S_{t})$ using $R_{t+1}$ and $\\gamma * V(S_{t+1})$**.\n",
    "\n",
    "But, since we didn't experience an entire episode, we don't have $G_{t}$ (expected return). Instead, we estimate $G_{t}$ by adding $R_{t+1}$ and the discounted value of the next state. This is called bootstrapping because TD bases its update part on an existing estimate $V(S_{t+1})$ and not on a complete sample $G_{t}$.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/TD-1.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "If we take the sample example as before (mouse & cheese game):\n",
    "\n",
    "* We just started to train our value function, so it returns 0 value for each state.\n",
    "* Our learning rate (lr) is 0.1, and our discount rate is 1 (no discount).\n",
    "* Our mouse explore the environment and take a random action: **going to the left**\n",
    "* It gets a reward $R_{t+1}$ since **it eats a piece of cheese**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/TD-1p.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "We can now update $V(S_{0})$:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_1/TD-3.png\" title=\"\" alt=\"\" width=\"350\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* New $V(S_{0}) = V(S_{0}) + lr * [R_{1} + \\gamma * V(S_{1}) - V(S_{0})]$\n",
    "* New $V(S_{0}) = 0 +0.1 * [1+1*0-0] = 0.1$\n",
    "\n",
    "We just updated our value function for state 0. Now we **continue to interact with this environment with our updated value function**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
