{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
      "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, FlavaForPreTraining\n",
    "\n",
    "model = FlavaForPreTraining.from_pretrained(\"facebook/flava-full\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\"], images=image, return_tensors=\"pt\", padding=True, return_codebook_pixels=True, return_image_mask=True)\n",
    "\n",
    "# outputs = model(**inputs)\n",
    "# logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'codebook_pixel_values', 'bool_masked_pos'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of input_ids: torch.Size([1, 7])\n",
      "Type of token_type_ids: torch.Size([1, 7])\n",
      "Type of attention_mask: torch.Size([1, 7])\n",
      "Type of pixel_values: torch.Size([1, 3, 224, 224])\n",
      "Type of codebook_pixel_values: torch.Size([1, 3, 112, 112])\n",
      "Type of bool_masked_pos: torch.Size([1, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "for key, value in inputs.items():\n",
    "    print(f'Type of {key}: {value.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'codebook_pixel_values', 'bool_masked_pos'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlavaLosses(mim=None, mlm=None, itm=None, global_contrastive=tensor(0., grad_fn=<MulBackward0>), mmm_image=tensor(7.1579, grad_fn=<MulBackward0>), mmm_text=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"loss_info\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize two descriptions at the same time\n",
    "\n",
    "In Flava, there is a contrastive loss that aims to connect images with the appropriate description.\n",
    "\n",
    "However, the FLAVA model expects a single string per item. For that reason, both descriptions, the correct and the incorrect have to be easily distinguishable. The way that we can distinguish between them is via[ the token type ids](https://huggingface.co/docs/transformers/glossary#token-type-ids). Which basically assign a 0 to the tokens of the first sentence and a 1 to the tokens of the second sentence:\n",
    "\n",
    "```python\n",
    "[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n",
    "\n",
    "# encoded_dict[\"token_type_ids\"]\n",
    "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence_a = \"HuggingFace is based in NYC\"\n",
    "sequence_b = \"Where is HuggingFace based?\"\n",
    "\n",
    "encoded_dict = tokenizer(sequence_a, sequence_b)\n",
    "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n",
    "\n",
    "print(decoded)\n",
    "print(encoded_dict[\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for MLM with in FLAVA DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, AutoTokenizer\n",
    "\n",
    "model_name = \"facebook/flava-full\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability=0.2, return_tensors=\"pt\")\n",
    "\n",
    "data_collator.torch_mask_tokens(inputs=inputs['input_ids'], special_tokens_mask=inputs['special_tokens_mask'])\n",
    "\n",
    "del inputs['special_tokens_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b6d8f2ebe446c191acc408a943504f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/42.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5476416d5dce4811ba3db1511b14ef30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/19.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'fetch_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/personal-pocs/flava_pretraining/notebooks/test_flava_for_pretraining.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/personal-pocs/flava_pretraining/notebooks/test_flava_for_pretraining.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m pmd \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mload_dataset(\u001b[39m\"\u001b[39m\u001b[39mfacebook/pmd\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwit\u001b[39m\u001b[39m\"\u001b[39m, use_auth_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, streaming\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/personal-pocs/flava_pretraining/notebooks/test_flava_for_pretraining.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m pmd_train_head \u001b[39m=\u001b[39m pmd[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtake(\u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/personal-pocs/flava_pretraining/notebooks/test_flava_for_pretraining.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m pmd_train_head_with_images \u001b[39m=\u001b[39m pmd_train_head\u001b[39m.\u001b[39mmap(fetch_images, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, fn_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mnum_threads\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m20\u001b[39m})\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f34306635643839622d636361342d346634332d383530622d3436653833316566396461362f7265736f7572636547726f7570732f4275792e456e672e496e6e6f766174696f6e2e4f6d6e69636f64696e672e417a7572654d4c2f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f6f6d6e69636f64696e672f636f6d70757465732f6665722d677075/home/azureuser/cloudfiles/code/Users/Fernando.Rodriguez/personal-pocs/flava_pretraining/notebooks/test_flava_for_pretraining.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m datapoint \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(pmd_train_head_with_images))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fetch_images' is not defined"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "pmd = datasets.load_dataset(\"facebook/pmd\", \"wit\", use_auth_token=True, streaming=True)\n",
    "pmd_train_head = pmd['train'].take(2)\n",
    "pmd_train_head_with_images = pmd_train_head.map(fetch_images, batched=True, batch_size=100, fn_kwargs={\"num_threads\": 20})\n",
    "datapoint = next(iter(pmd_train_head_with_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ceb209792b4aaa88eff3e7e1fbe46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore how MLM is done with BERT\n",
    "\n",
    "FLAVA is special because it does multiple pretraining objectives at the same time. One of them is Masked Language Modelin (MLM). In order to prepare the data for MLM in FLAVA we are going to start understanding how it is done in a simpler model such as BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1996, 3007, 1997, 2605, 2003,  103, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, 3000, -100, -100]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
