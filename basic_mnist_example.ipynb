{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMZrWiTHR1YEqDdaDY/JjD6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferjorosa/learn-fastai/blob/main/basic_mnist_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1xAial2amrd"
      },
      "source": [
        "# **Basic MNIST example**\n",
        "\n",
        "The objective of this notebook is to simply put my notes and examples of the MNIST example of Chapter 4 from the Fast AI book"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pObvZ9p-aYj"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM7BtI9vahp9"
      },
      "source": [
        "#hide (Google Colab)\n",
        "!pip install fastai --upgrade -q\n",
        "import fastai\n",
        "print(fastai.__version__)\n",
        "\n",
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFKKBVgDalMd"
      },
      "source": [
        "#hide\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "\n",
        "matplotlib.rc('image', cmap='Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHXWv_3N-mfK"
      },
      "source": [
        "### **Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhQ09YLcM__"
      },
      "source": [
        "path = untar_data(URLs.MNIST_SAMPLE)\n",
        "threes = (path/'train'/'3').ls().sorted()\n",
        "sevens = (path/'train'/'7').ls().sorted()\n",
        "threes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUnRQ1Cgcfqe"
      },
      "source": [
        "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
        "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
        "len(three_tensors),len(seven_tensors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNtJwGr_cZ9G"
      },
      "source": [
        "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
        "stacked_threes = torch.stack(three_tensors).float()/255\n",
        "stacked_threes.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZOkO4Q2dUP2"
      },
      "source": [
        "valid_3_tens = torch.stack([tensor(Image.open(o)) \n",
        "                            for o in (path/'valid'/'3').ls()])\n",
        "valid_3_tens = valid_3_tens.float()/255\n",
        "valid_7_tens = torch.stack([tensor(Image.open(o)) \n",
        "                            for o in (path/'valid'/'7').ls()])\n",
        "valid_7_tens = valid_7_tens.float()/255\n",
        "valid_3_tens.shape,valid_7_tens.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4kLrBhq--fh"
      },
      "source": [
        "### **Single-batch gradient descent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oidziwAcp_P"
      },
      "source": [
        "gv('''\n",
        "init->predict->loss->gradient->step->stop\n",
        "step->predict[label=repeat]\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3u83vqEcrcP"
      },
      "source": [
        "train_x =torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
        "train_y = tensor([0] * len(stacked_threes) + [1] *len(stacked_sevens)).unsqueeze(1)\n",
        "(train_x.shape, train_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqdJprAIdCh-"
      },
      "source": [
        "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1,28*28)\n",
        "valid_y = tensor([0] * len(valid_3_tens) + [1] * len(valid_7_tens)).unsqueeze(1)\n",
        "(valid_x.shape, valid_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZHBEUhIdaTP"
      },
      "source": [
        "def init_params(size, std=1.0): return (torch.randn(size) * std).requires_grad_()\n",
        "\n",
        "def mnist_loss(predictions, targets): \n",
        "  predictions = predictions.sigmoid()\n",
        "  return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "\n",
        "def linear3(x, w): return x@w + intercept\n",
        "  \n",
        "def predict(x, w):\n",
        "  linear_predictions = model(x, w)\n",
        "  predictions = linear_predictions.sigmoid()\n",
        "  return torch.where(predictions > 0.5, 1, 0)\n",
        "\n",
        "def validate(valid_x, w, valid_y):\n",
        "  binary_preds = predict(valid_x, w)\n",
        "  return (binary_preds == valid_y).count_nonzero() / valid_y.shape[0]\n",
        "\n",
        "def epoch1(x, y, w, valid_x, valid_y, prt=True):\n",
        "  pred = model(x, w)\n",
        "  loss = mnist_loss(pred, y)\n",
        "  loss.backward()\n",
        "  if prt: print(\"Loss: \" + str(loss))\n",
        "  accuracy = validate(valid_x, w, valid_y)\n",
        "  if prt: print(\"Validation accuracy: \" + str(accuracy))\n",
        "  w.data -= w.grad.data * lr\n",
        "\n",
        "w = init_params((784, 1))\n",
        "lr = 1e-3\n",
        "model = linear3\n",
        "weights = init_params((28*28,1))\n",
        "intercept = init_params(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyom4n7idfv3"
      },
      "source": [
        "epoch1(train_x, train_y, w, valid_x, valid_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O6JAiJpdicu"
      },
      "source": [
        "n_epochs = 100\n",
        "for i in range (1, n_epochs):\n",
        "  epoch1(train_x, train_y, w, valid_x, valid_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEprVnl6rI5M"
      },
      "source": [
        "### **Stochastic gradient descent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHcA3aP6_gq5"
      },
      "source": [
        "When working with gradient descent it may be interesting to consider only a hadful of batches at each iteration. We can do this with Fast AI in a ver simple\n",
        "manner with the *DataLoader* class.\n",
        "\n",
        "In the following implementation, each epoch considers all of the data. However, **each epoch updates the parameters multiple times** by considering all of the data in batch form in random order. \n",
        "\n",
        "**Note on theory:** I am not completely sure this is the traditional way of doing SGD, but it was the intuition I obtained from reading the AI book\n",
        "\n",
        "**Note on speed:** Previous implementation is much faster. I suppose due to the absence of the multiple for loops that are here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KUmlZ9tnwXD"
      },
      "source": [
        "train_dset = list(zip(train_x, train_y))\n",
        "valid_dset = list(zip(train_x, train_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv3GAJ6VrTyi"
      },
      "source": [
        "train_dl = DataLoader(train_dset, batch_size=256, shuffle=True)\n",
        "xb, yb = first(train_dl)\n",
        "print(xb.shape, yb.shape)\n",
        "\n",
        "valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True)\n",
        "xb, yb = first(valid_dl)\n",
        "print(xb.shape, yb.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzuzDkHdtYHj"
      },
      "source": [
        "class LinearModel:\n",
        "\n",
        "  def __init__(self, intercept, weights):\n",
        "    self.intercept = intercept\n",
        "    self.weights = weights\n",
        "    self.params = (intercept, weights)\n",
        "\n",
        "  def predict(self, data):\n",
        "    return data@self.weights + self.intercept\n",
        "\n",
        "\n",
        "def mnist_loss(predictions, targets): \n",
        "  predictions = predictions.sigmoid()\n",
        "  return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "\n",
        "def calc_grad(xb, yb, model, prt=True):\n",
        "  pred = model.predict(xb)\n",
        "  loss = mnist_loss(pred, yb)\n",
        "  loss.backward()\n",
        "  if prt: print(\"Loss: \" + str(loss))\n",
        "\n",
        "def train_epoch(train_dl, model, lr, prt=True):\n",
        "  for xb, yb in train_dl: # Iterates over batches, not individual instances\n",
        "    calc_grad(xb, yb, model, prt)\n",
        "    for p in model.params:\n",
        "      p.data -= p.grad * lr\n",
        "      p.grad.zero_() # See page 172, last paragraph\n",
        "  \n",
        "def hard_predict_mnist(xb, model):\n",
        "  linear_predictions = model.predict(xb)\n",
        "  predictions = linear_predictions.sigmoid()\n",
        "  return torch.where(predictions > 0.5, 1, 0)\n",
        "\n",
        "def validate_epoch(valid_dl, model):\n",
        "  batch_accs = [(hard_predict_mnist(valid_xb, model) == valid_yb).count_nonzero() \n",
        "                / valid_yb.shape[0] for valid_xb, valid_yb in valid_dl]\n",
        "  return torch.FloatTensor(batch_accs).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Us5COdWvl5c"
      },
      "source": [
        "# Small example just to see what was happening under the hood\n",
        "# xb and yb are now batches, not individual data instances\n",
        "\n",
        "# l = []\n",
        "# for xb, yb in train_dl:\n",
        "#   l.append(xb)\n",
        "# len(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTWUzhmb5r4r"
      },
      "source": [
        "weights = init_params((28*28,1))\n",
        "intercept = init_params(1)\n",
        "linear_model = LinearModel(intercept, weights)\n",
        "\n",
        "for i in range(1, 100):\n",
        "  train_epoch(train_dl, linear_model, 1e-3, False)\n",
        "  validation_acc = validate_epoch(valid_dl, linear_model)\n",
        "  print(\"Epoch validation accuracy: \" + str(validation_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkiaUvAk96Xy"
      },
      "source": [
        "### **Creating an optimizer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jCF9qK0Dt9h"
      },
      "source": [
        "Because stochastic gradient descent is such a general foundation, PyTorch provides some useful class to make it easier to implement."
      ]
    }
  ]
}