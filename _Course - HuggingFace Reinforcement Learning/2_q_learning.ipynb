{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - What is Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is an off-policy value-based method that uses a TD approach to train its action-value function. **Q-Learning is the algorithm we use to train our Q-function**, an **action-value** function that determines the value of being at a particular state and taking a specific action at that state.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-function.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Given a state and action, our Q Function outputs a state-action value (also called Q-value)\n",
    "\n",
    "The **Q comes from \"the Quality\" (the value) of that action at that state**. Let's quickly recap the difference between value and reward:\n",
    "\n",
    "* The value of a state, or a state-action pair is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to its policy.\n",
    "* The reward is the feedback I get from the environment after performing an action at a state\n",
    "\n",
    "**Internally, our Q-function has a Q-table, a tabe where each cell corresponds to a state-action pair value.** <span style=\"color:Blue\"><b>Think of this Q-table as the memory or cheat-sheet of our Q-function.</b></span>. given a state and action, our Q-function will search inside its Q-table to output the value.\n",
    "\n",
    "Consider the following \"maze\" and Q-table as an example:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Maze-3.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "If we recap, <span style=\"color:Blue\">Q-Learning</span> is the RL algorithm that:\n",
    "\n",
    "* Trains a Q-function (an **action-value function**), which internally is a **Q-table that contains all the state-action pair values**.\n",
    "* Given a state and action, our Q-function will **search into its Q-table the corresponding value**.\n",
    "* **When the training is done, we have an optimal Q-function, which means we have an optional Q-table**.\n",
    "* And if we have an optimal Q-function, **we have an optimal policy since we know for each state what is the best action to take**.\n",
    "\n",
    "But, in the beginning, our Q-table is useless since it gives arbitrary values for each state-action pair (most of the time, we initialize the Q-table to 0). As the agent explores the environment and we update the Q-table, it will give us better and better approximations to the optimal policy:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-1.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - The Q-Learning algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand what Q-Learning, Q-function, and Q-table are, let’s dive deeper into the Q-Learning algorithm.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-2.png\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Step 1: Initialize the Q-table (usually with values of 0)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-3.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Step 2: Choose an action using epsilon-greedy strategy\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-4.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The idea is that we define the initial epsilon $\\epsilon = 1.0$:\n",
    "\n",
    "* With probability $1-\\epsilon$: we do **exploitation** (aka our agent selects the action with the highest state-action pair value).\n",
    "* With probability $\\epsilon$: we do **exploration** (aka our agent tries a random action).\n",
    "\n",
    "At the beginning of the training, **the probability of doing exploration will be huge since $\\epsilon$ is very high, so most of the time, we'll explore.** But as the training goes on, and consequently our **Q-table gets better and better in its estimations, we progressively reduce the epsilon value** since we will need less and less exploration and more exploitation.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-5.png\" title=\"\" alt=\"\" width=\"200\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Step 3: Perform action $A_{t}$, get reward $R_{t+1}$ and next state $S_{t+1}$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-6.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Step 4: Update $Q(S_{t}, A_{t})$\n",
    "\n",
    "Remember that in TD learning, we update our policy or value function (depending on the RL method we choose) **after one step of the interaction**. To produce our TD target, **we use the immediate reward $R_{t+1}$ plus the discounted value of the next best state-action pair** (we call that bootstrap). Thefore, our $Q(S_{t}, A_{t})$ update formula goes like this:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Q-learning-8.png\" title=\"\" alt=\"\" width=\"550\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "This means that to update our $Q(S_{t}, A_{t})$:\n",
    "* We need $S_{t}, A_{t}, R_{t+1}, S_{t+1}$\n",
    "* To update our Q-value at a given state-action pair, we use the TD target\n",
    "\n",
    "How do we form the TD target?\n",
    "\n",
    "1. We obtain the reward $R_{t+1}$ after taking the action.\n",
    "2. To get the **best next-state-action pair value**, we use a greedy policy to select the next best action. Note that this is not an epsilon-greedy policy, this will always take the action with the highest state-action value.\n",
    "\n",
    "Then when the update of this Q-value is done, we start in a new state and select our action **using a epsilon-greedy policy again.**. This is why we say **Q-learning is an off-policy algorithm**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Off-policy vs On-policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Off-policy. **Using a different policy for acting (inference) and updating (training)**. For instance, with Q-Learning, the epsilon-greedy policy (acting policy), is different from the greedy policy that is used to select the best next-state action value to update our Q-value (updating policy). \n",
    "\n",
    "* On-policy. **Using the same policy for acting and updating**. For insance, with [Sarsa](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action), another value-based algorithm, the **epsilon-greedy policy selects the next state-action pair, not a greedy policy.**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/off-on-4.png\" title=\"\" alt=\"\" width=\"600\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - A Q-Learning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand Q-learning, let's take a simple example:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-1.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "The reward function goes like this:\n",
    "* +0: Going to a state with no cheese in it\n",
    "* +1: Going to a state with a small cheese in it\n",
    "* +10: Going to the state with the big pile of cheese\n",
    "* -10: Going to the state with the poison and thus die\n",
    "* +0 If we spend more than five steps\n",
    "\n",
    "To train our agent to have an optimal policy (so a policy that goes right, right, down), **we will use the Q-Learning algorithm**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Timestep 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Initialize the Q-table\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Example-1.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "So, for now, **our Q-table is useless; we need to train our Q-function using the Q-Learning algorithm**.\n",
    "\n",
    "Let’s do it for 2 training timesteps:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Timestep 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Choose action using Epsilon Greedy Strategy\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-3.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Because epsilon is big = 1.0, it takes a random action, in this case, it goes right.\n",
    "\n",
    "#### Step 3: Perform action $A_{t}$, gets $R_{t+1}$ and $S_{t+1}$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-4.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "By going right, it gest a small cheese, so $R_{t+1} = 1$, and it is in a new state\n",
    "\n",
    "#### Step 4: Update $Q(S_{t}, A_{t})$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-5.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/Example-4.png\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Timestep 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Choose action using Epsilon Greedy Strategy\n",
    "\n",
    "Since epsilon is still big (i.e., 0.99), it takes a random action again. It takes the \"down\" action, which is not good since it leads to the poison\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-6.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Step 3: Perform action $A_{t}$, gets $R_{t+1}$ and $S_{t+1}$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-7.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Step 4: Update $Q(S_{t}, A_{t})$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/q-ex-8.jpg\" title=\"\" alt=\"\" width=\"500\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Since the mouse is dead, we start a new episode**. But we can see that with two exploration steps, the agent became smarter.\n",
    "\n",
    "As we continue exploring and exploiting the environment and updating Q-values using TD target, **the Q-table will give us better and better approximations. And thus, at the end of the training, we’ll get an estimate of the optimal Q-function**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Building a FrokenLake agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train our agent with the Q-learning algorithm so that it learns to navigate the Frozen Lake environment.\n",
    "\n",
    "* <a href=\"https://gymnasium.farama.org/environments/toy_text/frozen_lake/\"><span style=\"color:blue\"><b><i>FrozenLake</i></b> environment documentation</span></a>\n",
    "* <a href=\"https://gymnasium.farama.org/tutorials/training_agents/FrozenLake_tuto/\"><span style=\"color:blue\"><b>Extended version of this example</b></span></a>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images_2/frozen_lake.gif\" title=\"\" alt=\"\" width=\"250\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install a series of libraries that allow us to play the video game and obtain information regarding the environment (image and score), as well as apply actions on\n",
    "it:\n",
    "\n",
    "\n",
    "```\n",
    "pip install gymnasium\n",
    "pip install imageio\n",
    "pip install imageio_ffmpeg\n",
    "pip install gymnasium[toy-text]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world (e.g., [3,3] for the 4x4 environment).\n",
    "\n",
    "* Holes in the ice are distributed in set locations when using a pre-determined map or in random locations when a random map is generated.\n",
    "\n",
    "* The player makes moves until they reach the goal or fall in a hole.\n",
    "\n",
    "* The lake is slippery (unless disabled) so the player may move perpendicular to the intended direction sometimes (see is_slippery).\n",
    "\n",
    "* Randomly generated worlds will always have a path to the goal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 - Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment has 4 types of states:\n",
    "* **S**: The initial state.\n",
    "* **G**: the target state.\n",
    "* **F**: states through which you can walk.\n",
    "* **H**: \"hole\" states that should be avoided.\n",
    "\n",
    "The size of the map is given by its name:\n",
    "* `map_name=\"4x4\"`: a map of size 4x4.\n",
    "* `map_name=\"8x8\"`: a map of size 8x8.\n",
    "\n",
    "The environment has two modes:\n",
    "* `is_slippery=False`. The agent always moves in the desired direction (deterministic).\n",
    "* `is_slippery=True`. The agent does not always move in the desired direction due to the slippery nature of the ground (stochastic)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 - Action space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action shape is (1,) in the range {0, 3} indicating which direction to move the player.\n",
    "\n",
    "* 0: Move left\n",
    "* 1: Move down\n",
    "* 2: Move right\n",
    "* 3: Move up\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 - Rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward schedule:\n",
    "\n",
    "* Reach **goal** (**G**): +1\n",
    "* Reach **hole** (**H**): 0\n",
    "* Reach **frozen** (**F**): 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 - Episode End"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode ends if the following happens:\n",
    "\n",
    "* **Termination**:\n",
    "  * The player moves into a hole.\n",
    "  * The player reaches the goal at max(nrow) * max(ncol) - 1 (location [max(nrow)-1, max(ncol)-1]).\n",
    "<br></br>\n",
    "* **Time limit** (when using the `time_limit` wrapper):\n",
    "  * The length of the episode is 100 for 4x4 environment, 200 for 8x8 environment.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That way we are loading the default map. We could also manually create our own map the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<FrozenLakeEnv<FrozenLake-v1>>>>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Discrete(16)\n",
      "Sample observation 10\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 - Initialize Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  16  possible states\n",
      "There are  4  possible actions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")\n",
    "\n",
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros((state_space, action_space))\n",
    "    return Qtable\n",
    "\n",
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)\n",
    "Qtable_frozenlake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 - Define policies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in Q-learning we use different policies to **act** and to **update our value function**:\n",
    "* **Act**: $\\epsilon$-greedy policy \n",
    "* **Update**: greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation: take the action with the highest state, action value\n",
    "  action = np.argmax(Qtable[state][:])\n",
    "  \n",
    "  return action\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Randomly generate a number between 0 and 1\n",
    "  random_int = random.uniform(0,1)\n",
    "  # if random_int > greater than epsilon --> exploitation\n",
    "  if random_int > epsilon:\n",
    "    # Take the action with the highest value given a state\n",
    "    # np.argmax can be useful here\n",
    "    action = greedy_policy(Qtable, state)\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "  \n",
    "  return action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 - Define hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploration related hyperparamters are some of the most important ones. \n",
    "\n",
    "- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n",
    "- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space and hence can't solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 10000  # Total training episodes\n",
    "learning_rate = 0.7          # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "eval_seed = []               # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability \n",
    "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 - Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "      # Choose the action At using epsilon greedy policy\n",
    "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "      # Take action At and observe Rt+1 and St+1\n",
    "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])   \n",
    "\n",
    "      # If terminated or truncated finish the episode\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "      \n",
    "      # Our next state is the new state\n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8588f5f72c08431caab4a82645e3f7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
       "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
       "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
       "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
       "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
       "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
       "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
       "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qtable_frozenlake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 - Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very simple evaluation to verify that the agent has learned to correctly reach the target state in the environment that we have prepared for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "    \n",
    "    episode_rewards = []\n",
    "    for episode in tqdm(range(n_eval_episodes)):\n",
    "        if seed:\n",
    "            state, info = env.reset(seed=seed[episode])\n",
    "        else:\n",
    "            state, info = env.reset()\n",
    "        step = 0\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Take the action (index) that have the maximum expected future reward given that state\n",
    "            action = greedy_policy(Q, state)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "        \n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4255cdfa4a4290b9187e3a42fb7ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=1.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos nuestro agente con 100 episodios de prueba\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang-chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
