{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING TRANSFORMERS FROM SCRATCH\n",
    "\n",
    "So far we've mostly worked on data-constrained applications where the amount of labeled training data is limited. In these cases, transfer learning helped us build performant\n",
    "models.\n",
    "\n",
    "In this chapter we’ll move to the other extreme and look at what we can do when we are drowning in all the data we could possibly want. We’ll explore the pretraining step\n",
    "itself and learn how to train a transformer from scratch. In working through this problem, we’ll look at some aspects of training that we have not considered yet, such\n",
    "as the following:\n",
    "\n",
    "* Gathering and processing a very large dataset\n",
    "* Creating a custom tokenizer for our dataset\n",
    "* Training a model on multiple GPUs at scale\n",
    "\n",
    "----\n",
    "\n",
    "<span style=\"color:yellow\"><b>WARNING:</b></span> Unlike the code in the others in this book (which can be run with a Jupyter notebook on a single GPU), the training code in this chapter is designed to be run as a script with multiple GPUs. If you want to train your own version of CodeParrot, [we recommend running the script provided in the Transformers repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot).\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyword\n",
    "import torch\n",
    "import datasets\n",
    "import multiprocessing\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import IterableDataset, DataLoader, Dataset\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset as HF_Dataset\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "valid_data = load_dataset('transformersbook/codeparrot-valid', split=\"validation\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repo_name': '132nd-etcher/EMFT',\n",
       " 'path': 'emft/gui/tab_about.py',\n",
       " 'copies': '1',\n",
       " 'size': '1269',\n",
       " 'content': \"# coding=utf-8\\n\\nfrom emft.core import constant\\nfrom emft.core.logging import make_logger\\nfrom emft.gui.base import GridLayout, HSpacer, Label, VLayout, VSpacer\\nfrom emft.gui.main_ui_tab_widget import MainUiTabChild\\n\\nLOGGER = make_logger(__name__)\\n\\n\\nclass TabChildAbout(MainUiTabChild):\\n    def tab_clicked(self):\\n        pass\\n\\n    @property\\n    def tab_title(self) -> str:\\n        return 'About'\\n\\n    def __init__(self, parent=None):\\n        super(TabChildAbout, self).__init__(parent)\\n\\n        repo_label = Label(\\n            '''<a href='{link}'>{link}</a>'''.format(link=constant.LINK_REPO)\\n        )\\n        repo_label.setOpenExternalLinks(True)\\n\\n        changelog_label = Label(\\n            '''<a href='{link}'>{link}</a>'''.format(link=constant.LINK_CHANGELOG)\\n        )\\n        changelog_label.setOpenExternalLinks(True)\\n\\n        self.setLayout(\\n            VLayout(\\n                [\\n                    GridLayout(\\n                        [\\n                            [Label('Github repository: '), repo_label, HSpacer()],\\n                            [Label('Changelog: '), changelog_label, HSpacer()],\\n                        ],\\n                        [0, 0, 1]\\n                    ),\\n                    VSpacer(),\\n                ]\\n            )\\n        )\\n\",\n",
       " 'license': 'gpl-3.0'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Large datasets and where to find them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many domains where you may actually have a large amount of data at hand, ranging from legal documents to biomedical datasets to programming codebases.\n",
    "In most cases, these datasets are unlabeled, and their large size means that they can usually only be labeled through the use of heuristics, or by using accompanying\n",
    "metadata that is stored during the gathering process.\n",
    "\n",
    "**Using a pretrained model forces you to use the model’s corresponding tokenizer, but using a tokenizer that is trained on a corpus from another domain is typically suboptimal.** For example, using GPT's pretrained tokenizer on legal documents, other languages, or even completely different sequences such as musical notes or DNA sequences will result in poor tokenization (as we will see shortly).\n",
    "\n",
    "As the amount of training data you have access to gets closer to the amount of dataused for pretraining, it thus becomes interesting to consider training the model and\n",
    "the tokenizer from scratch, provided the necessary computational resources are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Challenges of building large-scale corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The quality of a model after pretraining largely reflects the quality of the pretraining corpus**. In particular, the model will inherit any defects in the pretraining corpus.\n",
    "Thus, before we attempt to create one of our own it’s good to be aware of some of the common issues and challenges that are associated with building large corpora for\n",
    "pretraining.\n",
    "\n",
    "As the dataset gets larger and larger, the chances that you can fully control—or at least have a precise idea of—what is inside it diminish. A very large dataset is much more likely thathas been created in an automatic or semiautomatic way by collecting data that is generated as a side effect of other activities. For instance, it may consist\n",
    "of:\n",
    "\n",
    "* all the documents (e.g., contracts, purchase orders, etc.) that a company stores,\n",
    "* logs from user activities, \n",
    "* data gathered from the internet\n",
    "* etc.\n",
    "\n",
    "There are several important consequences that follow from the fact that large-scale datasets are mostly created with a high degree of automation. One of them is the risk of training a model on biased and lower-quality data.\n",
    "\n",
    "[Recent investigations of famous large-scale datasets like BookCorpus and C4, which were used to train BERT and T5, respectively, have uncovered (among other things) that](https://arxiv.org/abs/2104.08758):\n",
    "* A significant proportion of the C4 corpus is machine-translated rather than translated by humans.\n",
    "* Disparate erasure of African-American English as a result of stopword filtering in C4 has resulted in an underrepresentation of such content.\n",
    "* It is typically difficult in a large text corpus to find a middle ground between including (often too much) sexually or other explicit content and totally erasing\n",
    "all mention of sexuality or gender. As a surprising consequence of this, a rather common word like **“sex”** (which can have both neutral and explicit meanings) is\n",
    "completely unknown to a tokenizer that is trained on C4, since this word is fully absent from the corpus.\n",
    "* [There are many occurrences of copyright violation in BookCorpus, and probably in other large-scale datasets as well](https://arxiv.org/abs/2105.05241).\n",
    "* There is genre skew toward \"romance\" novels in BookCorpus.\n",
    "\n",
    "Let's illustrate the notion of a model being skewed by the data by comparing text generations from GPT and GPT-2.\n",
    "* GPT was mostly trained on BookCorpus\n",
    "* GPT-2 was trained on web pages, blogs, and news articles liked from Reddit\n",
    "\n",
    "We'll compare similar-sized versions of both models on the same prompt, so that the main difference is the pretraining dataset, and we'll use the `text-generation` pipeline to investigate the model outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c81648a7ad446a8a20480a350fd2511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c633154975e144648f9950b3122e3be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542235f8cb3b43769f85f5f85e91614e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06340b64f90d495d810788b5519d17c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5caf423445bf4b1ebebe9ea204c866a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abe373972c648b2b45dcfb229bfc19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfd49fa22944c2e80255f5cfcd063a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b20c0dff254974acde4b70a37c4a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4706b150857f4c5e8d6ce475dff60029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee94b8b90ac646b3b019ba5c213e703b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a373366abdc0479d93b46b721d8e4f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc95d4950f24f058bffada758a95600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
    "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s create a simple function to count the number of parameters in each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT size: 116.5M parameters\n",
      "GPT2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "    \n",
    "print(f\"GPT size: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
    "print(f\"GPT2 size: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original GPT model is about the same size as the smallest GPT-2 model. Now we can generate three different completions from each model, each with the same input prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT completions:\n",
      "1.\n",
      "When they came back with some money. they hadn't even bothered with a check to cover the gas and the gas tank. \" \n",
      " she glanced up, her eyes sad from pain. \" i'm clendon, by the way, by the way.\n",
      "2.\n",
      "When they came back on the porch. \n",
      " \" what are you looking at? \" he pulled his sweater back down, and then pulled off his jacket. \" what's this guy doing here? \" he said, \" i 'll bet he's a\n",
      "3.\n",
      "When they came back into the den. \" i'm not telling everyone that i know where you are or that you're with them. i 'll just be giving them a warning. let's say that i need to go somewhere and i want you to\n",
      "\n",
      "GPT-2 completions:\n",
      "1.\n",
      "When they came back from the airport, their car keys were locked. The car they saw was brand new and in the trunk. They turned away and told detectives he had stolen five cars from the area.\n",
      "\n",
      "The officers pulled him over for\n",
      "2.\n",
      "When they came back for business, I'd asked what was the most important thing they had in mind when they learned they could be arrested for taking the first shot.\n",
      "\n",
      "\"Do you think the person who shot you, or someone with your\n",
      "3.\n",
      "When they came back, they found a box and a pair of shoes. The one with the socks, she found a dresser, and he went to the tailor and bought his own shoes, too. She bought two pairs which didn't make\n"
     ]
    }
   ],
   "source": [
    "def enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\n",
    "    out = pipe(prompt, num_return_sequences=num_return_sequences, clean_up_tokenization_spaces=True)\n",
    "    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
    "\n",
    "prompt = \"\\nWhen they came back\"\n",
    "print(\"GPT completions:\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))\n",
    "print(\"\")\n",
    "print(\"GPT-2 completions:\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By just sampling a handful of outputs from both models we can already see the distinctive “romance” skew in GPT generation, which will typically imagine a dialogue with a romantic interaction between a woman and a man. On the other hand, GPT-2 was trained on webtext linked to and from Reddit articles and mostly adopts a neutral “they” in its generations, which contain “blog-like” or adventure-related elements.\n",
    "\n",
    "In general, any model trained on a dataset will reflect the language bias and over- or underrepresentation of populations and events in its training data. These biases in the\n",
    "behavior of the model are important to take into consideration with regard to the target audience interacting with the model; [for some useful guidelines, we refer you to a\n",
    "paper by Google that provides a framework for dataset development](https://arxiv.org/abs/2010.13561)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - To filter the noise or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some conscious choices to be made regarding how we want the system to perform in a real-world setting. Having some noise in the training dataset will make our system more robust to noisy inputs at inference time, but will also make its predictions more random. Depending on the intended use and whole system integration, you may choose more or less noisy data and add pre- and postfiltering operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Building a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapters we've used tokenizers that accompanied the models we used. This made sense since these models were pretrained using data passed through a specific preprocessing pipeline defined in the tokenizer. When using a pretrained model, it's important to stick with the same preprocessing design choices selected for pretraining. Otherwise the model may be fed out-of-distribution patterns or unknown tokens.\n",
    "\n",
    "However, when we train a language model from scratch, using a tokenizer prepare for another dataset can be suboptimal. Here are a few examples of the kinds of problems we might run into when using an existing tokenizer:\n",
    "\n",
    "* The T5 tokenizer was trained on the [C4 corpus](https://huggingface.co/datasets/c4) that we mentioned earlier, but an extensive step of stopword filtering was used to create it. As a result, the T5 tokenizer has never seen common English words such as \"sex\".\n",
    "* The CamemBERT tokenizer was also trained on a very large corpus of text, but only comprising French text (the French subset of the [OSCAR corpus](https://huggingface.co/datasets/oscar)). As such, it is unaware of common English words such as \"being\".\n",
    "\n",
    "We can easily observe this in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc07fed5cb84d73b68e5dfc99434a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ca2c6a4fd04d4ebea7fa250db2aca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498101827e8f4ba8ac949dd6ba7bfbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/transformers/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31917908ce5944cf91c5cee45b26bf1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6808457a8b42d7987f531c90241826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edccfc12049446abac7347907c1da718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 tokens for \"sex\": ['', 's', 'ex']\n",
      "CamemBERT tokens for \"being\": ['be', 'ing']\n"
     ]
    }
   ],
   "source": [
    "def tok_list(tokenizer, string):\n",
    "    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n",
    "    return [tokenizer.decode(tok) for tok in input_ids]\n",
    "    \n",
    "tokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "tokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "print(f'T5 tokens for \"sex\": {tok_list(tokenizer_T5,\"sex\")}')\n",
    "print(f'CamemBERT tokens for \"being\": {tok_list(tokenizer_camembert,\"being\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In many cases, splitting such short and common words into subparts will be inefficient, since this will increase the input sequence length of the model** (which has limited context). Therefore, it is important to be aware of the domain and preprocessing of the dataset that was used to train the tokenizer. The tokenizer and model can encode bias from the dataset that has an impact on the downstream behaviour of the model. To create an optimal tokenizaer for our dataset, we thus need to train one ourselves. \n",
    "\n",
    "-----\n",
    "\n",
    "**Training a model** involves starting from a given set of weights and using backprpagation from an error signal on a designed objective to minimize the loss of the model and find an optimal set of weights for the model to perform the task defined by the training objective.\n",
    "\n",
    "**Training a tokenizer**, on the other hand, does not involve backpropagation or weights. It is a way to create an optimal mapping from a string of text to a list of integers that can be ingested by the model. In today's tokenizers, the optimal string-to-integer conversion involves a vocabulary consisting of a list of atomic strings and an associated method to convert, normalize, cut, or map a text string into a list of indices with this vocabulary. This list of indices is then the input for our neural network.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - The Tokenizer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in Chapter 4, the tokenizer is a processing pipeline consisting of four steps:\n",
    "* Normalization\n",
    "* Pretokenization\n",
    "* Tokenizer model\n",
    "* Postprocessing\n",
    "\n",
    "The part of the tokenizer pipeline that can be trained on data is the tokenizer model. As we discussed in Chapter 2, there are several subword tokenization algorithms that can be used, such as BPE, WordPiece, and Unigram:\n",
    "\n",
    "* **BPE** starts from a list of basic units (single characters) and creates a vocabulary by a process of progressively creating new tokens formed by merging the most frequently co-occurring basic unit and adding them to the vocabulary. This process is reiterated until a predefined vocabulary size is reached.\n",
    "* **Unigram** starts from the other end, by initializing its base vocabulary with all the words in the corpus and potential subwords. Then, it progressibely removes or splits the less useful tokens to obtain a smaller and smaller vocabulary, until the target vocabulary is reached.\n",
    "* **WordPiece** is a predecessor of Unigram, and its official implementation was never open-sourced by Google.\n",
    "\n",
    "The impact of these various algorithms on downstream performance varies depending on the task, and overall it’s quite difficult to identify if one algorithm is clearly\n",
    "superior to the others. Both BPE and Unigram have reasonable performance in most cases, but let’s have a look at some aspects to consider when evaluating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Measuing Tokenizer performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optmimality and performance of a tokenizer are challenging to measure in practice. Some possible metrics include:\n",
    "* **Subword feritlity**, which calculates the average number of subwords produced per tokenized word.\n",
    "* **Proportion of continued words**, which refers to the proportion of tokenized words in a corpus that are split into at least two subtokens.\n",
    "* **Coverage metrics** like the proportion of unknown words or rarely used tokens in a tokenized corpus.\n",
    "\n",
    "In addition, robustness to misspelling or noise is often estimated, as well as model performance on such out-of-domain examples, as this strongly depends on the tokenization process.\n",
    "\n",
    "These measures give a set of different views on the tokenizer's performance, but they tend to ignore the interaction of the tokenizer with the model. For example, subword fertility can be minimized by including all the possible words in the vocabulary, but this will produce a very large vocabulary for the model.\n",
    "\n",
    "In the end, the performance of the various tokenization approaches is thus generally best estimated by using the downstream performance of the model as the ultimate metric. For instance, the good performance of early BPE approaches was demonstrated by showing improved performance on machine translation tasks by models trained on these tokenizers and vocabularies instead of character- or word-based tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Building a custom tokenizer for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a custom tokenizer for our use case: **tokenizing Python code**. The question of pretokenization merits some discussion for programming languages. If we split on whitespaces and remove them, we will lose all the indentation information, which in Python is important for the semantics of the program (just think about `while` loops, or `if-then-else` statements). On the other hand, line breaks are not meaningful and can be added or removed without impac on the semantics. Similarly, splitting on punctuation, like an underscore, which is used to compose a single variable name from several subparts, might not make as much sense as it would in natural language.\n",
    "\n",
    "**Using a natural language pretokenizer for tokenizing code thus seems potentially suboptimal**. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ċ', 'def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġsay', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "python_code = r\"\"\"\n",
    "def say_hello():\n",
    "    print(\"Hello, World!\")\n",
    "    # Print it\n",
    "    say_hello()\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite strange output, so let's try to understand what is happening here by running the various submodules of the tokenizer's pipeline. First let's see what normalization is applied in this tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the GPT-2 tokenizer uses no normalization. It works directly on the raw Unicode inputs without any normalization steps. Let’s now take a look at the pretokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ċ', (0, 1)), ('def', (1, 4)), ('Ġsay', (4, 8)), ('_', (8, 9)), ('hello', (9, 14)), ('():', (14, 17)), ('ĊĠĠĠ', (17, 21)), ('Ġprint', (21, 27)), ('(\"', (27, 29)), ('Hello', (29, 34)), (',', (34, 35)), ('ĠWorld', (35, 41)), ('!\")', (41, 44)), ('ĊĠĠĠ', (44, 48)), ('Ġ#', (48, 50)), ('ĠPrint', (50, 56)), ('Ġit', (56, 59)), ('ĊĠĠĠ', (59, 63)), ('Ġsay', (63, 67)), ('_', (67, 68)), ('hello', (68, 73)), ('()', (73, 75)), ('Ċ', (75, 76))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all these `Ġ` symbols, and what are the numbers accompanying the tokens? Let's explain both and see if we can understand better how this tokenizer works.\n",
    "\n",
    "Let's start with the numbers. 🤗 Tokenizers has a very useful feature for switching between strings and tokens, called *offset tracking*. All the operations on the input string are tracked so that it's possible to know exactly what part of the input string a token after tokenization corresponds to. These numbers simply indicate where in the original string each token comes from; for instance, the word 'hello' in the first line corresponds to the characters 8 to 13 in the original string. If some characters are removed in a normalization step, we are thus still able to associate each token with the respective part in the original string.\n",
    "\n",
    "The other curious feature of the tokenized text is the odd-looking characters, such as `Ċ` and `Ġ`. Byte-level means that this tokenizer woks on bytes instead of Unicode characters. Each Unicode character is composed of between 1 and 4 bytes, depending on the character. The nice thing about bytes is that while there are 143,859 Unicode characters in the Unicode alphabet, there are only 256 elements in the byte alphabet, and you can express each Unicode character as a sequence of these bytes. If we work on bytes we can thus express all the strings composed from the UTF-8 world as longer strings in this alphabet of 256 values. That is, we can have a model using an alphabet of only 256 words and be able to process any Unicode string.\n",
    "\n",
    "Let's have a look at what the byte representations of some characters look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`a` is encoded as `b'a'` with a single byte: 97\n",
      "`€` is encoded as `b'\\xe2\\x82\\xac'` with three bytes: [226, 130, 172]\n"
     ]
    }
   ],
   "source": [
    "a, e = u\"a\", u\"€\"\n",
    "byte = ord(a.encode(\"utf-8\"))\n",
    "print(f'`{a}` is encoded as `{a.encode(\"utf-8\")}` with a single byte: {byte}')\n",
    "byte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\n",
    "print(f'`{e}` is encoded as `{e.encode(\"utf-8\")}` with three bytes: {byte}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you might wonder: **why work on a byte level?**\n",
    "\n",
    "We could decide to build our vocabulary from the 143,859 Unicode characters, but we would also like to include words (i.e., combinations of Unicode characters—in our vocabulary), so this (already very large) size is only a lower bound for the total size of the vocabulary. **This will make our model’s embedding layer very large because it comprises one vector for each vocabulary token**.\n",
    "\n",
    "On the other extreme, if we only use the 256 byte values  as our vocabulary, the input sequences will be segmented in many small pieces (i.e., similar to character level tokenization), and as such **our model will have to work on long inputs and spend significant compute power on reconstructing Unicode characters from their separate bytes, and the words from these characters**. [This approach was studied in the ByT5 paper](https://arxiv.org/abs/2105.13626).\n",
    "\n",
    "A middle-ground solution is to construct medium-sized vocabulary by extending the 256-word vocabulary with the most common combinations of bytes. This is the approach taken by the BPE algorithm. The idea is to progressively construct a vocabulary of a predefined size by creating new vocabulary tokens through iteratively merging the most frequently co-occurring pair of tokens in the vocabulary. For instance, if `t` and `h` occur very frequently together, like in English, we'll add a token `th` to the vocabulary to model this pair of tokens instead of keeping them separated. The `t` and `h` tokens are also kept in the vocabulary to tokenize instances where they do not occur together.\n",
    "\n",
    "**There is just one issue when using a typical BPE algorithm in NLP. These algorithms are designed to work with clean Unicode strings as inputs, not bytes**, and expect regular ASCII characters in the inputs, without spaces or control characters. But in the Unicode characters corresponding to the 256 first bytes, **there are many control characters** (newline, tab, escape, line feed, and other nonprintable characters). To overcome this problem, the GPT-2 tokenizer first maps all the 256 input bytes to Unicode strings that can easily be digested by the standard BPE algorithms. That that is, we will map our 256 elementary values to Unicode strings that all correspond to standard printable Unicode characters.\n",
    "\n",
    "It’s not very important that these Unicode characters are each encoded with 1 \"byte\" (i.e., Unicode string after the mapping) or more; what is important is that we have 256 single values at the end, forming our base vocabulary, and that these 256 values are correctly handled by our BPE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our base vocabulary: 256\n",
      "First element: `!`, last element: `Ń`\n"
     ]
    }
   ],
   "source": [
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "print(f'Size of our base vocabulary: {len(base_vocab)}')\n",
    "print(f'First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an example of a Unicode character that is represented by more than one \"byte\" in the following tokenization example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ä', 'ł', 'Ġdef']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"Ġ def\").tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Ġ` symbol is a special token in BPE that represents a space. Therefore, when the text contains the Ġ Unicode string, it is represent by 2 \"bytes\" that have been mapped to other \"special\" Unicode strings. In this case, `Ä`, `ł`. The `Ġ` symbol has a control character in its composition (i.e., a nonbreakable space), represented by the `ł` string.\n",
    "\n",
    "**Examples of character mappings in BPE:**\n",
    "\n",
    "| Description                                    | Character | Bytes     | Mapped bytes |\n",
    "|-----------------------------------------------|-----------|-----------|--------------|\n",
    "| Regular characters `a` and `?`                | `a`       | 97        | `a`          |\n",
    "|                                                | `?`       | 63        | `?`          |\n",
    "| A nonprintable control character (carriage return) | `U+000D`  | 13        | `č`          |\n",
    "| A space                                        | ` `       | 32        | `Ġ`          |\n",
    "| A nonbreakable space                          | `\\xa0`    | 160       | `ł`          |\n",
    "| A newline character                            | `\\n`      | 10        | `Ċ`          |\n",
    "\n",
    "We could have used a more explicit conversion, like mapping newlines to a `NEWLINE` string, but **BPE algorithms are typically designed to work on characters**. For this reason, keeping one Unicode character for each byte character is easier to handle with an out-of-the-box BPE algorithm. Now that we have been introduced to the dark magic of Unicode encodings, we can understand our tokenization conversion a bit better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ċ', (0, 1)), ('def', (1, 4)), ('Ġsay', (4, 8)), ('_', (8, 9)), ('hello', (9, 14)), ('():', (14, 17)), ('ĊĠĠĠ', (17, 21)), ('Ġprint', (21, 27)), ('(\"', (27, 29)), ('Hello', (29, 34)), (',', (34, 35)), ('ĠWorld', (35, 41)), ('!\")', (41, 44)), ('ĊĠĠĠ', (44, 48)), ('Ġ#', (48, 50)), ('ĠPrint', (50, 56)), ('Ġit', (56, 59)), ('ĊĠĠĠ', (59, 63)), ('Ġsay', (63, 67)), ('_', (67, 68)), ('hello', (68, 73)), ('()', (73, 75)), ('Ċ', (75, 76))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recognize the newlines, which as we now know are mapped to `Ċ`, and the spaces, mapped to `Ġ`. We also see that:\n",
    "\n",
    "* Spaces, and in particular consecutive spaces, are conserved (for instance, the three spaces in `ĊĠĠĠ`).\n",
    "* Consecutive spaces are considered as a single word.\n",
    "* Each space preceding a word is attached to and considered a part of the subsequent word (e.g., in `Ġsay`).\n",
    "\n",
    "Let's now experiment with the BPE model. As we've mentioned, it's in charge of splitting the words into subunits until all subunits belong to the predefined vocabulary.\n",
    "\n",
    "**The vocabulary of our GPT-2 tokenizer comprises 50,257 words**:\n",
    "\n",
    "* The base vocabulary with the 256 bytes (which allows us to build any Unicode of the 143,859 Unicode characters in the Unicode alphabet)\n",
    "* 50,000 additional tokens created by repeatedly merging the most commonly occurring tokens\n",
    "* A special character added to the vocabulary to represent document boundaries\n",
    "\n",
    "We can easily check that by looking at the length attribute of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of the vocabulary: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the full pipeline on our input code gives us the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ċ', 'def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġsay', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the BPE tokenizer keeps most of the words but will split the multiple spaces of our indentation into several consecutive spaces. **This happens because this tokenizer is not specifically trained on code**, but mostly on texts where consecutive spaces are rare. The BPE model thus doesn’t include a specific token in the vocabulary for indentation. This is a case where the tokenizer model is poorly suited for the dataset’s domain. As we discussed earlier, **the solution is to retrain the tokenizer on the target corpus**. So let’s get to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Training a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrain our byte-level BPE tokenizer on a slice of our corpus to get a vocabulary better adapted to Python code. Retraining a tokenizer provided by 🤗 Transformers is simple. We just need to:\n",
    "\n",
    "* Specify our target vocabulary size.\n",
    "* Prepare an iterator to supply lists of input strings to process to train the tokenizer's model.\n",
    "* Call the `train_new_from_iterator()` method.\n",
    "\n",
    "Unlie deep learning models, which are often expected to memorize a lot of specific details from the training corpus, <span style=\"color:blue\"><b>tokenizers are really just trained to extract the main statistics.</b> In a nutshell, the tokenizer is just trained to know which letter combinations are the most frequent in our corpus.</span> \n",
    "\n",
    "Therefore, you don't necessarily need to train your tokenizer on a very large corpus; the corpus just needs to be representative of your domain and big enough for the tokenizer to extract statistically significant measure. But depending on the vocabulary size and the exact texts in the corpus, the tokenizer can end up storing unexpected words. We can see this, for instance, when looking at the longest words in the vocabulary of the GPT-2 tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ', 35496)\n",
      "('Ġ=================================================================', 38093)\n",
      "('Ġ----------------------------------------------------------------', 16529)\n",
      "('................................................................', 23193)\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "# This returns the token itself and the index in the vocabulary, the higher, the lesser frequent\n",
    "for t in tokens[:4]: \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tokens look like separator lines that are likely to be used in forums. This makes sense since GPT-2 was trained on a corpus centered around Reddit. Now let's have a look at the last words that were added to the vocabulary, and thus the least frequent ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|endoftext|>', 50256)\n",
      "('Ġgazed', 50255)\n",
      "('Ġinformants', 50254)\n",
      "('ĠCollider', 50253)\n",
      "('Ġregress', 50252)\n",
      "('ominated', 50251)\n",
      "('Ġamplification', 50250)\n",
      "('Compar', 50249)\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "for t in tokens[:8]: \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first token, `<|endoftext|>`, is the special token used to specify the end of a text sequence and was added after the BPE vocabulary was built. \n",
    "\n",
    "For each of these tokens our model will have to learn an associated word embedding, and we probably don't want the embeding matrix to contain too many noisy words. Also note how some very time- and space-specific knowledge of the world (e.g., proper nouns like `informants` and `amplification`) is embedded at a very low level in our modeling approach by these words being granted separate tokens with associated vectors in the vocabulary. The creation of such specific tokens by a BPE tokenizer can be an indication that the target vocabulary size is too large or that the corpus contains idiosyncratic tokens.\n",
    "\n",
    "Let's train a fresh tokenizer on our corpus and examine its learned vocabulary. Since we just need a corpus reasonably representative of our dataset statistics, let's select a subset of the data (e.g., 1-2GB of data, or about 100,000 documents from our corpus):\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** For test purposes, we are going to select an even smaller dataset, with around 1000 documents\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c918c96ad8264363989fc1b7f066aeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b3122e28b64afca0bb411578d6d7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc84ae184cf94bbe8081dda80374f1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def batch_iterator(length, batch_size=10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
    "\n",
    "dataset_name = 'transformersbook/codeparrot-train'\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "iter_dataset = iter(dataset)\n",
    "\n",
    "length = 1000\n",
    "vocab_size = 2500\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(length), vocab_size=vocab_size, initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate the first and last words created by our BPE algorithm to see how relevant our vocabulary is. We skip the 256 byte tokens and look at the first tokens added thereafter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ĠĠ', 257)\n",
      "('ĠĠĠĠ', 258)\n",
      "('ĠĠĠ', 259)\n",
      "('ĊĠĠĠĠ', 260)\n",
      "('se', 261)\n",
      "('re', 262)\n",
      "('in', 263)\n",
      "('on', 264)\n",
      "('te', 265)\n",
      "('ĊĠĠĠĠĠĠĠ', 266)\n",
      "('ĠĠĠĠĠĠĠĠ', 267)\n",
      "('ĊĠĠĠ', 268)\n",
      "('st', 269)\n",
      "('or', 270)\n",
      "('de', 271)\n",
      "('le', 272)\n",
      "('th', 273)\n",
      "('Ġ=', 274)\n",
      "('lf', 275)\n",
      "('al', 276)\n",
      "('self', 277)\n",
      "('me', 278)\n",
      "('ti', 279)\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "for t in tokens[257:280]: \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see various standard levels of indentation and whitespace tokens, as well as short common Python keywords like `self`, `or` and `in`. This is a good sign that our BPE algorithm is working as intended. Now let's check out the last words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ĠWITHOUT', 2488)\n",
      "('39', 2489)\n",
      "('69', 2490)\n",
      "('Option', 2491)\n",
      "('handler', 2492)\n",
      "('patch', 2493)\n",
      "('ĊĠĠĠĠĊĠĠĠ', 2494)\n",
      "('serialize', 2495)\n",
      "('Ġwrite', 2496)\n",
      "('utf', 2497)\n",
      "('Ġlook', 2498)\n",
      "('system', 2499)\n"
     ]
    }
   ],
   "source": [
    "for t in tokens[-12:]: \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are still some relatively common words like `serialize`, `system` and `write`, but there are also random numbers like `39` and `69`.\n",
    "\n",
    "We can now tokenize our simple example of Python code to see how our tokenizer is behaving on a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def say_hello():\n",
      "    print(\"Hello, World!\")\n",
      "    # Print it\n",
      "    say_hello()\n",
      "\n",
      "['Ċ', 'def', 'Ġs', 'ay', '_', 'h', 'el', 'lo', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'H', 'el', 'lo', ',', 'ĠW', 'or', 'ld', '!', '\")', 'ĊĠĠĠ', 'Ġ#', 'ĠP', 'rint', 'Ġit', 'ĊĠĠĠ', 'Ġs', 'ay', '_', 'h', 'el', 'lo', '()', 'Ċ']\n",
      "['Ċ', 'def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġsay', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(python_code)\n",
    "print(new_tokenizer(python_code).tokens())\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though they are not code keywords, it’s a little annoying to see common English words like World or say being split by our tokenizer, since we’d expect them to occur\n",
    "rather frequently in the corpus. **On the positive side, compared to GPT2, we can see that our tokenizer conviently kept the indents in the vocabulary.**\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** This is especially apparent due to our even smaller sample of data that was used to train the tokenizer\n",
    "\n",
    "----\n",
    "\n",
    "Let’s check if all the Python reserved keywords are in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 36 Python keywords.\n",
      "No, keyword `__peg_parser__` is not in the vocabulary\n",
      "No, keyword `async` is not in the vocabulary\n",
      "No, keyword `await` is not in the vocabulary\n",
      "No, keyword `break` is not in the vocabulary\n",
      "No, keyword `continue` is not in the vocabulary\n",
      "No, keyword `elif` is not in the vocabulary\n",
      "No, keyword `except` is not in the vocabulary\n",
      "No, keyword `finally` is not in the vocabulary\n",
      "No, keyword `global` is not in the vocabulary\n",
      "No, keyword `nonlocal` is not in the vocabulary\n",
      "No, keyword `raise` is not in the vocabulary\n",
      "No, keyword `while` is not in the vocabulary\n",
      "No, keyword `yield` is not in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "print(f'There are in total {len(keyword.kwlist)} Python keywords.')\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer.vocab:\n",
    "        print(f'No, keyword `{keyw}` is not in the vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that several quite frequent keywords, like `elif`, `while`, `yield`, etc. are not in the vocabulary either. Let's try building a larger vocabulary using a larger sample of our dataset. For instance, we can build a vocabulary of 5096 words (multiples of 8 are better for some efficient GPU/TPU computations) and train the tokenizer on a twice as large slice of our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fba6ba8720491da1ac550830871270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "length = 10000\n",
    "vocab_size = 5096\n",
    "new_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(length), vocab_size=vocab_size, initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don’t expect the most frequent tokens to change much when adding more documents, but let’s look at the last tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('parameter', 5084)\n",
      "('PARAM', 5085)\n",
      "('morphic', 5086)\n",
      "('break', 5087)\n",
      "('ence', 5088)\n",
      "('ĠIO', 5089)\n",
      "('Ġpickle', 5090)\n",
      "('DATE', 5091)\n",
      "('New', 5092)\n",
      "('vas', 5093)\n",
      "('Plugin', 5094)\n",
      "('builtin', 5095)\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "for t in tokens[-12:]: \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief inspection doesn’t show any regular programming keywords here, which is promising. Let’s try tokenizing our sample code example with the new larger tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def say_hello():\n",
      "    print(\"Hello, World!\")\n",
      "    # Print it\n",
      "    say_hello()\n",
      "\n",
      "['Ċ', 'def', 'Ġs', 'ay', '_', 'h', 'ello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'H', 'ello', ',', 'ĠW', 'or', 'ld', '!', '\")', 'ĊĠĠĠ', 'Ġ#', 'ĠP', 'rint', 'Ġit', 'ĊĠĠĠ', 'Ġs', 'ay', '_', 'h', 'ello', '()', 'Ċ']\n",
      "['Ċ', 'def', 'Ġs', 'ay', '_', 'h', 'el', 'lo', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'H', 'el', 'lo', ',', 'ĠW', 'or', 'ld', '!', '\")', 'ĊĠĠĠ', 'Ġ#', 'ĠP', 'rint', 'Ġit', 'ĊĠĠĠ', 'Ġs', 'ay', '_', 'h', 'el', 'lo', '()', 'Ċ']\n",
      "['Ċ', 'def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġsay', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(python_code)\n",
    "print(new_tokenizer_larger(python_code).tokens())\n",
    "print(new_tokenizer(python_code).tokens())\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here also the indents are conveniently kept in the vocabulary, and we see that common english words like `Hello` are getting merged, although not to the degree we would like (`Hello` should probably be stored as a single token). This occurss because we have a very small sample of the data.\n",
    "\n",
    "Let’s investigate the common Python keywords, as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, keyword `__peg_parser__` is not in the vocabulary\n",
      "No, keyword `async` is not in the vocabulary\n",
      "No, keyword `await` is not in the vocabulary\n",
      "No, keyword `continue` is not in the vocabulary\n",
      "No, keyword `finally` is not in the vocabulary\n",
      "No, keyword `nonlocal` is not in the vocabulary\n",
      "No, keyword `while` is not in the vocabulary\n",
      "No, keyword `yield` is not in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer_larger.vocab:\n",
    "        print(f'No, keyword `{keyw}` is not in the vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still missing some common keywords like `yield` and `while`, but others like `elif` have been finally included. After this manual inspection, our larger tokenizer seems better adapted for our task, but as we mentioned earlier, objectively evaluating the performance of a tokenizer is a challenging task without measuing the model's performance. \n",
    "\n",
    "We will proceed with this one and train a model to see how well it works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why not use Python's built-in `tokenize` module?\n",
    "\n",
    "Python has a built-in `tokenize` module that splits Python code strings into meaningful units (code operation, comments, indent and dedent, etc.). \n",
    "\n",
    "One issue with using this approach is that **this pretokenizer is Python-based and as such is typically rather slow and limited by the Python global interpreter lock (GIL)**. \n",
    "\n",
    "On the other hand, most of the tokenizers in the 🤗 Transformers library are provided by the 🤗 Tokenizers library and are coded in Rust. **The Rust tokenizers are many orders of magnitude faster to train and to use**, and we will thus likely want to use them given the size of our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training a model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll decide which architecture works best for the task, initialize a fresh model without pretrained weights, set up a custom data loading class, and create a scalable training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - A tale of pretraining objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have access to a \"large-scale\" pretraining corpus and an efficient tokenizer, we can start thinking about how to pretrain a transformer model.\n",
    "\n",
    "With such a large codebase consisting of code snippets like the one shown in the following Figure, we can tackle several tasks.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img title=\"\" src=\"images_ch10/data_example_pretraining.PNG\" alt=\"\" height=\"100\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Which one we choose will influence our choice of pretraining objectives. Let’s have a look at three common tasks:\n",
    "\n",
    "* Causal language modeling\n",
    "* Masked language modeling\n",
    "* Sequence-to-sequence training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 - Causal language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural task with textual data is to provide a model with the beginning of a code sample and ask to generate possible completions. This is a self-supervised training objective in which we can use the dataset without annotations.\n",
    "\n",
    "**In causal language modeling, the future tokens are masked and the model has to predict them.**\n",
    "\n",
    "A decoder-only architecture such as the GPT family of models is usually best suited for this task.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img title=\"\" src=\"images_ch10/causal_language_modeling_example.PNG\" alt=\"\" height=\"100\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 - Masked language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related but slightly different task is to provide a model with a noisy code sample, for instance with a code instruction replaced by a random or masked word, and ask it to reconstruct the original clean sample.\n",
    "\n",
    "**In masked language modeling some of the input tokens are either masked or replaced, and the model’s task is to predict the original tokens.**\n",
    "\n",
    "A encoder-only architecture such as the BERT family of models is usually trained this task (also the encoder part of an encoder-decoder model).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img title=\"\" src=\"images_ch10/masked_language_modeling_example.PNG\" alt=\"\" height=\"100\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "This is also a self-supervised training objective and is commonly called *masked language modeling* or the *denoising objective*. It's harder to think about a downstream task directly related to denoising, but denoising is generally a good pretraining task to learn general representations for later downstream tasks.\n",
    "\n",
    "Many of the decoder-only models (like BERT, and XLM-RoBERTa) are pretrained that way. Training a masked language model on a large corpus can thus be combined with fine-tuning the model on a downstream task with a limited number of labeled examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 - Sequence-to-sequence training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative task is to use a heuristic like regular expressions to separate comments or docstrings from code and build a large-scale dataset of `(code, comments)` pairs that can be used as an annotated dataset. The training task is then a supervised training objective in which one category (e.g., `comment`) is used as input for the model and the other category (e.g., `code`) is used as labels. This is a case of *supervised learning* with (input, labels) pairs, as highlighted in the following Figure:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img title=\"\" src=\"images_ch10/sequence_to_sequence_example.PNG\" alt=\"\" height=\"200\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "With a large, clean, and diverse dataset as well as a model with sufficient capacity, we can try to train a model that learns to translate comments into code or vice versa. A downstream task directly related to this supervised training task is then documentation generation from code or code generation from documentation, depending on how we set our input/outputs. \n",
    "\n",
    "In this setting a sequence is translated into another sequence, which is where **encoder-decoder architectures such as T5, BART, and PEGASUS shine**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Since we want to build a code autocompletion model, <b>we’ll select the first objective and choose a GPT architecture for the task</b>. So let’s initialize a fresh GPT-2 model!</span>\n",
    "\n",
    "Since are training a model from scratch, we won't use the `from_pretrained()` method to load a model but instead we will initalize a new model. We will, however load the configuration of `gpt2` so that we use the same hyperparameters and only adapt the vocabulary size for the new tokenizer. We then initialize a new model with this configuration with the `from_config()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 89.8M parameters\n"
     ]
    }
   ],
   "source": [
    "# We load a fresh instance of the GPT2 model and adjust its vocabulary to that of our previously learned tokenizer\n",
    "model_ckpt = \"gpt2\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_ckpt) # Base tokenizer\n",
    "tokenizer = new_tokenizer_larger # Previously learned tokenizer (specific for Python code)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ckpt, vocab_size=len(tokenizer))\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "print(f'GPT-2 size: {model_size(model)/1000**2:.1f}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how changing the tokenizer we use has consequences in the number of model parameters (due to the vocabulary size differences). For instance, since the vocabulary of our previously learned tokenizer is much shorter, the embedding layer has fewer parameters and thus the model as a whole is smaller in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Implementing the `InfiniteConstantLenghtDataset` & `ConstantLengthDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to train with maximal efficiency, we will want to supply our model with sequences filling its context. For example, if the context length of our model is 1024 tokens, we always want to provide 1024-token sequences during training. But some of our code examples might be shorter or longer than 1024 tokens.\n",
    "\n",
    "To feed batches with full sequences of `sequence_length` to our model (i.e., length of 1024 in case of GPT-2), we should either drop the last incomplete sequence or pad it. So, for example, if each example corresponds to the code of a Python file, we would pass it to the tokenizer, tokenize it and either drop the last part if the number of generated tokens is larger than the `sequence_length`, or pad it until `sequence_length` if it is shorter. However, this has two main inconvenients:\n",
    "* It makes our training less computationally efficient (i.e., we need to use an attention mask to ignore padding tokens)\n",
    "* It introduces bias towards shorter files because in longer ones we are always dropping the \"last parts\".\n",
    "\n",
    "For these reasons, we can use a little trick to avoid dropping the last parts of files and also to avoid needing to pad short files. The idea would be to create a \"buffer\", which we would fill with samples (i.e., Python files) until a specified number of characters is reached. After this, we would tokenize the whole buffer and concatenate the individual sequences with the `EOS` token. Thus generating a very long sequence. Finally, we split this long sequence into equally sized chunks (of `sequence_lenght`) as shown in the Figure below. With this approach, we lose at most a small fraction of the characters at the end of the last Python file being concatenated.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img title=\"\" src=\"images_ch10/data_preparation.PNG\" alt=\"\" height=\"300\" data-align=\"center\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Now, the only thing left would be to decide the size of this buffer. The longer this buffer is, the fewer information we would obviously lose. However, it would also mean, more information in memory (remember that the dataset we are working with is really big and it is loaded in `streaming=True` mode).\n",
    "\n",
    "For convenience, we can estimate an approximate buffer size from the number of sequences we want to store in memory and the `sequence_length` of the model we want to train:\n",
    "\n",
    "```python\n",
    "\n",
    "buffer_characters = number_of_sequences * sequence_length * characters_per_token\n",
    "\n",
    "```\n",
    "\n",
    "where:\n",
    "\n",
    "* `buffer_characters` is the number of characters in our buffer.\n",
    "* `number_of_sequences` is the number of (truncated) sequences we would like from our tokenizer.\n",
    "* `sequence_length` is the number of tokens per sequence returned by the tokenizer (e.g., 1024 for GPT2).\n",
    "* `characters_per_token` is the average number of characters per token (needs to be estimated before hard)\n",
    "\n",
    "Now, let's say we have estimated the average number of characters per token to be `3.5`. If we want to generate 100 sequences of 1024 tokens, we would need a buffer o `10` * `1024` * `3.1` = `31744`.\n",
    "\n",
    "----\n",
    "\n",
    "**Note:** There two special cases where this approach could be \"problematic\".\n",
    " \n",
    "1. **If documents are really long**, so much that each of them are longer than the buffer_characters size, we would be losing information every time, unless we would increase the buffer size accordingly. \n",
    "2. Maybe not all documents are really long, but if we are unlucky and the last document is appended to the buffer but only a small set of characters are left in it. In that case we would lose most of the information in that document. It was unlucky to be selected.\n",
    "\n",
    "If these issues are very common in our data, we may need to do some extra adjustments to our strategy.\n",
    "\n",
    "----\n",
    "\n",
    "To apply this approach for our case, let's first estimate the average character length per token in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the average number of characters per token in the 'transformersbook/codeparrot-train' dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195bd6c82e804782b54d1abe37618000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5e02c367274baa96e8e377b1c125e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3044 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11763181026712\n"
     ]
    }
   ],
   "source": [
    "def calculate_characters_per_token(dataset_name, tokenizer, examples=500):\n",
    "    \"\"\"\n",
    "    Calculate the average number of characters per token in the specified dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        examples (int): The number of examples to use for calculation.\n",
    "\n",
    "    Returns:\n",
    "        float: The average number of characters per token.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating the average number of characters per token in the '{dataset_name}' dataset\")\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "    total_characters, total_tokens = 0, 0\n",
    "\n",
    "    for _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n",
    "        total_characters += len(example['content'])\n",
    "        total_tokens += len(tokenizer(example['content']).input_ids)\n",
    "\n",
    "    characters_per_token = total_characters / total_tokens\n",
    "    return characters_per_token\n",
    "\n",
    "dataset_name = 'transformersbook/codeparrot-train'\n",
    "characters_per_token = calculate_characters_per_token(dataset_name, new_tokenizer_larger, 500)\n",
    "print(characters_per_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 - `InfiniteConstantLenghtDataset`\n",
    "\n",
    "**This is an infinite dataset**. When learning with it, we should not use epochs, but steps instead...\n",
    "\n",
    "With that we have all that's needed to create our own `IterableDataset` (which is a helper class provided by PyTorch) for preparing constant-length inputs for the model. We just need to inherit from `IterableDataset` and set up the `__iter__()` function that yields the next element with the logic we just walked through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteConstantLengthDataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        dataset,\n",
    "        text_attribute_name,\n",
    "        seq_length=1024,\n",
    "        num_of_sequences=1024,\n",
    "        chars_per_token=3.6,\n",
    "        debug=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a custom dataset for constant-length sequences for language modeling.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (transformers.PreTrainedTokenizer): A tokenizer object for text preprocessing.\n",
    "            dataset (torch.utils.data.Dataset): An existing dataset containing text examples.\n",
    "            seq_length (int): The desired sequence length for language modeling.\n",
    "            num_of_sequences (int): The number of sequences to concatenate.\n",
    "            chars_per_token (float): Average number of characters per token in the text data.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id  # End-of-sequence token ID\n",
    "        self.dataset = dataset\n",
    "        self.text_attribute_name = text_attribute_name  # name of the \"column\" that contains the text in the HF dataset (e.g., \"content\", \"text\", etc)\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences# buffer size\n",
    "        self.debug = debug\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterator function for the dataset.\n",
    "        \"\"\"\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                # Check if the buffer has reached the desired character limit\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    if self.debug:\n",
    "                        print(f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\")\n",
    "\n",
    "                    break\n",
    "                try:\n",
    "                    if self.debug:\n",
    "                        print(f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\")\n",
    "\n",
    "                    # Append the content of the next example to the buffer\n",
    "                    buffer.append(next(iterator)[self.text_attribute_name])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    # If there are no more examples, reset the iterator to reuse the dataset\n",
    "                    if buffer_len > 0:\n",
    "                        iterator = iter(self.dataset)\n",
    "                    # Unless the buffer has size 0, when we stop to avoid infinite loops\n",
    "                    else:\n",
    "                        more_examples = False\n",
    "                        break\n",
    "\n",
    "            if more_examples:\n",
    "                # Once the buffer is filled with characters, we tokenize them and prepare the sequences\n",
    "                # with the specified length\n",
    "                all_token_ids = []\n",
    "                tokenized_buffer = self.tokenizer(buffer, truncation=False)\n",
    "                for tokenized_input in tokenized_buffer[\"input_ids\"]:\n",
    "                    # Extend the list of token IDs with end-of-sequence token IDs\n",
    "                    all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "\n",
    "                for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                    input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                    if len(input_ids) == self.seq_length:\n",
    "                        # Yield a tensor representing a sequence of the specified length\n",
    "                        yield torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__iter__()` function builds up a buffer of strings until it contains enough characters. All the elements in the buffer are tokenized and concatenated with the `EOS` token, then the long sequence in `all_token_ids` is chunked in `seq_length`-size slices. **Normally, we would need attention masks to stack padded sequences** of varying length and make sure the padding is ignored during training. **We have taken care of this by only providing sequences of the same (maximal) length**, so we don't need the masks here and only return the `input_ids` (i.e., no need for padding). Let's test our iterable dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1024\n",
      "tensor([ 638, 3806,  199,  ...,  365,  482,   70])\n",
      "1: 1024\n",
      "tensor([1054,  426,  478,  ..., 2729,  274, 1040])\n",
      "2: 1024\n",
      "tensor([  63, 2729,  267,  ...,  282,  571,  400])\n",
      "3: 1024\n",
      "tensor([4480,  433,  313,  ...,   63,  693, 3057])\n",
      "4: 1024\n",
      "tensor([   8, 4512, 1046,  ...,  199,    3,  258])\n"
     ]
    }
   ],
   "source": [
    "shuffled_dataset = dataset.shuffle(buffer_size=100)\n",
    "# As an example, we create a dataset that is compoased of 10 sequences\n",
    "constant_length_dataset = InfiniteConstantLengthDataset(tokenizer, shuffled_dataset, \"content\", num_of_sequences=10)\n",
    "\n",
    "dataset_iterator = iter(constant_length_dataset)\n",
    "\n",
    "for i in range(0, 5):\n",
    "    b = next(dataset_iterator)\n",
    "    print(f\"{i}: {len(b)}\")\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, this works as intended and we get constant-length inputs for the model. Now that we have a reliable data source for the model, it's time to build the actual training loop.\n",
    "\n",
    "-----\n",
    "\n",
    "Notice that we shuffled the raw dataset before creating a ConstantLengthDataset. Since this is an iterable dataset, we can’t just shuffle the whole dataset at the beginning. Instead, we set up a buffer with size `buffer_size` and shuffle the elements in this buffer before we get elements from the dataset\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 - `ConstantLenghtDataset`\n",
    "\n",
    "This is a finite version of the ConstantLenghtDataset. In the previous case, by making the dataset infinite we don't need to preoccupy ourselves with certain issues such as the number of buffers we can generate from a dataset because"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedConstantLengthDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer, \n",
    "        dataset,\n",
    "        text_attribute_name,\n",
    "        seq_length=1024,\n",
    "        num_of_sequences=1024, \n",
    "        chars_per_token=3.6,\n",
    "        debug=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a custom dataset for constant-length sequences for language modeling.\n",
    "        \n",
    "        Args:\n",
    "            tokenizer (transformers.PreTrainedTokenizer): A tokenizer object for text preprocessing.\n",
    "            dataset (torch.utils.data.Dataset): An existing dataset containing text examples.\n",
    "            seq_length (int): The desired sequence length for language modeling.\n",
    "            num_of_sequences (int): The number of sequences to concatenate.\n",
    "            chars_per_token (float): Average number of characters per token in the text data.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id  # End-of-sequence token ID\n",
    "        self.dataset = dataset\n",
    "        self.text_attribute_name = text_attribute_name # name of the \"column\" that contains the text in the HF dataset (e.g., \"content\", \"text\", etc)\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences # buffer size\n",
    "        self.debug=debug\n",
    "\n",
    "    def generate_all_buffers(self):\n",
    "        \"\"\"\n",
    "        Generate all buffers from the entire dataset and return them as a list.\n",
    "        \"\"\"\n",
    "        buffers = []\n",
    "        iterator = iter(self.dataset)\n",
    "        \n",
    "        while True:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                # Check if the buffer has reached the desired character limit\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    break\n",
    "                try:\n",
    "                    # Append the content of the next example to the buffer\n",
    "                    buffer.append(next(iterator)[self.text_attribute_name])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    # If there are no more examples, break out of the loop\n",
    "                    break\n",
    "            \n",
    "            if not buffer:\n",
    "                # If the buffer is empty, there are no more examples to process\n",
    "                break\n",
    "            \n",
    "            # Once the buffer is filled with characters, tokenize them and add to the list\n",
    "            all_token_ids = []\n",
    "            tokenized_buffer = self.tokenizer(buffer, truncation=False)\n",
    "            for tokenized_input in tokenized_buffer['input_ids']:\n",
    "                # Extend the list of token IDs with end-of-sequence token IDs\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            \n",
    "            buffers.append(torch.tensor(all_token_ids))\n",
    "        \n",
    "        return buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fixed_dataset(streaming_dataset, num_examples):\n",
    "\n",
    "    # Initialize an empty list to store the examples\n",
    "    examples = []\n",
    "\n",
    "    # Loop through the streaming dataset and collect examples\n",
    "    for example in streaming_dataset:\n",
    "        examples.append(example)\n",
    "        \n",
    "        # Break the loop once you have collected the desired number of examples\n",
    "        if len(examples) == num_examples:\n",
    "            break\n",
    "\n",
    "    # Create a new Hugging Face dataset from the collected examples (easist way to do is with Pandas)\n",
    "    return datasets.Dataset.from_pandas(pd.DataFrame(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # # Once the buffer is filled with characters, we tokenize them and prepare the sequences\n",
    "            # # with the specified length\n",
    "            # all_token_ids = []\n",
    "            # tokenized_buffer = self.tokenizer(buffer, truncation=False)\n",
    "            # for tokenized_input in tokenized_buffer['input_ids']:\n",
    "            #     # Extend the list of token IDs with end-of-sequence token IDs\n",
    "            #     all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            \n",
    "            # for i in range(0, len(all_token_ids), self.seq_length):\n",
    "            #     input_ids = all_token_ids[i : i + self.seq_length]\n",
    "            #     if len(input_ids) == self.seq_length:\n",
    "            #         # Yield a tensor representing a sequence of the specified length\n",
    "            #         yield torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbc0e18765d4884b11bca58f79f1e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = 'transformersbook/codeparrot-train'\n",
    "streaming_dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "fixed_dataset = generate_fixed_dataset(streaming_dataset, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Defining the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the elements to write our training loop. One obvious limitation of training our own language model is the memory limits on the GPUs we will use. Even\n",
    "on a modern graphics card you can’t train a model at GPT-2 scale in reasonable time.\n",
    "\n",
    "In this tutorial we will implement a training loop with PyTorch lightning, which simplifies the process and makes it more scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 - Create Lightning Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeParrotDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer, \n",
    "        seq_length, \n",
    "        num_of_sequences, \n",
    "        chars_per_token,\n",
    "        train_batch_size,\n",
    "        val_batch_size,\n",
    "        shuffle_buffer,\n",
    "        seed,\n",
    "        num_workers, \n",
    "        debug\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        self.num_of_sequences = num_of_sequences\n",
    "        self.chars_per_token = chars_per_token\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "        self.seed = seed\n",
    "        self.num_workers = num_workers\n",
    "        self.debug = debug\n",
    "\n",
    "    # Multiple GPU\n",
    "    def setup(self, stage):\n",
    "        # Load data\n",
    "        train_data = load_dataset('transformersbook/codeparrot-train', split=\"train\", streaming=True)\n",
    "        # This shuffles the Python files that will then be concantenated to form the \"buffer\" in the ConstantLengthDataset\n",
    "        train_data = train_data.shuffle(buffer_size=self.shuffle_buffer, seed=self.seed)\n",
    "        valid_data = load_dataset('transformersbook/codeparrot-valid', split=\"validation\", streaming=True)\n",
    "        \n",
    "        # Create ConstantLengthDataset, which have an internal buffer\n",
    "        # to avoid the necessity of padding and the bias of dropping the last part of Python files\n",
    "        self.train_ds = InfiniteConstantLengthDataset(tokenizer, train_data, seq_length=self.seq_length, text_attribute_name=\"content\", debug=self.debug)\n",
    "        self.val_ds = InfiniteConstantLengthDataset(tokenizer, valid_data, seq_length=self.seq_length, text_attribute_name=\"content\", debug=self.debug)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train_ds,\n",
    "            batch_size=self.train_batch_size,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.val_ds,\n",
    "            batch_size=self.val_batch_size,\n",
    "            num_workers=self.num_workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 - Create Lightning Model Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2LightningModule(pl.LightningModule):\n",
    "    def __init__(self, tokenizer, learning_rate, model_checkpoint=\"gpt2\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        config = AutoConfig.from_pretrained(model_checkpoint, vocab_size=len(tokenizer))\n",
    "        self.model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def _common_step(self, batch):\n",
    "        # We automatically pass a Tensor, it is not a dictionary since there are no labels or attention_mask\n",
    "        input_ids = batch\n",
    "        labels = batch\n",
    "\n",
    "        outputs = self(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": loss,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._common_step(batch)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"val_loss\": loss,\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.validation_step_outputs = [{\"val_loss\": loss}]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss = torch.stack(\n",
    "            [x[\"val_loss\"] for x in self.validation_step_outputs]\n",
    "        ).mean()\n",
    "        perplexity = torch.exp(val_loss)\n",
    "        self.log(\"val_perplexity\", perplexity, prog_bar=True)\n",
    "\n",
    "    def generate_text(self, input_text, max_length=50, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate text given an initial input string using the trained GPT-2 model.\n",
    "\n",
    "        Args:\n",
    "            input_text (str): The initial input text.\n",
    "            max_length (int): Maximum length of the generated text.\n",
    "            temperature (float): Sampling temperature (higher values increase randomness).\n",
    "\n",
    "        Returns:\n",
    "            str: The generated text.\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `validation_step()` method does not return anything by default. This is because PyTorch Lightning will automatically aggregate the losses and other metrics that you log in the validation_step() method at the end of each epoch. This allows you to focus on writing code to compute and log your metrics, and let PyTorch Lightning take care of the rest.\n",
    "\n",
    "However, there are some cases where you may want to return something from the `validation_step()` method. For example, you may want to return a list of predictions or other outputs that you can use in the `on_validation_epoch_end()` method.\n",
    "\n",
    "If you do want to return something from the validation_step() method, you can simply add a return statement at the end of your method. For example:\n",
    "\n",
    "```python\n",
    "def validation_step(self, batch, batch_idx):\n",
    "    # rest of the code\n",
    "    ...\n",
    "\n",
    "    # Return the predictions\n",
    "    return self.model.generate(batch[\"input_ids\"], max_length=50)\n",
    "```\n",
    "\n",
    "In this example, the `validation_step()` method returns a list of predictions for each batch. These predictions can then be used in the `on_validation_epoch_end()` method to compute additional metrics, such as the accuracy on a held-out validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 - Custom evaluation callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with infinite training and validation datasets, the idea is to run Y steps of evaluation after X steps of training. While PyTorch Lightning has a `val_check_interval` that allows us to indicate X, the issue is that it does not provide a way to also establish Y, so we need to do this manually. And for that, we are going to define a custom evaluation callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEvaluationCallback(pl.Callback):\n",
    "    def __init__(self, evaluate_every_n_batches, evaluation_steps):\n",
    "        super().__init__()\n",
    "        self.evaluate_every_n_batches = evaluate_every_n_batches\n",
    "        self.evaluation_steps = evaluation_steps\n",
    "        self.batch_counter = 0\n",
    "    \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        self.batch_counter += 1\n",
    "\n",
    "        if self.batch_counter % self.evaluate_every_n_batches == 0:\n",
    "            print(self.batch_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the average number of characters per token in the 'transformersbook/codeparrot-train' dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c336e807827a4039b977f9ee5ad0ff1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33106eef3e1c4f8fbc8a2ba253325bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = 'transformersbook/codeparrot-train'\n",
    "tokenizer = new_tokenizer_larger\n",
    "chars_per_token = calculate_characters_per_token(dataset_name, tokenizer, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "debug = False\n",
    "\n",
    "# Data\n",
    "seq_length = 1024 # GPT2 specific\n",
    "num_of_sequences = 200 # The higher the better data quality\n",
    "shuffle_buffer = 100\n",
    "train_batch_size = 2\n",
    "val_batch_size = 2\n",
    "text_attribute_name = \"content\"\n",
    "\n",
    "# Training\n",
    "learning_rate = 5e-4\n",
    "max_train_steps = 50000\n",
    "eval_interval = 100 # every X training steps, we run evaluation\n",
    "max_eval_steps = 10 # We run Y evaluation steps\n",
    "precision = \"16-mixed\"\n",
    "num_epochs = 3\n",
    "num_workers  = multiprocessing.cpu_count() - 1\n",
    "\n",
    "# Compute\n",
    "compute_accelerator = \"gpu\"\n",
    "compute_devices = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Too many dataloader workers: 3 (max is dataset.n_shards=1). Stopping 2 dataloader workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "enumerator = enumerate(data_module.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " tensor([[1310,  313, 1628,  ...,  382, 3434,  274],\n",
       "         [1249,    8, 2393,  ...,  610,   59,   76]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop\n",
    "\n",
    "There seems to be an issue between FastTokenizers and PyTorchLightning. Would need to take a deeper look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "data_module = CodeParrotDataModule(\n",
    "    tokenizer, seq_length, num_of_sequences, chars_per_token, train_batch_size, val_batch_size, shuffle_buffer, seed, num_workers, debug\n",
    ")\n",
    "\n",
    "model = GPT2LightningModule(tokenizer, learning_rate, \"gpt2\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    devices=compute_devices,\n",
    "    accelerator=compute_accelerator,\n",
    "    min_epochs=1,\n",
    "    max_epochs=num_epochs,\n",
    "    precision=precision,\n",
    "    callbacks=[CustomEvaluationCallback(evaluate_every_n_batches=10, evaluation_steps=50)],\n",
    "    # logger=logger,\n",
    "    # profiler=profiler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae9dcd8f4db400f9ef86d2a9c5ee013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 89.8 M\n",
      "------------------------------------------\n",
      "89.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "89.8 M    Total params\n",
      "359.025   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477e7f099fa84584a1df155100edbee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Too many dataloader workers: 3 (max is dataset.n_shards=1). Stopping 2 dataloader workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e33defe9d55421984a106de1270fb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/transformers/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)\n",
    "# trainer.validate(model, data_module)\n",
    "# trainer.test(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero-touch-coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
